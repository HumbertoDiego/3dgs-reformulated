{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/HumbertoDiego/3dgs-reformulated/blob/main/Torch_Backpropagation_Tests.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B_7htBtewwmv"
      },
      "source": [
        "#### Time everything"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%load_ext autotime"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bKJDMfBrw28w"
      },
      "source": [
        "## Check Cuda and environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hpLhVpnWw6hr",
        "outputId": "b6e6d9f6-e3d3-40fc-81fd-cac87618d898"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi\n",
        "!nvcc --version\n",
        "!gcc --version\n",
        "!python -c \"import torch; print(torch.cuda.is_available(), torch.__version__, torch.cuda.get_device_name(0))\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Exemplo 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Sobre o funcionamento da diferenciação automática do PyTorch `.autograd`:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Uma variável folha (<i>leaf</i>) é uma variável que está no início do grafo de retropropagação da diferenciação. Isso significa que nenhuma operação rastreada pelo mecanismo `.autograd` a criou. Não são rastreadas:\n",
        "- Operações em tensores com `requires_grad=False`.\n",
        "- Operações realizadas fora do contexto de cálculo de gradiente (como dentro de `torch.no_grad()`).\n",
        "- Operações in-place (modificações diretas no tensor) podem causar problemas e não serem rastreadas corretamente.\n",
        "\n",
        "Assim, variáveis folha é o que se deseja quando se otimiza redes neurais, pois geralmente são seus pesos ou entrada."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "x = torch.tensor([3.0], requires_grad=True) # x is a leaf variable\n",
        "\n",
        "# Forward pass\n",
        "y = x **2\n",
        "z = y + 2\n",
        "# Backward pass\n",
        "z.backward()\n",
        "# Print gradients\n",
        "print(\"x is leaf?\" , x.is_leaf, \"-->\\tx.grad=\",  x.grad)\n",
        "print(\"y is leaf?\" , y.is_leaf, \"-->\\ty.grad=\",  y.grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Como criar um tensor cujo gradiente NÃO seja populado pelo mecanismo `.autograd`?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def info(mytensor, name='a'):\n",
        "    # Forward pass\n",
        "    b = mytensor**2\n",
        "    # Backward pass\n",
        "    b.backward()\n",
        "    # Print gradients\n",
        "    print(f\"{name} is leaf?\" , mytensor.is_leaf, f\"-->\\t{name}.grad=\",  mytensor.grad)\n",
        "\n",
        "def info2(mytensor, name='a'):\n",
        "    # Forward pass\n",
        "    b = mytensor.T @ mytensor\n",
        "    # Backward pass\n",
        "    b.backward()\n",
        "    # Print gradients\n",
        "    print(f\"{name} is leaf?\" , mytensor.is_leaf, f\"-->\\t{name}.grad=\",  mytensor.grad)\n",
        "    \n",
        "# a is NOT a leaf variable because .cuda() is an operation creating it.\n",
        "a = torch.tensor(10.0, requires_grad=True).cuda() \n",
        "info(a)\n",
        "\n",
        "# b is NOT a leaf variable because .mean() is an operation creating it.\n",
        "b = torch.rand(3, requires_grad=True).mean() \n",
        "info(b,'b')\n",
        "\n",
        "# c[i] is NOT a leaf variable because slicing a tensor is not tracked by autograd.\n",
        "c = torch.rand(3).requires_grad_()\n",
        "info(c[0], 'c[0]')\n",
        "\n",
        "# d[:] is NOT a leaf variable because slicing a tensor is not tracked by autograd.\n",
        "d = torch.rand(3).requires_grad_()\n",
        "info2(d[:], 'd[:]')\n",
        "\n",
        "# b is a leaf variable.\n",
        "# But e is NOT a leaf variable because it has an operation over b creating it.\n",
        "# Nevertheless, e requires_grad=True by contagion, so it's gradient can reach b.\n",
        "b = torch.rand(3).requires_grad_()\n",
        "e = b.T @ b \n",
        "info(e, 'e')\n",
        "\n",
        "# f is a leaf variable \n",
        "# But a view of a leaf Variable that requires grad is being used in an in-place operation.\n",
        "f = torch.rand(3).requires_grad_()\n",
        "try:\n",
        "    f[0] = 1.0\n",
        "    info2(f, 'f')\n",
        "except RuntimeError as e:\n",
        "    print(f\"f - \\033[91mError: {e}\\033[0m\")\n",
        "\n",
        "# g is a leaf variable\n",
        "# But b ias a new tensor, and does not preserve the computational graph of a[0,0]\n",
        "# breaking the connection to a.\n",
        "# So the gradient is computed with respect to this new tensor, not the original tensor a\n",
        "g = torch.tensor([[1.0], [1.0], [1.0]], requires_grad=True)\n",
        "h = torch.tensor([g[0,0]], requires_grad=True)\n",
        "h.backward()\n",
        "print(\"g is leaf?\" , g.is_leaf, \"-->\\tg.grad=\",  g.grad)\n",
        "print(\"h is leaf?\" , h.is_leaf, \"-->\\th.grad=\",  h.grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Como criar um tensor cujo gradiente seja populado pelo mecanismo `.autograd`?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# a is a leaf variable, requires grad, has no operation creating it.\n",
        "a = torch.tensor(10.0).requires_grad_()\n",
        "info(a)\n",
        "\n",
        "# b is a leaf variable, requires grad, has no operation creating it.\n",
        "b = torch.tensor(10.0, requires_grad=True, device=\"cuda\") \n",
        "info(b, 'b')\n",
        "\n",
        "# c is a leaf variable, requires grad, has no operation creating it.\n",
        "c = torch.rand(3).requires_grad_()\n",
        "info2(c, 'c')\n",
        "\n",
        "# Use torch.stack() or torch.cat() to combine tensors while keeping gradient flow intact.\n",
        "d = torch.tensor([1.0, 1.0, 1.0], requires_grad=True)\n",
        "e = torch.stack([d[0], d[1]]).norm()\n",
        "e.backward()\n",
        "print(\"d is leaf?\" , d.is_leaf, \"-->\\td.grad=\",  d.grad)\n",
        "print(\"e is leaf?\" , e.is_leaf, \"-->\\te.grad=\",  e.grad)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As principais operações rastreadas pelo mecanismo `.autograd` são:\n",
        "\n",
        "1. Operações Matemáticas:\n",
        "- Soma, subtração, multiplicação, divisão.\n",
        "- Operações elementares como exponenciação (`torch.exp`), logaritmo (`torch.log`), seno, cosseno, etc.\n",
        "- Operações de redução, como `torch.sum`, `torch.mean`, `torch.max`, etc.\n",
        "2. Operações de Álgebra Linear:\n",
        "- Produto escalar e matricial (`torch.matmul`, `torch.mm`).\n",
        "- Decomposições como SVD, Cholesky, etc.\n",
        "- Inversão de matrizes e determinantes.\n",
        "3. Operações de Manipulação de Tensores\n",
        "- Transposição (`torch.transpose`), reshape (`torch.view`, `torch.reshape`).\n",
        "- Concatenação (`torch.cat`) e divisão (`torch.split`).\n",
        "- Indexação avançada que altera o tensor.\n",
        "4. Operações de Redes Neurais\n",
        "- Camadas como `torch.nn.Linear`, `torch.nn.Conv2d`, etc.\n",
        "- Funções de ativação como `ReLU`, `Sigmoid`, `Tanh`.\n",
        "- Funções de perda como `torch.nn.CrossEntropyLoss`.\n",
        "5. Operações Específicas\n",
        "- Operações que envolvem `autograd`, como `torch.autograd.grad` e `torch.autograd.backward`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Como mesclar com operações que não fazem usem do grafo de retropropagação da diferenciação do PyTorch?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "$y = x^2$\n",
        "\n",
        "$z = y^2$\n",
        "\n",
        "$\\nabla z(x) = \\frac{\\partial z}{\\partial x} =\\frac{\\partial y}{\\partial x} \\cdot \\frac{\\partial z}{\\partial y} = 2x \\cdot 2x^2$\n",
        "\n",
        "Para $x = 3 \\rightarrow \\nabla z(x) = 108$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(tensor(108.), tensor(18.), tensor(81., grad_fn=<PowBackward0>))"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "(np.float32(108.0), np.float32(18.0), np.float32(81.0))"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "def foward(x):\n",
        "    return x**2\n",
        "\n",
        "# all torch\n",
        "X_torch = torch.tensor(3.0, requires_grad=True)\n",
        "Y_torch = foward(X_torch)\n",
        "Y_torch.retain_grad()\n",
        "Z_torch = foward(Y_torch)\n",
        "\n",
        "Z_torch.backward()\n",
        "\n",
        "display((X_torch.grad, Y_torch.grad, Z_torch)) # 108 ,18 ,81\n",
        "\n",
        "# all numpy\n",
        "X_numpy = X_torch.detach().numpy()\n",
        "Y_numpy = foward(X_numpy)\n",
        "Z_numpy = foward(Y_numpy)\n",
        "\n",
        "def backward(grad, input):\n",
        "    \"\"\"\n",
        "    In the backward pass we receive a Tensor containing the gradient of the loss\n",
        "    with respect to the output, and we need to compute the gradient of the loss\n",
        "    with respect to the input.\n",
        "    \"\"\"\n",
        "    return grad*2*input\n",
        "\n",
        "Y_numpy_grad = backward(1, Y_numpy)\n",
        "X_numpy_grad = backward(Y_numpy_grad, X_numpy)\n",
        "\n",
        "display((X_numpy_grad, Y_numpy_grad, Z_numpy)) # 108 ,18 ,81"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SpaQWAQg1VtD"
      },
      "source": [
        "# Exemplo 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hCJ8Q0ZKex7z"
      },
      "source": [
        "$y = x^2$\n",
        "\n",
        "$\\nabla(x) = \\frac{dy}{dx}=2x$\n",
        "\n",
        "Para $x=3 \\rightarrow \\nabla(x)=6$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Implementação PyTorch:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5HlzC_wc2ZiB",
        "outputId": "bbd865ef-263c-4325-df3d-d9c419c3e762"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "x = torch.tensor([3.0], requires_grad=True)\n",
        "\n",
        "# Forward pass\n",
        "y = x **2\n",
        "\n",
        "# Backward pass\n",
        "y.backward()\n",
        "print(x.grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V_t9-fiKeh43"
      },
      "source": [
        "# Exemplo 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2g3dFVs4gahF"
      },
      "source": [
        "$y = x  w + 1$\n",
        "\n",
        "$\\nabla(x) = \\frac{\\partial y}{\\partial x}=w$\n",
        "\n",
        "$\\nabla(w) = \\frac{\\partial y}{\\partial w}=x$\n",
        "\n",
        "Para $x= 2$ e $w=3  \\rightarrow$\n",
        "\n",
        "$\\nabla(x)=3$\n",
        "\n",
        "$\\nabla(w)=2$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Implementação PyTorch:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c_q1wVPteLjH",
        "outputId": "16717168-ca49-4c0b-b874-8319141ae054"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "x = torch.tensor(2.0, requires_grad=True)\n",
        "w = torch.tensor(3.0, requires_grad=True)\n",
        "\n",
        "# Forward pass\n",
        "y = x * w + 1\n",
        "\n",
        "# Backward pass\n",
        "y.backward()\n",
        "print(x.grad)\n",
        "print(w.grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Exemplo 4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "$m = x_1w_1 + x_2w_2 + x_3w_3 + b$\n",
        "\n",
        "Onde $w_i$ e $b$ são parâmetros a serem atualizados pelo gradiente descendente.\n",
        "\n",
        "$\\nabla(w_1) = \\frac{\\partial m}{\\partial w_1}=x_1$\n",
        "\n",
        "$\\nabla(w_2) = \\frac{\\partial m}{\\partial w_2}=x_2$\n",
        "\n",
        "$\\nabla(w_3) = \\frac{\\partial m}{\\partial w_2}=x_3$\n",
        "\n",
        "$\\nabla(b) = \\frac{\\partial m}{\\partial b}=1$\n",
        "\n",
        "Para $[x_1, x_2, x_3]=[2, 3, 4]$ e quaisquer $[w_1, w_2, w_3,b] \\rightarrow$\n",
        "\n",
        "$\\nabla(w_1) = 2$\n",
        "\n",
        "$\\nabla(w_2) = 3$\n",
        "\n",
        "$\\nabla(w_3) = 4$\n",
        "\n",
        "$\\nabla(b) = 1$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Implementação PyTorch:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "# Define a simple model\n",
        "m = torch.nn.Linear(in_features=3, out_features=1)  # Single layer: m = xW^T + b\n",
        "\n",
        "# Input and target\n",
        "x = torch.tensor([[2.0, 3.0, 4.0]], requires_grad=True)\n",
        "\n",
        "# Forward pass\n",
        "pred = m(x)\n",
        "\n",
        "# Backward pass\n",
        "pred.backward()\n",
        "\n",
        "# Gradients\n",
        "print(\"Weight gradient:\", m.weight.grad)\n",
        "print(\"Bias gradient:\", m.bias.grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Exemplo 5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "$\\mathbf{x} = [x_1 , x_2 , x_3]$\n",
        "\n",
        "$y = min([x_1^2 , x_2^2, x_3^2])$\n",
        "\n",
        "$\\nabla(\\mathbf{x}) = \\frac{\\partial y}{\\partial \\mathbf{x}} = $\n",
        "\n",
        "- Se $x_1^2$ é o mínimo:\n",
        "\n",
        "$\\frac{\\partial y}{\\partial \\mathbf{x}} = \\frac{\\partial x_1^2}{\\partial \\mathbf{x}} = [\\frac{\\partial x_1^2}{\\partial x_1}, \\frac{\\partial x_1^2}{\\partial x_2}, \\frac{\\partial x_1^2}{\\partial x_3}] = [2x_1, 0, 0 ] $\n",
        "\n",
        "- Se $x_2^2$ é o mínimo:\n",
        "\n",
        "$\\frac{\\partial y}{\\partial \\mathbf{x}} = \\frac{\\partial x_2^2}{\\partial \\mathbf{x}} = [\\frac{\\partial x_2^2}{\\partial x_1}, \\frac{\\partial x_2^2}{\\partial x_2}, \\frac{\\partial x_2^2}{\\partial x_3}] = [0, 2x_2, 0 ] $\n",
        "\n",
        "- Se $x_3^2$ é o mínimo:\n",
        "\n",
        "$\\frac{\\partial y}{\\partial \\mathbf{x}} = \\frac{\\partial x_3^2}{\\partial \\mathbf{x}} = [\\frac{\\partial x_3^2}{\\partial x_1}, \\frac{\\partial x_3^2}{\\partial x_2}, \\frac{\\partial x_3^2}{\\partial x_3}] = [0, 0, 2x_3 ] $\n",
        "\n",
        "Para $x= \\begin{bmatrix}0.1 & 0.8 & 0.4\\end{bmatrix} \\rightarrow x_1^2$ é o mínimo $\\rightarrow y = 0.01$\n",
        "\n",
        "$\\nabla(x)= [2x_1, 0, 0 ] = [0.2, 0, 0 ]$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Implementação PyTorch:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "x = torch.tensor([[0.1], [0.8], [0.4]], requires_grad=True)\n",
        "\n",
        "# Forward pass\n",
        "x2 = x * x\n",
        "y = x2.min(dim=0).values\n",
        "\n",
        "# Backward pass\n",
        "y.backward()\n",
        "\n",
        "print(x.grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Exemplo 6"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "$y = e^{(-x^2/2)}$\n",
        "\n",
        "Onde $x$ é um parâmetro a ser atualizado pelo gradiente descendente.\n",
        "\n",
        "$\\nabla(x) = \\frac{\\partial y}{\\partial x}= -xe^{(-x^2/2)}$\n",
        "\n",
        "Para $[x]=[2] \\rightarrow$\n",
        "\n",
        "$\\nabla(x) = -0.270670566473225$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Implementação SymPy:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sympy as sp\n",
        "\n",
        "x= sp.symbols('x')\n",
        "y = sp.exp(-x**2 / 2)\n",
        "\n",
        "y_derivative = sp.diff(y, x)\n",
        "\n",
        "print(\"y:\")\n",
        "display(y)\n",
        "\n",
        "print(\"dy/dx:\")\n",
        "display(y_derivative)\n",
        "\n",
        "print(\"Derivative evaluated at x=2:\",  y_derivative.evalf(subs={x: 2}))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Implementação PyTorch:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "x = torch.tensor(2.0, requires_grad=True)\n",
        "\n",
        "# Forward pass\n",
        "y = torch.exp(-x**2 / 2)\n",
        "\n",
        "# Backward pass\n",
        "y.backward()\n",
        "\n",
        "# Gradients\n",
        "print(x.grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Exemplo 7"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "$y = \\frac{1}{\\sigma\\sqrt{2\\pi}} e^{-0.5\\frac{(x-\\mu)^2}{\\sigma^2}}$\n",
        "\n",
        "Onde $x$ é um parâmetro a ser atualizado pelo gradiente descendente.\n",
        "\n",
        "$\\nabla(x) = \\frac{\\partial y}{\\partial x}= - \\frac{0.25 \\sqrt{2} \\left(2 x - 2 μ\\right) e^{- \\frac{0.5 \\left(x - μ\\right)^{2}}{σ^{2}}}}{\\sqrt{\\pi} σ^{3}}$\n",
        "\n",
        "Para $[x, \\mu, \\sigma]=[2,0,1] \\rightarrow$\n",
        "\n",
        "$\\nabla(x) = ? $"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Implementação SymPy:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sympy as sp\n",
        "\n",
        "x= sp.symbols('x')\n",
        "mu = sp.symbols('μ')\n",
        "sigma = sp.symbols('σ')\n",
        "\n",
        "y = 1/(sigma * sp.sqrt(2 * sp.pi)) * sp.exp(-1/2 * (x - mu)**2 / sigma**2)\n",
        "\n",
        "dy_dx = sp.diff(y, x)\n",
        "print(\"Function:\", y)\n",
        "print(\"Derivative w.r.t x:\", dy_dx)\n",
        "print(\"Derivative w.r.t x evaluated at x=2, μ=0, σ=1:\",  dy_dx.evalf(subs={x: 2, mu: 0, sigma: 1}))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Implementação PyTorch:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "x = torch.tensor(2.0, requires_grad=True)\n",
        "mu = torch.tensor(0.0, requires_grad=False)\n",
        "sigma = torch.tensor(1.0, requires_grad=False)\n",
        "\n",
        "# Forward pass\n",
        "y = 1/(sigma * torch.sqrt(torch.tensor(2.0 * torch.pi))) * torch.exp(-1/2 * (x - mu)**2 / sigma**2)\n",
        "\n",
        "# Backward pass\n",
        "y.backward()\n",
        "\n",
        "# Gradients\n",
        "print(x.grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Exemplo 8"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "$y = \\frac{1}{\\sigma\\sqrt{2\\pi}} e^{-0.5\\frac{(x-\\mu)^2}{\\sigma^2}}$\n",
        "\n",
        "Onde $\\mu$ e $\\sigma$ são parâmetros a seres atualizados pelo gradiente descendente.\n",
        "\n",
        "$\\nabla(\\mu) = \\frac{\\partial y}{\\partial \\mu}= - \\frac{0.25 \\sqrt{2} \\left(- 2 x + 2 μ\\right) e^{- \\frac{0.5 \\left(x - μ\\right)^{2}}{σ^{2}}}}{\\sqrt{\\pi} σ^{3}}$\n",
        "\n",
        "$\\nabla(\\sigma) = \\frac{\\partial y}{\\partial \\sigma}= - \\frac{\\sqrt{2} e^{- \\frac{0.5 \\left(x - μ\\right)^{2}}{σ^{2}}}}{2 \\sqrt{\\pi} σ^{2}} + \\frac{0.5 \\sqrt{2} \\left(x - μ\\right)^{2} e^{- \\frac{0.5 \\left(x - μ\\right)^{2}}{σ^{2}}}}{\\sqrt{\\pi} σ^{4}} $\n",
        "\n",
        "Para $[x, \\mu, \\sigma]=[2,0,1] \\rightarrow$\n",
        "\n",
        "$\\nabla(\\mu) = ? $\n",
        "\n",
        "$\\nabla(\\sigma) = ?$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Implementação SymPy:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sympy as sp\n",
        "\n",
        "x= sp.symbols('x')\n",
        "mu = sp.symbols('μ')\n",
        "sigma = sp.symbols('σ')\n",
        "\n",
        "y = 1/(sigma * sp.sqrt(2 * sp.pi)) * sp.exp(-1/2 * (x - mu)**2 / sigma**2)\n",
        "\n",
        "dy_dmu = sp.diff(y, mu)\n",
        "dy_dsigma = sp.diff(y, sigma)\n",
        "\n",
        "print(\"Function:\", y)\n",
        "print(\"Derivative w.r.t μ:\", dy_dmu)\n",
        "print(\"Derivative w.r.t σ:\", dy_dsigma)\n",
        "print(\"Derivative w.r.t μ evaluated at x=2, μ=0, σ=1:\",  dy_dmu.evalf(subs={x: 2, mu: 0, sigma: 1}))\n",
        "print(\"Derivative w.r.t σ evaluated at x=2, μ=0, σ=1:\",  dy_dsigma.evalf(subs={x: 2, mu: 0, sigma: 1}))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Implementação PyTorch:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "x = torch.tensor(2.0, requires_grad=False)\n",
        "mu = torch.tensor(0.0, requires_grad=True)\n",
        "sigma = torch.tensor(1.0, requires_grad=True)\n",
        "# Forward pass\n",
        "y = 1/(sigma * torch.sqrt(torch.tensor(2.0 * torch.pi))) * torch.exp(-1/2 * (x - mu)**2 / sigma**2)\n",
        "\n",
        "# Backward pass\n",
        "y.backward()\n",
        "\n",
        "# Gradients\n",
        "print(mu.grad)\n",
        "print(sigma.grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Exemplo 9"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "$G(x) = e^{-0.5 (\\mathbf{x}-\\mathbf{\\mu})^T \\Sigma^{-1} (\\mathbf{x}-\\mathbf{\\mu}) }$\n",
        "\n",
        "Onde:\n",
        "- $\\mathbf{\\mu},\\mathbf{x} \\in \\mathbb{R}^3$;\n",
        "- $\\Sigma \\in \\mathbb{R}^{3\\times 3}$;\n",
        "- $\\mathbf{\\mu}$ é um parâmetro a ser atualizado pelo gradiente descendente.\n",
        "\n",
        "$\\nabla(\\mu) = \\frac{\\partial G}{\\partial \\mu}= \\left[\\frac{\\partial G}{\\partial \\mu_1}, \\frac{\\partial G}{\\partial \\mu_2}, \\frac{\\partial G}{\\partial \\mu_3}\\right] = \n",
        "\\begin{bmatrix}- \\frac{\\left(- 0.5 x_{1} + 0.5 μ_{1}\\right) \\left({\\Sigma}_{1,1} {\\Sigma}_{2,2} - {\\Sigma}_{1,2} {\\Sigma}_{2,1}\\right)}{{\\Sigma}_{0,0} {\\Sigma}_{1,1} {\\Sigma}_{2,2} - {\\Sigma}_{0,0} {\\Sigma}_{1,2} {\\Sigma}_{2,1} - {\\Sigma}_{0,1} {\\Sigma}_{1,0} {\\Sigma}_{2,2} + {\\Sigma}_{0,1} {\\Sigma}_{1,2} {\\Sigma}_{2,0} + {\\Sigma}_{0,2} {\\Sigma}_{1,0} {\\Sigma}_{2,1} - {\\Sigma}_{0,2} {\\Sigma}_{1,1} {\\Sigma}_{2,0}} + \\frac{0.5 \\left(x_{1} - μ_{1}\\right) \\left({\\Sigma}_{1,1} {\\Sigma}_{2,2} - {\\Sigma}_{1,2} {\\Sigma}_{2,1}\\right)}{{\\Sigma}_{0,0} {\\Sigma}_{1,1} {\\Sigma}_{2,2} - {\\Sigma}_{0,0} {\\Sigma}_{1,2} {\\Sigma}_{2,1} - {\\Sigma}_{0,1} {\\Sigma}_{1,0} {\\Sigma}_{2,2} + {\\Sigma}_{0,1} {\\Sigma}_{1,2} {\\Sigma}_{2,0} + {\\Sigma}_{0,2} {\\Sigma}_{1,0} {\\Sigma}_{2,1} - {\\Sigma}_{0,2} {\\Sigma}_{1,1} {\\Sigma}_{2,0}} - \\frac{\\left(- 0.5 x_{2} + 0.5 μ_{2}\\right) \\left(- {\\Sigma}_{1,0} {\\Sigma}_{2,2} + {\\Sigma}_{1,2} {\\Sigma}_{2,0}\\right)}{{\\Sigma}_{0,0} {\\Sigma}_{1,1} {\\Sigma}_{2,2} - {\\Sigma}_{0,0} {\\Sigma}_{1,2} {\\Sigma}_{2,1} - {\\Sigma}_{0,1} {\\Sigma}_{1,0} {\\Sigma}_{2,2} + {\\Sigma}_{0,1} {\\Sigma}_{1,2} {\\Sigma}_{2,0} + {\\Sigma}_{0,2} {\\Sigma}_{1,0} {\\Sigma}_{2,1} - {\\Sigma}_{0,2} {\\Sigma}_{1,1} {\\Sigma}_{2,0}} + \\frac{0.5 \\left(x_{2} - μ_{2}\\right) \\left(- {\\Sigma}_{0,1} {\\Sigma}_{2,2} + {\\Sigma}_{0,2} {\\Sigma}_{2,1}\\right)}{{\\Sigma}_{0,0} {\\Sigma}_{1,1} {\\Sigma}_{2,2} - {\\Sigma}_{0,0} {\\Sigma}_{1,2} {\\Sigma}_{2,1} - {\\Sigma}_{0,1} {\\Sigma}_{1,0} {\\Sigma}_{2,2} + {\\Sigma}_{0,1} {\\Sigma}_{1,2} {\\Sigma}_{2,0} + {\\Sigma}_{0,2} {\\Sigma}_{1,0} {\\Sigma}_{2,1} - {\\Sigma}_{0,2} {\\Sigma}_{1,1} {\\Sigma}_{2,0}} - \\frac{\\left(- 0.5 x_{3} + 0.5 μ_{3}\\right) \\left({\\Sigma}_{1,0} {\\Sigma}_{2,1} - {\\Sigma}_{1,1} {\\Sigma}_{2,0}\\right)}{{\\Sigma}_{0,0} {\\Sigma}_{1,1} {\\Sigma}_{2,2} - {\\Sigma}_{0,0} {\\Sigma}_{1,2} {\\Sigma}_{2,1} - {\\Sigma}_{0,1} {\\Sigma}_{1,0} {\\Sigma}_{2,2} + {\\Sigma}_{0,1} {\\Sigma}_{1,2} {\\Sigma}_{2,0} + {\\Sigma}_{0,2} {\\Sigma}_{1,0} {\\Sigma}_{2,1} - {\\Sigma}_{0,2} {\\Sigma}_{1,1} {\\Sigma}_{2,0}} + \\frac{0.5 \\left(x_{3} - μ_{3}\\right) \\left({\\Sigma}_{0,1} {\\Sigma}_{1,2} - {\\Sigma}_{0,2} {\\Sigma}_{1,1}\\right)}{{\\Sigma}_{0,0} {\\Sigma}_{1,1} {\\Sigma}_{2,2} - {\\Sigma}_{0,0} {\\Sigma}_{1,2} {\\Sigma}_{2,1} - {\\Sigma}_{0,1} {\\Sigma}_{1,0} {\\Sigma}_{2,2} + {\\Sigma}_{0,1} {\\Sigma}_{1,2} {\\Sigma}_{2,0} + {\\Sigma}_{0,2} {\\Sigma}_{1,0} {\\Sigma}_{2,1} - {\\Sigma}_{0,2} {\\Sigma}_{1,1} {\\Sigma}_{2,0}}\\\\- \\frac{\\left(- 0.5 x_{1} + 0.5 μ_{1}\\right) \\left(- {\\Sigma}_{0,1} {\\Sigma}_{2,2} + {\\Sigma}_{0,2} {\\Sigma}_{2,1}\\right)}{{\\Sigma}_{0,0} {\\Sigma}_{1,1} {\\Sigma}_{2,2} - {\\Sigma}_{0,0} {\\Sigma}_{1,2} {\\Sigma}_{2,1} - {\\Sigma}_{0,1} {\\Sigma}_{1,0} {\\Sigma}_{2,2} + {\\Sigma}_{0,1} {\\Sigma}_{1,2} {\\Sigma}_{2,0} + {\\Sigma}_{0,2} {\\Sigma}_{1,0} {\\Sigma}_{2,1} - {\\Sigma}_{0,2} {\\Sigma}_{1,1} {\\Sigma}_{2,0}} + \\frac{0.5 \\left(x_{1} - μ_{1}\\right) \\left(- {\\Sigma}_{1,0} {\\Sigma}_{2,2} + {\\Sigma}_{1,2} {\\Sigma}_{2,0}\\right)}{{\\Sigma}_{0,0} {\\Sigma}_{1,1} {\\Sigma}_{2,2} - {\\Sigma}_{0,0} {\\Sigma}_{1,2} {\\Sigma}_{2,1} - {\\Sigma}_{0,1} {\\Sigma}_{1,0} {\\Sigma}_{2,2} + {\\Sigma}_{0,1} {\\Sigma}_{1,2} {\\Sigma}_{2,0} + {\\Sigma}_{0,2} {\\Sigma}_{1,0} {\\Sigma}_{2,1} - {\\Sigma}_{0,2} {\\Sigma}_{1,1} {\\Sigma}_{2,0}} - \\frac{\\left(- 0.5 x_{2} + 0.5 μ_{2}\\right) \\left({\\Sigma}_{0,0} {\\Sigma}_{2,2} - {\\Sigma}_{0,2} {\\Sigma}_{2,0}\\right)}{{\\Sigma}_{0,0} {\\Sigma}_{1,1} {\\Sigma}_{2,2} - {\\Sigma}_{0,0} {\\Sigma}_{1,2} {\\Sigma}_{2,1} - {\\Sigma}_{0,1} {\\Sigma}_{1,0} {\\Sigma}_{2,2} + {\\Sigma}_{0,1} {\\Sigma}_{1,2} {\\Sigma}_{2,0} + {\\Sigma}_{0,2} {\\Sigma}_{1,0} {\\Sigma}_{2,1} - {\\Sigma}_{0,2} {\\Sigma}_{1,1} {\\Sigma}_{2,0}} + \\frac{0.5 \\left(x_{2} - μ_{2}\\right) \\left({\\Sigma}_{0,0} {\\Sigma}_{2,2} - {\\Sigma}_{0,2} {\\Sigma}_{2,0}\\right)}{{\\Sigma}_{0,0} {\\Sigma}_{1,1} {\\Sigma}_{2,2} - {\\Sigma}_{0,0} {\\Sigma}_{1,2} {\\Sigma}_{2,1} - {\\Sigma}_{0,1} {\\Sigma}_{1,0} {\\Sigma}_{2,2} + {\\Sigma}_{0,1} {\\Sigma}_{1,2} {\\Sigma}_{2,0} + {\\Sigma}_{0,2} {\\Sigma}_{1,0} {\\Sigma}_{2,1} - {\\Sigma}_{0,2} {\\Sigma}_{1,1} {\\Sigma}_{2,0}} - \\frac{\\left(- 0.5 x_{3} + 0.5 μ_{3}\\right) \\left(- {\\Sigma}_{0,0} {\\Sigma}_{2,1} + {\\Sigma}_{0,1} {\\Sigma}_{2,0}\\right)}{{\\Sigma}_{0,0} {\\Sigma}_{1,1} {\\Sigma}_{2,2} - {\\Sigma}_{0,0} {\\Sigma}_{1,2} {\\Sigma}_{2,1} - {\\Sigma}_{0,1} {\\Sigma}_{1,0} {\\Sigma}_{2,2} + {\\Sigma}_{0,1} {\\Sigma}_{1,2} {\\Sigma}_{2,0} + {\\Sigma}_{0,2} {\\Sigma}_{1,0} {\\Sigma}_{2,1} - {\\Sigma}_{0,2} {\\Sigma}_{1,1} {\\Sigma}_{2,0}} + \\frac{0.5 \\left(x_{3} - μ_{3}\\right) \\left(- {\\Sigma}_{0,0} {\\Sigma}_{1,2} + {\\Sigma}_{0,2} {\\Sigma}_{1,0}\\right)}{{\\Sigma}_{0,0} {\\Sigma}_{1,1} {\\Sigma}_{2,2} - {\\Sigma}_{0,0} {\\Sigma}_{1,2} {\\Sigma}_{2,1} - {\\Sigma}_{0,1} {\\Sigma}_{1,0} {\\Sigma}_{2,2} + {\\Sigma}_{0,1} {\\Sigma}_{1,2} {\\Sigma}_{2,0} + {\\Sigma}_{0,2} {\\Sigma}_{1,0} {\\Sigma}_{2,1} - {\\Sigma}_{0,2} {\\Sigma}_{1,1} {\\Sigma}_{2,0}}\\\\- \\frac{\\left(- 0.5 x_{1} + 0.5 μ_{1}\\right) \\left({\\Sigma}_{0,1} {\\Sigma}_{1,2} - {\\Sigma}_{0,2} {\\Sigma}_{1,1}\\right)}{{\\Sigma}_{0,0} {\\Sigma}_{1,1} {\\Sigma}_{2,2} - {\\Sigma}_{0,0} {\\Sigma}_{1,2} {\\Sigma}_{2,1} - {\\Sigma}_{0,1} {\\Sigma}_{1,0} {\\Sigma}_{2,2} + {\\Sigma}_{0,1} {\\Sigma}_{1,2} {\\Sigma}_{2,0} + {\\Sigma}_{0,2} {\\Sigma}_{1,0} {\\Sigma}_{2,1} - {\\Sigma}_{0,2} {\\Sigma}_{1,1} {\\Sigma}_{2,0}} + \\frac{0.5 \\left(x_{1} - μ_{1}\\right) \\left({\\Sigma}_{1,0} {\\Sigma}_{2,1} - {\\Sigma}_{1,1} {\\Sigma}_{2,0}\\right)}{{\\Sigma}_{0,0} {\\Sigma}_{1,1} {\\Sigma}_{2,2} - {\\Sigma}_{0,0} {\\Sigma}_{1,2} {\\Sigma}_{2,1} - {\\Sigma}_{0,1} {\\Sigma}_{1,0} {\\Sigma}_{2,2} + {\\Sigma}_{0,1} {\\Sigma}_{1,2} {\\Sigma}_{2,0} + {\\Sigma}_{0,2} {\\Sigma}_{1,0} {\\Sigma}_{2,1} - {\\Sigma}_{0,2} {\\Sigma}_{1,1} {\\Sigma}_{2,0}} - \\frac{\\left(- 0.5 x_{2} + 0.5 μ_{2}\\right) \\left(- {\\Sigma}_{0,0} {\\Sigma}_{1,2} + {\\Sigma}_{0,2} {\\Sigma}_{1,0}\\right)}{{\\Sigma}_{0,0} {\\Sigma}_{1,1} {\\Sigma}_{2,2} - {\\Sigma}_{0,0} {\\Sigma}_{1,2} {\\Sigma}_{2,1} - {\\Sigma}_{0,1} {\\Sigma}_{1,0} {\\Sigma}_{2,2} + {\\Sigma}_{0,1} {\\Sigma}_{1,2} {\\Sigma}_{2,0} + {\\Sigma}_{0,2} {\\Sigma}_{1,0} {\\Sigma}_{2,1} - {\\Sigma}_{0,2} {\\Sigma}_{1,1} {\\Sigma}_{2,0}} + \\frac{0.5 \\left(x_{2} - μ_{2}\\right) \\left(- {\\Sigma}_{0,0} {\\Sigma}_{2,1} + {\\Sigma}_{0,1} {\\Sigma}_{2,0}\\right)}{{\\Sigma}_{0,0} {\\Sigma}_{1,1} {\\Sigma}_{2,2} - {\\Sigma}_{0,0} {\\Sigma}_{1,2} {\\Sigma}_{2,1} - {\\Sigma}_{0,1} {\\Sigma}_{1,0} {\\Sigma}_{2,2} + {\\Sigma}_{0,1} {\\Sigma}_{1,2} {\\Sigma}_{2,0} + {\\Sigma}_{0,2} {\\Sigma}_{1,0} {\\Sigma}_{2,1} - {\\Sigma}_{0,2} {\\Sigma}_{1,1} {\\Sigma}_{2,0}} - \\frac{\\left(- 0.5 x_{3} + 0.5 μ_{3}\\right) \\left({\\Sigma}_{0,0} {\\Sigma}_{1,1} - {\\Sigma}_{0,1} {\\Sigma}_{1,0}\\right)}{{\\Sigma}_{0,0} {\\Sigma}_{1,1} {\\Sigma}_{2,2} - {\\Sigma}_{0,0} {\\Sigma}_{1,2} {\\Sigma}_{2,1} - {\\Sigma}_{0,1} {\\Sigma}_{1,0} {\\Sigma}_{2,2} + {\\Sigma}_{0,1} {\\Sigma}_{1,2} {\\Sigma}_{2,0} + {\\Sigma}_{0,2} {\\Sigma}_{1,0} {\\Sigma}_{2,1} - {\\Sigma}_{0,2} {\\Sigma}_{1,1} {\\Sigma}_{2,0}} + \\frac{0.5 \\left(x_{3} - μ_{3}\\right) \\left({\\Sigma}_{0,0} {\\Sigma}_{1,1} - {\\Sigma}_{0,1} {\\Sigma}_{1,0}\\right)}{{\\Sigma}_{0,0} {\\Sigma}_{1,1} {\\Sigma}_{2,2} - {\\Sigma}_{0,0} {\\Sigma}_{1,2} {\\Sigma}_{2,1} - {\\Sigma}_{0,1} {\\Sigma}_{1,0} {\\Sigma}_{2,2} + {\\Sigma}_{0,1} {\\Sigma}_{1,2} {\\Sigma}_{2,0} + {\\Sigma}_{0,2} {\\Sigma}_{1,0} {\\Sigma}_{2,1} - {\\Sigma}_{0,2} {\\Sigma}_{1,1} {\\Sigma}_{2,0}}\\end{bmatrix}^T$\n",
        "\n",
        "Para $\\mathbf{x}=[2,2,3],\\  \\mathbf{\\mu}=[1,1,1] \\ $ e $\\Sigma = I \\rightarrow$\n",
        "\n",
        "$\\nabla(\\mu) = ? $"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Implementação SymPy:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sympy as sp\n",
        "\n",
        "x1, x2, x3= sp.symbols('x_1 x_2 x_3')\n",
        "mu1, mu2, mu3 = sp.symbols('μ_1 μ_2 μ_3')\n",
        "x = sp.Matrix([x1, x2, x3])\n",
        "mu = sp.Matrix([mu1, mu2, mu3])\n",
        "Sigma = sp.Matrix([[1,0,0],[0,1,0],[0,0,1]])\n",
        "\n",
        "G = sp.exp(-0.5 * (x - mu).T * Sigma.inv() * (x - mu)) \n",
        "\n",
        "dG_dmu = sp.diff(G, mu)\n",
        "\n",
        "print(\"G:\")\n",
        "display(G)\n",
        "print(\"Derivative of G w.r.t μ (dG/dmu):\")\n",
        "display(sp.simplify(dG_dmu))\n",
        "dG_dmu_num = dG_dmu.subs({x1:2, x2:2, x3:3, mu1:1, mu2:1, mu3:1}).tolist()\n",
        "print(\"Derivative w.r.t μ evaluated at x=[2,2,3], μ=[1,1,1]:\\n\", dG_dmu_num )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Implementação PyTorch:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "x = torch.tensor([[2.0], [2.0], [3.0]], requires_grad=True)\n",
        "mu = torch.tensor([[1.0], [1.0], [1.0]], requires_grad=True)\n",
        "Sigma = torch.eye(3)\n",
        "\n",
        "# Forward pass\n",
        "dx = x - mu\n",
        "g = torch.exp(-0.5 * dx.T @ torch.inverse(Sigma) @ dx)\n",
        "\n",
        "# Backward pass\n",
        "g.backward()\n",
        "\n",
        "# Gradients\n",
        "print(mu.grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Comparação:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dg_dmu = torch.tensor(dG_dmu_num, dtype=torch.float32).reshape(mu.grad.shape)\n",
        "\n",
        "# Gradients\n",
        "print(mu.grad,\"=\\n\", dg_dmu)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Exemplo 10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "$\\displaystyle C(\\mathbf{p}) = \\sum_{i=1}^{N} \\left[ c_i \\ G_i(\\mathbf{p})  \\prod_{j=1}^{i-1} (1- G_j(\\mathbf{p})) \\right]$\n",
        "\n",
        "Onde:\n",
        "- $\\displaystyle G_i(\\mathbf{p}) = exp(-0.5 (\\mathbf{p}-\\mathbf{\\mu_i})^T \\Sigma_i^{'-1} (\\mathbf{p}-\\mathbf{\\mu_i}))$;\n",
        "- $c_i \\in \\mathbb{R}$;\n",
        "- $\\mathbf{\\mu_i},\\mathbf{p} \\in \\mathbb{R}^2$;\n",
        "- $\\Sigma_i' \\in \\mathbb{R}^{2\\times 2}$;\n",
        "- $\\mathbf{\\mu_i}$ são parâmetros a ser atualizados pelo gradiente descendente.\n",
        "\n",
        "$\\displaystyle \\nabla(\\mu) = \\frac{\\partial C}{\\partial \\mu} = \\sum_{i=1}^{N} \\left[ c_i \\ \\frac{\\partial G_i}{\\partial \\mu} \\prod_{j=1}^{i-1} (1- G_j)  + c_i G_i \\sum_{k=1}^{i-1} \\left(\\prod_{j=1}^{k-1}  (1- G_j)\\right) \\left(-\\frac{\\partial G_k}{\\partial \\mu}\\right) \\left(\\prod_{j=k+1}^{i-1}  (1- G_j)\\right) \\right]$\n",
        "\n",
        "Para $N=3, \\mathbf{p}=[2,3],\\ c_i=1/(i+1),\\ \\mathbf{\\mu_i}=[1+i,1+i] \\ $ e $\\Sigma_i^{'-1} = I \\rightarrow$\n",
        "\n",
        "$\\nabla(\\mu) = ?$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Implementação SymPy:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sympy as sp\n",
        "\n",
        "N = 3  # Number of components\n",
        "Sigma_inv_i = sp.Matrix([[1.0, 0.0], [0.0, 1.0]]) # assumir conhecida por simplicidade\n",
        "Sigma_inv_j = sp.Matrix([[1.0, 0.0], [0.0, 1.0]])\n",
        "\n",
        "# variam para n=1,...,N\n",
        "c = sp.IndexedBase('c')\n",
        "mux = sp.IndexedBase('μ_x')\n",
        "muy = sp.IndexedBase('μ_y')\n",
        "\n",
        "# Ponto de pesquisa\n",
        "px, py= sp.symbols('p_x p_y')\n",
        "p = sp.Matrix([px, py])\n",
        "\n",
        "# Generate values to apply to the derivative\n",
        "values = {px:2, py:3}\n",
        "for i in range(N):\n",
        "    values = values | {mux[i]:1+i, muy[i]:1+i, c[i]:1/(i+1)}\n",
        "\n",
        "# Expression for C(x)\n",
        "C = 0 \n",
        "for i in range(0,N):\n",
        "    d_i = p - sp.Matrix([mux[i], muy[i]])\n",
        "    G_i = sp.exp(-0.5 * (d_i.T * Sigma_inv_i * d_i)[0, 0])\n",
        "    prod = 1\n",
        "    for j in range(0, i):\n",
        "        d_j = p - sp.Matrix([mux[j], muy[j]])\n",
        "        G_j = sp.exp(-0.5 * (d_j.T * Sigma_inv_j * d_j)[0, 0])\n",
        "        prod *= (1 - G_j)\n",
        "    C += c[i] * G_i * prod\n",
        "\n",
        "print(\"C(p)=\")\n",
        "display(C)\n",
        "print(f\"C(p) evaluated at {values}:\", C.subs(values).evalf())\n",
        "\n",
        "dC_dmu =[]\n",
        "for i in range(N):\n",
        "    dC_dmu_i = sp.diff(C, sp.Matrix([mux[i], muy[i]]))\n",
        "    dC_dmu_num = dC_dmu_i.subs(values).evalf().tolist()\n",
        "    print(f\"Derivative w.r.t μ_{i} evaluated:\", dC_dmu_num )\n",
        "    dC_dmu.append(dC_dmu_num)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Implementação PyTorch:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "N = 3  # Number of components\n",
        "mu = torch.tensor([[1.0+i, 1.0+i] for i in range(N)], requires_grad=True) \n",
        "\n",
        "p = torch.tensor([2.0, 3.0], requires_grad=False)\n",
        "c = torch.tensor([1.0/(i+1) for i in range(N)], requires_grad=False)\n",
        "Sigma_inv_i = torch.tensor([[1.0, 0.0],[0.0, 1.0]], requires_grad=False)\n",
        "Sigma_inv_j = torch.tensor([[1.0, 0.0],[0.0, 1.0]], requires_grad=False)\n",
        "\n",
        "# Forward pass\n",
        "C = torch.tensor(0.0) \n",
        "for i in range(0,N):\n",
        "    d_i = p - mu[i]\n",
        "    G_i = torch.exp(-0.5 * (d_i.T @ Sigma_inv_i @ d_i))\n",
        "    prod = 1\n",
        "    for j in range(0, i):\n",
        "        d_j = p - mu[j]\n",
        "        G_j = torch.exp(-0.5 * (d_j.T @ Sigma_inv_j @ d_j))\n",
        "        prod *= (1 - G_j)\n",
        "    C += c[i] * G_i * prod\n",
        "print(\"C(p)=\", C)\n",
        "# Backward pass\n",
        "C.backward()\n",
        "\n",
        "# Gradients\n",
        "print(mu.grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Comparação:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dC_dmu = torch.tensor(dC_dmu, dtype=torch.float32).reshape(mu.grad.shape)\n",
        "\n",
        "# Gradients\n",
        "print(mu.grad, \"=\\n\", dC_dmu)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Exemplo 11"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "$\\displaystyle C(\\mathbf{p}) = \\sum_{i=1}^{N} \\left[ c_i \\ G_i(\\mathbf{p})  \\prod_{j=1}^{i-1} (1- G_j(\\mathbf{p})) \\right]$\n",
        "\n",
        "Onde:\n",
        "- $G_i(\\mathbf{p}) = exp(-0.5 (\\mathbf{p}-\\mathbf{\\mu_i})^T \\Sigma_i^{'-1} (\\mathbf{p}-\\mathbf{\\mu_i}))$;\n",
        "- $\\Sigma_i' = JWR_iS_iS_i^TR_i^TW^TJ^T$;\n",
        "- $S_i = diag(\\mathbf{s_i})$;\n",
        "- $J \\in \\mathbb{R}^{2 \\times 3}$ e $W,R_i,S_i \\in \\mathbb{R}^{3 \\times 3}$;\n",
        "- $c_i \\in \\mathbb{R}$, $\\mathbf{\\mu_i},\\mathbf{p} \\in \\mathbb{R}^2$ e $\\mathbf{s_i} \\in \\mathbb{R}^3$;\n",
        "- $\\mathbf{\\mu_i}$ e $\\mathbf{s_i}$ serão atualizados pelo gradiente descendente.\n",
        "\n",
        "$\\displaystyle \\nabla(\\mu) = \\frac{\\partial C}{\\partial \\mu} = \\sum_{i=1}^{N} \\left[ c_i \\ \\frac{\\partial G_i}{\\partial \\mu} \\prod_{j=1}^{i-1} (1- G_j)  + c_i G_i \\sum_{k=1}^{i-1} \\left(\\prod_{j=1}^{k-1}  (1- G_j)\\right) \\left(-\\frac{\\partial G_k}{\\partial \\mu}\\right) \\left(\\prod_{j=k+1}^{i-1}  (1- G_j)\\right) \\right]$\n",
        "\n",
        "$\\displaystyle \\nabla(\\mathbf{s}) = \\frac{\\partial C}{\\partial \\mathbf{s}} = \\sum_{i=1}^{N} \\left[ c_i \\ \\frac{\\partial G_i}{\\partial \\mathbf{s}} \\prod_{j=1}^{i-1} (1- G_j)  + c_i G_i \\sum_{k=1}^{i-1} \\left(\\prod_{j=1}^{k-1}  (1- G_j)\\right) \\left(-\\frac{\\partial G_k}{\\partial \\mathbf{s}}\\right) \\left(\\prod_{j=k+1}^{i-1}  (1- G_j)\\right) \\right]$\n",
        "\n",
        "Para $N=3, \\mathbf{p}=[2,3],\\ c_i=1/(i+1),\\ \\mathbf{\\mu_i}=[1+i,1+i],\\ \\mathbf{s_i} = [0.5(i+1),0.5(i+1),0.5(i+1)]$, $J = diag(1,1,0), W = I, R_i=I \\rightarrow$\n",
        "\n",
        "$\\nabla(\\mu) = ?$\n",
        "\n",
        "$\\nabla(\\mathbf{s}) = ?$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Implementação SymPy:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sympy as sp\n",
        "\n",
        "N = 3  # Number of components\n",
        "R_i = sp.Matrix([[1.0, 0.0, 0.0], [0.0, 1.0, 0.0],[0.0, 0.0, 1.0]]) # assumir conhecida por simplicidade\n",
        "W = sp.Matrix([[1.0, 0.0, 0.0], [0.0, 1.0, 0.0],[0.0, 0.0, 1.0]])\n",
        "J = sp.Matrix([[1.0, 0.0, 0.0], [0.0, 1.0, 0.0]])\n",
        "\n",
        "# variam para n=1,...,N\n",
        "c = sp.IndexedBase('c')\n",
        "mux = sp.IndexedBase('μ_x')\n",
        "muy = sp.IndexedBase('μ_y')\n",
        "sx = sp.IndexedBase('s_x')\n",
        "sy = sp.IndexedBase('s_y')\n",
        "sz = sp.IndexedBase('s_z')\n",
        "\n",
        "# Ponto de pesquisa\n",
        "px, py= sp.symbols('p_x p_y')\n",
        "p = sp.Matrix([px, py])\n",
        "\n",
        "# Generate values to apply to the derivative\n",
        "values = {px:2, py:3}\n",
        "for i in range(N):\n",
        "    values = values | {mux[i]:1+i, muy[i]:1+i, c[i]:1/(i+1), sx[i]:0.5*(i+1), sy[i]:0.5*(i+1), sz[i]:0.5*(i+1)}\n",
        "\n",
        "# Expression for C(x)\n",
        "C = 0 \n",
        "for i in range(0,N):\n",
        "    d_i = p - sp.Matrix([mux[i], muy[i]])\n",
        "    S_i = sp.Matrix([[sx[i], 0, 0], [0, sy[i], 0], [0, 0, sz[i]]])\n",
        "    Sigma_2D_i = J * W * R_i * S_i * S_i.T * R_i.T * W.T * J.T\n",
        "    Sigma_2D_inv_i = Sigma_2D_i.inv()\n",
        "    G_i = sp.exp(-0.5 * (d_i.T * Sigma_2D_inv_i * d_i)[0, 0])\n",
        "    prod = 1\n",
        "    for j in range(0, i):\n",
        "        d_j = p - sp.Matrix([mux[j], muy[j]])\n",
        "        S_j = sp.Matrix([[sx[j], 0, 0], [0, sy[j], 0], [0, 0, sz[j]]])\n",
        "        Sigma_2D_j = J * W * R_i * S_j * S_j.T * R_i.T * W.T * J.T\n",
        "        Sigma_2D_inv_j = Sigma_2D_j.inv()\n",
        "        G_j = sp.exp(-0.5 * (d_j.T * Sigma_2D_inv_j * d_j)[0, 0])\n",
        "        prod *= (1 - G_j)\n",
        "    C += c[i] * G_i * prod\n",
        "\n",
        "print(\"C(p)=\")\n",
        "display(C)\n",
        "\n",
        "print(f\"Evaluation data: {values}:\")\n",
        "print(f\"C(p) evaluated:\", C.subs(values).evalf())\n",
        "\n",
        "dC_dmu = []\n",
        "for i in range(N):\n",
        "    dC_dmu_i = sp.diff(C, sp.Matrix([mux[i], muy[i]]))\n",
        "    dC_dmu_num = dC_dmu_i.subs(values).evalf().tolist()\n",
        "    print(f\"Derivative w.r.t μ_{i} evaluated:\", dC_dmu_num )\n",
        "    dC_dmu.append(dC_dmu_num)\n",
        "\n",
        "dC_ds = []\n",
        "for i in range(N):\n",
        "    dC_ds_i = sp.diff(C, sp.Matrix([sx[i], sy[i], sz[i]]))\n",
        "    dC_ds_num = dC_ds_i.subs(values).evalf().tolist()\n",
        "    print(f\"Derivative w.r.t s_{i} evaluated:\", dC_ds_num )\n",
        "    dC_ds.append(dC_ds_num)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Implementação PyTorch:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "N = 3  # Number of components\n",
        "mu = torch.tensor([[1.0+i, 1.0+i] for i in range(N)], requires_grad=True)\n",
        "s = torch.tensor([[0.5*(i+1), 0.5*(i+1), 0.5*(i+1)] for i in range(N)], requires_grad=True) \n",
        "\n",
        "p = torch.tensor([2.0, 3.0], requires_grad=False)\n",
        "c = torch.tensor([1.0/(i+1) for i in range(N)], requires_grad=False)\n",
        "R_i = torch.tensor([[1.0, 0.0, 0.0], [0.0, 1.0, 0.0],[0.0, 0.0, 1.0]], requires_grad=False)\n",
        "W = torch.tensor([[1.0, 0.0, 0.0], [0.0, 1.0, 0.0],[0.0, 0.0, 1.0]], requires_grad=False)\n",
        "J = torch.tensor([[1.0, 0.0, 0.0], [0.0, 1.0, 0.0]], requires_grad=False)\n",
        "\n",
        "# Forward pass\n",
        "C = torch.tensor(0.0) \n",
        "for i in range(N):\n",
        "    d_i = p - mu[i]\n",
        "    S_i = torch.diag(s[i])\n",
        "    Sigma_2D_i = J @ W @ R_i @ S_i @ S_i.T @ R_i.T @ W.T @ J.T\n",
        "    Sigma_2D_inv_i = torch.linalg.inv(Sigma_2D_i)\n",
        "    G_i = torch.exp(-0.5 * (d_i.T @ Sigma_2D_inv_i @ d_i))\n",
        "    prod = 1\n",
        "    for j in range(i):\n",
        "        d_j = p - mu[j]\n",
        "        S_j = torch.diag(s[j])\n",
        "        Sigma_2D_j = J @ W @ R_i @ S_j @ S_j.T @ R_i.T @ W.T @ J.T\n",
        "        Sigma_2D_inv_j = torch.linalg.inv(Sigma_2D_j)\n",
        "        G_j = torch.exp(-0.5 * (d_j.T @ Sigma_2D_inv_j @ d_j))\n",
        "        prod *= (1 - G_j)\n",
        "    C += c[i] * G_i * prod\n",
        "print(\"C(p) =\", C)\n",
        "\n",
        "# Backward pass\n",
        "C.backward()\n",
        "\n",
        "# Gradients\n",
        "print(mu.grad)\n",
        "print(s.grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Comparação:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dC_dmu = torch.tensor(dC_dmu, dtype=torch.float32).reshape(mu.grad.shape)\n",
        "dC_ds = torch.tensor(dC_ds, dtype=torch.float32).reshape(s.grad.shape)\n",
        "\n",
        "# Gradients\n",
        "print(mu.grad, \"=\\n\", dC_dmu, \"\\n\")\n",
        "print(s.grad, \"=\\n\", dC_ds, \"\\n\" )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Exemplo 12"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- $\\displaystyle C(\\mathbf{p_k}) = \\sum_{i=1}^{N} \\left[ c_i \\ G_i(\\mathbf{p_k})  \\prod_{j=1}^{i-1} (1- G_j(\\mathbf{p_k})) \\right]$;\n",
        "\n",
        "Onde:\n",
        "- $G_i(\\mathbf{p_k}) = exp(-0.5 (\\mathbf{p_k}-\\mathbf{\\mu}^{2D}_i)^T \\Sigma_i^{'-1} (\\mathbf{p_k}-\\mathbf{\\mu}^{2D}_i))$;\n",
        "- $\\mu^{2D}_{i} = \\begin{bmatrix}\\frac{μ^{3D}_{ix} {P}_{0,0} + μ^{3D}_{iy} {P}_{0,1} + μ^{3D}_{iz} {P}_{0,2} + {P}_{0,3}}{μ^{3D}_{ix} {P}_{2,0} + μ^{3D}_{iy} {P}_{2,1} + μ^{3D}_{iz} {P}_{2,2} + {P}_{2,3}}\\\\\\frac{μ^{3D}_{ix} {P}_{1,0} + μ^{3D}_{iy} {P}_{1,1} + μ^{3D}_{iz} {P}_{1,2} + {P}_{1,3}}{μ^{3D}_{ix} {P}_{2,0} + μ^{3D}_{iy} {P}_{2,1} + μ^{3D}_{iz} {P}_{2,2} + {P}_{2,3}}\\end{bmatrix}$\n",
        "- $\\Sigma_i' = JWR_iS_iS_i^TR_i^TW^TJ^T$;\n",
        "- $S_i = diag(\\mathbf{s}_{i}) = \\begin{bmatrix} s_{ix} & 0 & 0 \\\\ 0 & s_{iy} &0 \\\\ 0 & 0 & s_{iz} \\end{bmatrix}$;\n",
        "- $J \\in \\mathbb{R}^{2 \\times 3}$ e $W,R_i,S_i \\in \\mathbb{R}^{3 \\times 3}$;\n",
        "- $c_i, P_{m,n} \\in \\mathbb{R}$, $\\mathbf{\\mu}^{2D}_i,\\mathbf{p_k} \\in \\mathbb{R}^2$ e $\\mathbf{\\mu}^{3D}_i, \\mathbf{s}_{i} \\in \\mathbb{R}^3$;\n",
        "- $\\mathbf{\\mu}^{3D}_{i}$ e $\\mathbf{s}_{i}$ serão atualizados pelo gradiente descendente.\n",
        "\n",
        "$\\displaystyle \\nabla(μ^{3D}) = \\frac{\\partial C}{\\partial μ^{3D}} = \\sum_{i=1}^{N} \\left[ c_i \\ \\frac{\\partial G_i}{\\partial μ^{3D}} \\prod_{j=1}^{i-1} (1- G_j)  + c_i G_i \\sum_{k=1}^{i-1} \\left(\\prod_{j=1}^{k-1}  (1- G_j)\\right) \\left(-\\frac{\\partial G_k}{\\partial μ^{3D}}\\right) \\left(\\prod_{j=k+1}^{i-1}  (1- G_j)\\right) \\right]$\n",
        "\n",
        "$\\displaystyle \\nabla(\\mathbf{s}) = \\frac{\\partial C}{\\partial \\mathbf{s}} = \\sum_{i=1}^{N} \\left[ c_i \\ \\frac{\\partial G_i}{\\partial \\mathbf{s}} \\prod_{j=1}^{i-1} (1- G_j)  + c_i G_i \\sum_{k=1}^{i-1} \\left(\\prod_{j=1}^{k-1}  (1- G_j)\\right) \\left(-\\frac{\\partial G_k}{\\partial \\mathbf{s}}\\right) \\left(\\prod_{j=k+1}^{i-1}  (1- G_j)\\right) \\right]$\n",
        "\n",
        "Para $N=3, \\mathbf{p}=[2,3],\\ c_i=1/(i+1),\\ \\mathbf{\\mu^{3D}_i}=[1+i,1+i,1+i],\\ \\mathbf{s_i} = [0.5(i+1),0.5(i+1),0.5(i+1)]$, $J = diag(1,1,0), P = diag(1,1,1,0), W = I, R_i=I \\rightarrow$\n",
        "\n",
        "$\\nabla(\\mu) = ?$\n",
        "\n",
        "$\\nabla(\\mathbf{s}) = ?$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Implementação SymPy:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sympy as sp\n",
        "\n",
        "N = 3  # Number of components\n",
        "R_i = sp.Matrix([[1.0, 0.0, 0.0], [0.0, 1.0, 0.0],[0.0, 0.0, 1.0]]) # assumir conhecida por simplicidade\n",
        "W = sp.Matrix([[1.0, 0.0, 0.0], [0.0, 1.0, 0.0],[0.0, 0.0, 1.0]])\n",
        "J = sp.Matrix([[1.0, 0.0, 0.0], [0.0, 1.0, 0.0]])\n",
        "P = sp.Matrix([[1000.0, 0.0, 500.0, 300.0], [0.0, 1000.0, 500.0, 300.0], [300.0, 300.0, 300.0, 300.0]])\n",
        "\n",
        "# variam para n=1,...,N\n",
        "c = sp.IndexedBase('c')\n",
        "\n",
        "mu3Dx = sp.IndexedBase('μ_x')\n",
        "mu3Dy = sp.IndexedBase('μ_y')\n",
        "mu3Dz = sp.IndexedBase('μ_z')\n",
        "\n",
        "sx = sp.IndexedBase('s_x')\n",
        "sy = sp.IndexedBase('s_y')\n",
        "sz = sp.IndexedBase('s_z')\n",
        "\n",
        "# Coordenada do ponto de pesquisa\n",
        "p_k = sp.Matrix([[i, j] for i in range(16) for j in range(16)])\n",
        "\n",
        "# Generate values to apply to the derivative\n",
        "values = {}\n",
        "for i in range(N):\n",
        "    values = values | {mu3Dx[i]: 1+i, mu3Dy[i]: 1+i, mu3Dz[i]: 1+i,\n",
        "                       sx[i]:0.5*(i+1), sy[i]:0.5*(i+1), sz[i]:0.5*(i+1),\n",
        "                       c[i]:1/(i+1)}\n",
        "def C_(p):\n",
        "    # Expression for C(x)\n",
        "    C_values = 0 \n",
        "    for i in range(0,N):\n",
        "        mu2Dx_i = (mu3Dx[i] * P[0, 0] + mu3Dy[i] * P[0, 1] + mu3Dz[i] * P[0, 2] + P[0, 3])/(mu3Dx[i] * P[2, 0] + mu3Dy[i] * P[2,1] + mu3Dz[i] * P[2, 2] + P[2, 3])\n",
        "        mu2Dy_i = (mu3Dx[i] * P[1, 0] + mu3Dy[i] * P[1, 1] + mu3Dz[i] * P[1, 2] + P[1, 3])/(mu3Dx[i] * P[2, 0] + mu3Dy[i] * P[2,1] + mu3Dz[i] * P[2, 2] + P[2, 3])\n",
        "        d_i = p - sp.Matrix([mu2Dx_i, mu2Dy_i])\n",
        "        S_i = sp.Matrix([[sx[i], 0, 0], [0, sy[i], 0], [0, 0, sz[i]]])\n",
        "        Sigma_2D_i = J * W * R_i * S_i * S_i.T * R_i.T * W.T * J.T\n",
        "        Sigma_2D_inv_i = Sigma_2D_i.inv()\n",
        "        G_i = sp.exp(-0.5 * (d_i.T * Sigma_2D_inv_i * d_i)[0, 0])\n",
        "        prod = 1\n",
        "        for j in range(0, i):\n",
        "            mu2Dx_j = (mu3Dx[j] * P[0, 0] + mu3Dy[j] * P[0, 1] + mu3Dz[j] * P[0, 2] + P[0, 3])/(mu3Dx[j] * P[2, 0] + mu3Dy[j] * P[2,1] + mu3Dz[j] * P[2, 2] + P[2, 3])\n",
        "            mu2Dy_j = (mu3Dx[j] * P[1, 0] + mu3Dy[j] * P[1, 1] + mu3Dz[j] * P[1, 2] + P[1, 3])/(mu3Dx[j] * P[2, 0] + mu3Dy[j] * P[2,1] + mu3Dz[j] * P[2, 2] + P[2, 3])\n",
        "            d_j = p - sp.Matrix([mu2Dx_j, mu2Dy_j])\n",
        "            S_j = sp.Matrix([[sx[j], 0, 0], [0, sy[j], 0], [0, 0, sz[j]]])\n",
        "            Sigma_2D_j = J * W * R_i * S_j * S_j.T * R_i.T * W.T * J.T\n",
        "            Sigma_2D_inv_j = Sigma_2D_j.inv()\n",
        "            G_j = sp.exp(-0.5 * (d_j.T * Sigma_2D_inv_j * d_j)[0, 0])\n",
        "            prod *= (1 - G_j)\n",
        "        C_values += c[i] * G_i * prod\n",
        "    return C_values\n",
        "\n",
        "C = C_(p_k.row(16*2+3).T)\n",
        "\n",
        "print(\"C(p)=\")\n",
        "display(C)\n",
        "\n",
        "print(f\"Evaluation data: {values}:\")\n",
        "print(f\"C evaluated:\", C.subs(values).evalf())\n",
        "\n",
        "dC_dmu = []\n",
        "for i in range(N):\n",
        "    dC_dmu_i = sp.diff(C, sp.Matrix([mu3Dx[i], mu3Dy[i], mu3Dz[i]]))\n",
        "    dC_dmu_num = dC_dmu_i.subs(values).evalf().tolist()\n",
        "    print(f\"Derivative w.r.t μ_{i} evaluated:\", dC_dmu_num )\n",
        "    dC_dmu.append(dC_dmu_num)\n",
        "\n",
        "dC_ds = []\n",
        "for i in range(N):\n",
        "    dC_ds_i = sp.diff(C, sp.Matrix([sx[i], sy[i], sz[i]]))\n",
        "    dC_ds_num = dC_ds_i.subs(values).evalf().tolist()\n",
        "    print(f\"Derivative w.r.t s_{i} evaluated:\", dC_ds_num )\n",
        "    dC_ds.append(dC_ds_num)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Implementação PyTorch:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "N = 3  # Number of components\n",
        "mu3D = torch.tensor([[1.0+i, 1.0+i, 1.0+i] for i in range(N)], requires_grad=True)\n",
        "s = torch.tensor([[0.5*(i+1), 0.5*(i+1), 0.5*(i+1)] for i in range(N)], requires_grad=True) \n",
        "p_k = torch.tensor([[i, j] for i in range(16) for j in range(16)], requires_grad=False)\n",
        "c = torch.tensor([1.0/(i+1) for i in range(N)], requires_grad=False)\n",
        "\n",
        "R_i = torch.tensor([[1.0, 0.0, 0.0], [0.0, 1.0, 0.0],[0.0, 0.0, 1.0]], requires_grad=False)\n",
        "W = torch.tensor([[1.0, 0.0, 0.0], [0.0, 1.0, 0.0],[0.0, 0.0, 1.0]], requires_grad=False)\n",
        "J = torch.tensor([[1.0, 0.0, 0.0], [0.0, 1.0, 0.0]], requires_grad=False)\n",
        "P = torch.tensor([[1000.0, 0.0, 500.0, 300.0], [0.0, 1000.0, 500.0, 300.0], [300.0, 300.0, 300.0, 300.0]], requires_grad=False)\n",
        "\n",
        "#Forward pass\n",
        "def C_(p):\n",
        "    # Expression for C(x)\n",
        "    C_values = torch.tensor(0.0) \n",
        "    for i in range(N):\n",
        "        mu2Dx_i = (mu3D[i,0] * P[0, 0] + mu3D[i,1] * P[0, 1] + mu3D[i,2] * P[0, 2] + P[0, 3])/(mu3D[i,0] * P[2, 0] + mu3D[i,1] * P[2,1] + mu3D[i,2] * P[2, 2] + P[2, 3])\n",
        "        mu2Dy_i = (mu3D[i,0] * P[1, 0] + mu3D[i,1] * P[1, 1] + mu3D[i,2] * P[1, 2] + P[1, 3])/(mu3D[i,0] * P[2, 0] + mu3D[i,1] * P[2,1] + mu3D[i,2] * P[2, 2] + P[2, 3])\n",
        "        d_i = p - torch.stack([mu2Dx_i, mu2Dy_i])\n",
        "        S_i = torch.diag(s[i])\n",
        "        Sigma_2D_i = J @ W @ R_i @ S_i @ S_i.T @ R_i.T @ W.T @ J.T\n",
        "        Sigma_2D_inv_i = torch.linalg.inv(Sigma_2D_i)\n",
        "        G_i = torch.exp(-0.5 * (d_i.T @ Sigma_2D_inv_i @ d_i))\n",
        "        prod = 1\n",
        "        for j in range(i):\n",
        "            mu2Dx_j = (mu3D[j,0] * P[0, 0] + mu3D[j,1] * P[0, 1] + mu3D[j,2] * P[0, 2] + P[0, 3])/(mu3D[j,0] * P[2, 0] + mu3D[j,1] * P[2,1] + mu3D[j,2] * P[2, 2] + P[2, 3])\n",
        "            mu2Dy_j = (mu3D[j,0] * P[1, 0] + mu3D[j,1] * P[1, 1] + mu3D[j,2] * P[1, 2] + P[1, 3])/(mu3D[j,0] * P[2, 0] + mu3D[j,1] * P[2,1] + mu3D[j,2] * P[2, 2] + P[2, 3])\n",
        "            d_j = p - torch.stack([mu2Dx_j, mu2Dy_j])\n",
        "            S_j = torch.diag(s[j])\n",
        "            Sigma_2D_j = J @ W @ R_i @ S_j @ S_j.T @ R_i.T @ W.T @ J.T\n",
        "            Sigma_2D_inv_j = torch.linalg.inv(Sigma_2D_j)\n",
        "            G_j = torch.exp(-0.5 * (d_j.T @ Sigma_2D_inv_j @ d_j))\n",
        "            prod *= (1 - G_j)\n",
        "        C_values += c[i] * G_i * prod\n",
        "    return C_values\n",
        "C = C_(p_k[16*2+3])\n",
        "print(\"C(p) =\", C)\n",
        "\n",
        "# Backward pass\n",
        "C.backward()\n",
        "\n",
        "# Gradients\n",
        "print(mu3D.grad)\n",
        "print(s.grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Comparação:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dC_dmu = torch.tensor(dC_dmu, dtype=torch.float32).reshape(mu3D.grad.shape)\n",
        "dC_ds = torch.tensor(dC_ds, dtype=torch.float32).reshape(s.grad.shape)\n",
        "\n",
        "# Gradients\n",
        "print(mu3D.grad, \"=\\n\", dC_dmu, \"\\n\")\n",
        "print(s.grad, \"=\\n\", dC_ds, \"\\n\" )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Exemplo 13"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- $\\displaystyle C(\\mathbf{p_k}) = \\sum_{i=1}^{N} \\left[ c_i \\ G_i(\\mathbf{p_k})  \\prod_{j=1}^{i-1} (1- G_j(\\mathbf{p_k})) \\right]$;\n",
        "\n",
        "Onde:\n",
        "- $\\displaystyle c_i = \\sum_{l=0}^{l_{max}}\\sum_{m=-l}^{l}k_l^mY_l^m$;\n",
        "- $Y_l^m$ é uma função harmônica esférica de grau $l$ e ordem $m$;\n",
        "- $G_i(\\mathbf{p_k}) = exp(-0.5 (\\mathbf{p_k}-\\mathbf{\\mu}^{2D}_i)^T \\Sigma_i^{'-1} (\\mathbf{p_k}-\\mathbf{\\mu}^{2D}_i))$;\n",
        "\n",
        "- $\\mu^{2D}_{i} = \\begin{bmatrix}\\frac{μ^{3D}_{ix} {P}_{0,0} + μ^{3D}_{iy} {P}_{0,1} + μ^{3D}_{iz} {P}_{0,2} + {P}_{0,3}}{μ^{3D}_{ix} {P}_{2,0} + μ^{3D}_{iy} {P}_{2,1} + μ^{3D}_{iz} {P}_{2,2} + {P}_{2,3}}\\\\\\frac{μ^{3D}_{ix} {P}_{1,0} + μ^{3D}_{iy} {P}_{1,1} + μ^{3D}_{iz} {P}_{1,2} + {P}_{1,3}}{μ^{3D}_{ix} {P}_{2,0} + μ^{3D}_{iy} {P}_{2,1} + μ^{3D}_{iz} {P}_{2,2} + {P}_{2,3}}\\end{bmatrix}$ , $P_{m,n}$ são as entradas de uma matriz $P \\in \\mathbb{R}^{3 \\times 4}$;\n",
        "- $\\Sigma_i' = JWR_iS_iS_i^TR_i^TW^TJ^T$;\n",
        "- $S_i = diag(\\mathbf{s}_{i}) = \\begin{bmatrix} s_{ix} & 0 & 0 \\\\ 0 & s_{iy} &0 \\\\ 0 & 0 & s_{iz} \\end{bmatrix}$;\n",
        "- $J \\in \\mathbb{R}^{2 \\times 3}$ e $W,R_i \\in \\mathbb{R}^{3 \\times 3}$;\n",
        "- ${k_{li}^m} \\in \\mathbb{R}$, $\\mathbf{\\mu}^{2D}_i,\\mathbf{p_k} \\in \\mathbb{R}^2$ e $cp, \\mathbf{\\mu}^{3D}_i, \\mathbf{s}_{i} \\in \\mathbb{R}^3$;\n",
        "- $\\mathbf{\\mu}^{3D}_{i}, {k_{li}^m}$ e $\\mathbf{s}_{i}$ serão atualizados pelo gradiente descendente.\n",
        "\n",
        "$\\displaystyle \\nabla(μ^{3D}) = \\frac{\\partial C}{\\partial μ^{3D}} = \\sum_{i=1}^{N} \\left[ c_i \\ \\frac{\\partial G_i}{\\partial μ^{3D}} \\prod_{j=1}^{i-1} (1- G_j)  + c_i G_i \\sum_{k=1}^{i-1} \\left(\\prod_{j=1}^{k-1}  (1- G_j)\\right) \\left(-\\frac{\\partial G_k}{\\partial μ^{3D}}\\right) \\left(\\prod_{j=k+1}^{i-1}  (1- G_j)\\right) \\right]$\n",
        "\n",
        "$\\displaystyle \\nabla(\\mathbf{s}) = \\frac{\\partial C}{\\partial \\mathbf{s}} = \\sum_{i=1}^{N} \\left[ c_i \\ \\frac{\\partial G_i}{\\partial \\mathbf{s}} \\prod_{j=1}^{i-1} (1- G_j)  + c_i G_i \\sum_{k=1}^{i-1} \\left(\\prod_{j=1}^{k-1}  (1- G_j)\\right) \\left(-\\frac{\\partial G_k}{\\partial \\mathbf{s}}\\right) \\left(\\prod_{j=k+1}^{i-1}  (1- G_j)\\right) \\right]$\n",
        "\n",
        "$\\displaystyle \\nabla(k) = \\frac{\\partial C}{\\partial k} = \\sum_{i=1}^{N} \\left[ \\frac{\\partial c_i}{\\partial k} \\ G_i \\prod_{j=1}^{i-1} (1- G_j) \\right]$\n",
        "\n",
        "Para $N=3, l_{max}=1, k_{li}^m=1/(i+1), cp = [0,0,0],  \\mathbf{p}_k=[2,3],\\ \\mathbf{\\mu^{3D}_i}=[1+i,1+i,1+i],\\ \\mathbf{s_i} = [0.5(i+1),0.5(i+1),0.5(i+1)]$, $J = diag(1,1,0), P = diag(1,1,1,0), W = I, R_i=I \\rightarrow$\n",
        "\n",
        "$\\nabla(\\mu) = ?$\n",
        "\n",
        "$\\nabla(\\mathbf{s}) = ?$\n",
        "\n",
        "$\\nabla(k) = ?$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Implementação SymPy:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sympy as sp\n",
        "\n",
        "def build_color(sh, rays_d):\n",
        "    lmax = 1  # Maximum spherical harmonics degree\n",
        "    C0 = 0.28209479177387814\n",
        "    C1 = 0.4886025119029199\n",
        "\n",
        "    color = C0 * sh[:, 0][0]\n",
        "    if lmax > 0:\n",
        "        x, y, z = rays_d[0, :][0], rays_d[1, :][0], rays_d[2, :][0]\n",
        "        color = (color -\n",
        "                C1 * y * sh[:, 1][0] +\n",
        "                C1 * z * sh[:, 2][0] -\n",
        "                C1 * x * sh[:, 3][0])\n",
        "    \n",
        "    color = (color + 0.5)\n",
        "    return color\n",
        "\n",
        "N = 3  # Number of components\n",
        "R_i = sp.Matrix([[1.0, 0.0, 0.0], [0.0, 1.0, 0.0],[0.0, 0.0, 1.0]]) # assumir conhecida por simplicidade\n",
        "W = sp.Matrix([[1.0, 0.0, 0.0], [0.0, 1.0, 0.0],[0.0, 0.0, 1.0]])\n",
        "J = sp.Matrix([[1.0, 0.0, 0.0], [0.0, 1.0, 0.0]])\n",
        "P = sp.Matrix([[1000.0, 0.0, 500.0, 300.0], [0.0, 1000.0, 500.0, 300.0], [300.0, 300.0, 300.0, 300.0]])\n",
        "cp = sp.Matrix([0.0, 0.0, 0.0])\n",
        "\n",
        "\n",
        "# variam para n=1,...,N\n",
        "sh0 = sp.IndexedBase('sh_0') # k_0^0   <-- grau_max = 0 \n",
        "sh1 = sp.IndexedBase('sh_1') # k_1^{-1}\n",
        "sh2 = sp.IndexedBase('sh_2') # k_1^{0}\n",
        "sh3 = sp.IndexedBase('sh_3') # k_1^{1} <-- grau_max = 1 \n",
        "\n",
        "mu3Dx = sp.IndexedBase('μ_x')\n",
        "mu3Dy = sp.IndexedBase('μ_y')\n",
        "mu3Dz = sp.IndexedBase('μ_z')\n",
        "\n",
        "sx = sp.IndexedBase('s_x')\n",
        "sy = sp.IndexedBase('s_y')\n",
        "sz = sp.IndexedBase('s_z')\n",
        "\n",
        "# Coordenada do ponto de pesquisa\n",
        "p_k = sp.Matrix([[i, j] for i in range(16) for j in range(16)])\n",
        "\n",
        "# Generate values to apply to the derivative\n",
        "values = {}\n",
        "for i in range(N):\n",
        "    values = values | {mu3Dx[i]: 1+i, mu3Dy[i]: 1+i, mu3Dz[i]: 1+i,\n",
        "                       sx[i]:0.5*(i+1), sy[i]:0.5*(i+1), sz[i]:0.5*(i+1),\n",
        "                       sh0[i]:1/(i+1),sh1[i]:1/(i+1), sh2[i]:1/(i+1), sh3[i]:1/(i+1) }\n",
        "def C_(p):\n",
        "    # Expression for C(x)\n",
        "    C_values = 0 \n",
        "    for i in range(0,N):   \n",
        "        sh_i = sp.Matrix([sh0[i], sh1[i], sh2[i], sh3[i]])\n",
        "        mu3D_i = sp.Matrix([mu3Dx[i], mu3Dy[i], mu3Dz[i]])\n",
        "        mu2Dx_i = (mu3Dx[i] * P[0, 0] + mu3Dy[i] * P[0, 1] + mu3Dz[i] * P[0, 2] + P[0, 3])/(mu3Dx[i] * P[2, 0] + mu3Dy[i] * P[2,1] + mu3Dz[i] * P[2, 2] + P[2, 3])\n",
        "        mu2Dy_i = (mu3Dx[i] * P[1, 0] + mu3Dy[i] * P[1, 1] + mu3Dz[i] * P[1, 2] + P[1, 3])/(mu3Dx[i] * P[2, 0] + mu3Dy[i] * P[2,1] + mu3Dz[i] * P[2, 2] + P[2, 3])\n",
        "        d_i = p - sp.Matrix([mu2Dx_i, mu2Dy_i])\n",
        "        S_i = sp.Matrix([[sx[i], 0, 0], [0, sy[i], 0], [0, 0, sz[i]]])\n",
        "        Sigma_2D_i = J * W * R_i * S_i * S_i.T * R_i.T * W.T * J.T\n",
        "        Sigma_2D_inv_i = Sigma_2D_i.inv()\n",
        "        G_i = sp.exp(-0.5 * (d_i.T * Sigma_2D_inv_i * d_i)[0, 0])\n",
        "        prod = 1\n",
        "        for j in range(0, i):\n",
        "            mu2Dx_j = (mu3Dx[j] * P[0, 0] + mu3Dy[j] * P[0, 1] + mu3Dz[j] * P[0, 2] + P[0, 3])/(mu3Dx[j] * P[2, 0] + mu3Dy[j] * P[2,1] + mu3Dz[j] * P[2, 2] + P[2, 3])\n",
        "            mu2Dy_j = (mu3Dx[j] * P[1, 0] + mu3Dy[j] * P[1, 1] + mu3Dz[j] * P[1, 2] + P[1, 3])/(mu3Dx[j] * P[2, 0] + mu3Dy[j] * P[2,1] + mu3Dz[j] * P[2, 2] + P[2, 3])\n",
        "            d_j = p - sp.Matrix([mu2Dx_j, mu2Dy_j])\n",
        "            S_j = sp.Matrix([[sx[j], 0, 0], [0, sy[j], 0], [0, 0, sz[j]]])\n",
        "            Sigma_2D_j = J * W * R_i * S_j * S_j.T * R_i.T * W.T * J.T\n",
        "            Sigma_2D_inv_j = Sigma_2D_j.inv()\n",
        "            G_j = sp.exp(-0.5 * (d_j.T * Sigma_2D_inv_j * d_j)[0, 0])\n",
        "            prod *= (1 - G_j)\n",
        "        direction = mu3D_i - cp\n",
        "        c_i = build_color(sh_i.T, direction)\n",
        "        C_values += c_i * G_i * prod\n",
        "    return C_values\n",
        "\n",
        "C = C_(p_k.row(16*2+3).T)\n",
        "\n",
        "print(\"C(p)=\")\n",
        "display(C)\n",
        "\n",
        "print(f\"Evaluation data: {values}:\")\n",
        "print(f\"C evaluated:\", C.subs(values).evalf())\n",
        "\n",
        "dC_dmu = []\n",
        "for i in range(N):\n",
        "    dC_dmu_i = sp.diff(C, sp.Matrix([mu3Dx[i], mu3Dy[i], mu3Dz[i]]))\n",
        "    dC_dmu_num = dC_dmu_i.subs(values).evalf().tolist()\n",
        "    print(f\"Derivative w.r.t μ_{i} evaluated:\", dC_dmu_num )\n",
        "    dC_dmu.append(dC_dmu_num)\n",
        "\n",
        "dC_ds = []\n",
        "for i in range(N):\n",
        "    dC_ds_i = sp.diff(C, sp.Matrix([sx[i], sy[i], sz[i]]))\n",
        "    dC_ds_num = dC_ds_i.subs(values).evalf().tolist()\n",
        "    print(f\"Derivative w.r.t s_{i} evaluated:\", dC_ds_num )\n",
        "    dC_ds.append(dC_ds_num)\n",
        "\n",
        "dC_dsh = []\n",
        "for i in range(N):\n",
        "    dC_dsh_i = sp.diff(C, sp.Matrix([sh0[i], sh1[i], sh2[i], sh3[i]]))\n",
        "    dC_dsh_num = dC_dsh_i.subs(values).evalf().tolist()\n",
        "    print(f\"Derivative w.r.t sh_{i} evaluated:\",dC_dsh_num )\n",
        "    dC_dsh.append(dC_dsh_num)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Implementação PyTorch:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "def build_color(sh_i, rays_d):\n",
        "    lmax = 1  # Maximum spherical harmonics degree\n",
        "    C0 = 0.28209479177387814\n",
        "    C1 = 0.4886025119029199\n",
        "    color = C0 * sh_i[..., 0]\n",
        "    if lmax > 0:\n",
        "        x, y, z = rays_d[0, ...], rays_d[1, ...], rays_d[2, ...]\n",
        "        color = (color -\n",
        "                C1 * y * sh_i[..., 1] +\n",
        "                C1 * z * sh_i[..., 2] -\n",
        "                C1 * x * sh_i[..., 3])\n",
        "    \n",
        "    color = (color + 0.5)\n",
        "    return color\n",
        "\n",
        "N = 3  # Number of components\n",
        "mu3D = torch.tensor([[1.0+i, 1.0+i, 1.0+i] for i in range(N)], requires_grad=True)\n",
        "s = torch.tensor([[0.5*(i+1), 0.5*(i+1), 0.5*(i+1)] for i in range(N)], requires_grad=True) \n",
        "sh = torch.tensor([[1.0/(i+1), 1.0/(i+1), 1.0/(i+1), 1.0/(i+1)] for i in range(N)], requires_grad=True)\n",
        "\n",
        "p_k = torch.tensor([[i, j] for i in range(16) for j in range(16)], requires_grad=False)\n",
        "c = torch.tensor([1.0/(i+1) for i in range(N)], requires_grad=False)\n",
        "R_i = torch.tensor([[1.0, 0.0, 0.0], [0.0, 1.0, 0.0],[0.0, 0.0, 1.0]], requires_grad=False)\n",
        "W = torch.tensor([[1.0, 0.0, 0.0], [0.0, 1.0, 0.0],[0.0, 0.0, 1.0]], requires_grad=False)\n",
        "J = torch.tensor([[1.0, 0.0, 0.0], [0.0, 1.0, 0.0]], requires_grad=False)\n",
        "P = torch.tensor([[1000.0, 0.0, 500.0, 300.0], [0.0, 1000.0, 500.0, 300.0], [300.0, 300.0, 300.0, 300.0]], requires_grad=False)\n",
        "cp = torch.tensor([0.0, 0.0, 0.0], requires_grad=False)\n",
        "\n",
        "#Forward pass\n",
        "def C_(p):\n",
        "    # Expression for C(x)\n",
        "    C_values = torch.tensor(0.0) \n",
        "    for i in range(N):\n",
        "        mu2Dx_i = (mu3D[i,0] * P[0, 0] + mu3D[i,1] * P[0, 1] + mu3D[i,2] * P[0, 2] + P[0, 3])/(mu3D[i,0] * P[2, 0] + mu3D[i,1] * P[2,1] + mu3D[i,2] * P[2, 2] + P[2, 3])\n",
        "        mu2Dy_i = (mu3D[i,0] * P[1, 0] + mu3D[i,1] * P[1, 1] + mu3D[i,2] * P[1, 2] + P[1, 3])/(mu3D[i,0] * P[2, 0] + mu3D[i,1] * P[2,1] + mu3D[i,2] * P[2, 2] + P[2, 3])\n",
        "        d_i = p - torch.stack([mu2Dx_i, mu2Dy_i])\n",
        "        S_i = torch.diag(s[i])\n",
        "        Sigma_2D_i = J @ W @ R_i @ S_i @ S_i.T @ R_i.T @ W.T @ J.T\n",
        "        Sigma_2D_inv_i = torch.linalg.inv(Sigma_2D_i)\n",
        "        G_i = torch.exp(-0.5 * (d_i.T @ Sigma_2D_inv_i @ d_i))\n",
        "        prod = 1\n",
        "        for j in range(i):\n",
        "            mu2Dx_j = (mu3D[j,0] * P[0, 0] + mu3D[j,1] * P[0, 1] + mu3D[j,2] * P[0, 2] + P[0, 3])/(mu3D[j,0] * P[2, 0] + mu3D[j,1] * P[2,1] + mu3D[j,2] * P[2, 2] + P[2, 3])\n",
        "            mu2Dy_j = (mu3D[j,0] * P[1, 0] + mu3D[j,1] * P[1, 1] + mu3D[j,2] * P[1, 2] + P[1, 3])/(mu3D[j,0] * P[2, 0] + mu3D[j,1] * P[2,1] + mu3D[j,2] * P[2, 2] + P[2, 3])\n",
        "            d_j = p - torch.stack([mu2Dx_j, mu2Dy_j])\n",
        "            S_j = torch.diag(s[j])\n",
        "            Sigma_2D_j = J @ W @ R_i @ S_j @ S_j.T @ R_i.T @ W.T @ J.T\n",
        "            Sigma_2D_inv_j = torch.linalg.inv(Sigma_2D_j)\n",
        "            G_j = torch.exp(-0.5 * (d_j.T @ Sigma_2D_inv_j @ d_j))\n",
        "            prod *= (1 - G_j)\n",
        "        direction = mu3D[i] - cp\n",
        "        c_i = build_color(sh[i], direction)\n",
        "        C_values += c_i * G_i * prod\n",
        "    return C_values\n",
        "C = C_(p_k[16*2+3])\n",
        "print(\"C(p) =\", C)\n",
        "\n",
        "# Backward pass\n",
        "C.backward()\n",
        "\n",
        "# Gradients\n",
        "print(mu3D.grad)\n",
        "print(s.grad)\n",
        "print(sh.grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Comparação:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dC_dmu = torch.tensor(dC_dmu, dtype=torch.float32).reshape(mu3D.grad.shape)\n",
        "dC_ds = torch.tensor(dC_ds, dtype=torch.float32).reshape(s.grad.shape)\n",
        "dC_dsh = torch.tensor(dC_dsh, dtype=torch.float32).reshape(sh.grad.shape)\n",
        "\n",
        "# Gradients\n",
        "print(mu3D.grad, \"=\\n\", dC_dmu, \"\\n\")\n",
        "print(s.grad, \"=\\n\", dC_ds, \"\\n\" )\n",
        "print(sh.grad, \"=\\n\", dC_dsh, \"\\n\" )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Exemplo 14"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "$\\displaystyle \\mathcal{L} = L_{recon}(C, C')$\n",
        "\n",
        "Onde:\n",
        "- $C$ e $C'$ são as imagens, reconstruída e prevista respectivamente, vetorizadas com $M$ pixels cada\n",
        "- $\\displaystyle L_{recon}(C, C') =\\sum_{k=1}^{M} || C(\\mathbf{p_k}) - C'(\\mathbf{p_k}) ||^2 $\n",
        "- $\\displaystyle C(\\mathbf{p_k}) = \\sum_{i=1}^{N} \\left[ c_i \\ G_i(\\mathbf{p_k})  \\prod_{j=1}^{i-1} (1- G_j(\\mathbf{p_k})) \\right]$;\n",
        "- $\\displaystyle c_i = \\sum_{l=0}^{l_{max}}\\sum_{m=-l}^{l}k_l^mY_l^m$;\n",
        "- $Y_l^m$ é uma função harmônica esférica de grau $l$ e ordem $m$;\n",
        "- $G_i(\\mathbf{p_k}) = exp(-0.5 (\\mathbf{p_k}-\\mathbf{\\mu}^{2D}_i)^T \\Sigma_i^{'-1} (\\mathbf{p_k}-\\mathbf{\\mu}^{2D}_i))$;\n",
        "\n",
        "- $\\mu^{2D}_{i} = \\begin{bmatrix}\\frac{μ^{3D}_{ix} {P}_{0,0} + μ^{3D}_{iy} {P}_{0,1} + μ^{3D}_{iz} {P}_{0,2} + {P}_{0,3}}{μ^{3D}_{ix} {P}_{2,0} + μ^{3D}_{iy} {P}_{2,1} + μ^{3D}_{iz} {P}_{2,2} + {P}_{2,3}}\\\\\\frac{μ^{3D}_{ix} {P}_{1,0} + μ^{3D}_{iy} {P}_{1,1} + μ^{3D}_{iz} {P}_{1,2} + {P}_{1,3}}{μ^{3D}_{ix} {P}_{2,0} + μ^{3D}_{iy} {P}_{2,1} + μ^{3D}_{iz} {P}_{2,2} + {P}_{2,3}}\\end{bmatrix}$ , $P_{m,n}$ são as entradas de uma matriz $P \\in \\mathbb{R}^{3 \\times 4}$;\n",
        "- $\\Sigma_i' = JWR_iS_iS_i^TR_i^TW^TJ^T$;\n",
        "- $S_i = diag(\\mathbf{s}_{i}) = \\begin{bmatrix} s_{ix} & 0 & 0 \\\\ 0 & s_{iy} &0 \\\\ 0 & 0 & s_{iz} \\end{bmatrix}$;\n",
        "- $J \\in \\mathbb{R}^{2 \\times 3}$ e $W,R_i \\in \\mathbb{R}^{3 \\times 3}$;\n",
        "- ${k_{li}^m} \\in \\mathbb{R}$, $\\mathbf{\\mu}^{2D}_i,\\mathbf{p_k} \\in \\mathbb{R}^2$ e $cp, \\mathbf{\\mu}^{3D}_i, \\mathbf{s}_{i} \\in \\mathbb{R}^3$;\n",
        "- $\\mathbf{\\mu}^{3D}_{i}, {k_{li}^m}$ e $\\mathbf{s}_{i}$ serão atualizados pelo gradiente descendente.\n",
        "\n",
        "$\\displaystyle \\nabla(\\mu^{3D}) = \\frac{\\partial \\mathcal{L}}{\\partial \\mu^{3D}} = \\sum_{k=1}^{256} 2 \\cdot || C - C' || \\cdot || \\frac{\\partial C}{\\partial \\mu^{3D}}|| $\n",
        "\n",
        "$\\displaystyle \\nabla(\\mathbf{s}) = \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{s}} = \\sum_{k=1}^{256} 2 \\cdot || C - C' || \\cdot || \\frac{\\partial C}{\\partial \\mathbf{s}}||$\n",
        "\n",
        "$\\displaystyle \\nabla(k) = \\frac{\\partial \\mathcal{L}}{\\partial k} = \\sum_{k=1}^{256} 2 \\cdot || C - C' || \\cdot || \\frac{\\partial C}{\\partial k}||$\n",
        "\n",
        "Para $N=3, M=4, C'(\\mathbf{p_k}) = k, l_{max}=1, k_{li}^m=1/(i+1), cp = [0,0,0], \\ \\mathbf{\\mu^{3D}_i}=[1+i,1+i,1+i],\\ \\mathbf{s_i} = [0.5(i+1),0.5(i+1),0.5(i+1)]$, $J = diag(1,1,0), P = diag(1,1,1,0), W = I, R_i=I \\rightarrow$\n",
        "\n",
        "$\\nabla(\\mu) = ?$\n",
        "\n",
        "$\\nabla(\\mathbf{s}) = ?$\n",
        "\n",
        "$\\nabla(k) = ?$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Implementação SymPy:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sympy as sp\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "def build_color(sh, rays_d):\n",
        "    lmax = 1  # Maximum spherical harmonics degree\n",
        "    C0 = 0.28209479177387814\n",
        "    C1 = 0.4886025119029199\n",
        "\n",
        "    color = C0 * sh[:, 0][0]\n",
        "    if lmax > 0:\n",
        "        x, y, z = rays_d[0, :][0], rays_d[1, :][0], rays_d[2, :][0]\n",
        "        color = (color -\n",
        "                C1 * y * sh[:, 1][0] +\n",
        "                C1 * z * sh[:, 2][0] -\n",
        "                C1 * x * sh[:, 3][0])\n",
        "    \n",
        "    color = (color + 0.5)\n",
        "    return color\n",
        "\n",
        "N = 3  # Number of gaussians\n",
        "M = 4 # Number of pixels\n",
        "R_i = sp.Matrix([[1.0, 0.0, 0.0], [0.0, 1.0, 0.0],[0.0, 0.0, 1.0]]) # assumir conhecida por simplicidade\n",
        "W = sp.Matrix([[1.0, 0.0, 0.0], [0.0, 1.0, 0.0],[0.0, 0.0, 1.0]])\n",
        "J = sp.Matrix([[1.0, 0.0, 0.0], [0.0, 1.0, 0.0]])\n",
        "P = sp.Matrix([[1000.0, 0.0, 500.0, 300.0], [0.0, 1000.0, 500.0, 300.0], [300.0, 300.0, 300.0, 300.0]])\n",
        "cp = sp.Matrix([0.0, 0.0, 0.0])\n",
        "\n",
        "# variam para n=1,...,N\n",
        "sh0 = sp.IndexedBase('sh_0') # k_0^0   <-- grau_max = 0 \n",
        "sh1 = sp.IndexedBase('sh_1') # k_1^{-1}\n",
        "sh2 = sp.IndexedBase('sh_2') # k_1^{0}\n",
        "sh3 = sp.IndexedBase('sh_3') # k_1^{1} <-- grau_max = 1 \n",
        "\n",
        "mu3Dx = sp.IndexedBase('μ_x')\n",
        "mu3Dy = sp.IndexedBase('μ_y')\n",
        "mu3Dz = sp.IndexedBase('μ_z')\n",
        "\n",
        "sx = sp.IndexedBase('s_x')\n",
        "sy = sp.IndexedBase('s_y')\n",
        "sz = sp.IndexedBase('s_z')\n",
        "\n",
        "# Coordenada do ponto de pesquisa\n",
        "p_k = sp.Matrix([[i, j] for i in range(int(M**(0.5))) for j in range(int(M**(0.5)))])\n",
        "\n",
        "# Observações\n",
        "C_hat = sp.Matrix([[k] for k in range(M)])\n",
        "\n",
        "# Generate values to apply to the derivative\n",
        "values = {}\n",
        "for i in range(N):\n",
        "    values = values | {mu3Dx[i]: 1+i, mu3Dy[i]: 1+i, mu3Dz[i]: 1+i,\n",
        "                       sx[i]:0.5*(i+1), sy[i]:0.5*(i+1), sz[i]:0.5*(i+1),\n",
        "                       sh0[i]:1/(i+1),sh1[i]:1/(i+1), sh2[i]:1/(i+1), sh3[i]:1/(i+1) }\n",
        "def C_(p):\n",
        "    # Expression for C(x)\n",
        "    C_values = 0 \n",
        "    for i in range(0,N):   \n",
        "        sh_i = sp.Matrix([sh0[i], sh1[i], sh2[i], sh3[i]])\n",
        "        mu3D_i = sp.Matrix([mu3Dx[i], mu3Dy[i], mu3Dz[i]])\n",
        "        mu2Dx_i = (mu3Dx[i] * P[0, 0] + mu3Dy[i] * P[0, 1] + mu3Dz[i] * P[0, 2] + P[0, 3])/(mu3Dx[i] * P[2, 0] + mu3Dy[i] * P[2,1] + mu3Dz[i] * P[2, 2] + P[2, 3])\n",
        "        mu2Dy_i = (mu3Dx[i] * P[1, 0] + mu3Dy[i] * P[1, 1] + mu3Dz[i] * P[1, 2] + P[1, 3])/(mu3Dx[i] * P[2, 0] + mu3Dy[i] * P[2,1] + mu3Dz[i] * P[2, 2] + P[2, 3])\n",
        "        d_i = p - sp.Matrix([mu2Dx_i, mu2Dy_i])\n",
        "        S_i = sp.Matrix([[sx[i], 0, 0], [0, sy[i], 0], [0, 0, sz[i]]])\n",
        "        Sigma_2D_i = J * W * R_i * S_i * S_i.T * R_i.T * W.T * J.T\n",
        "        Sigma_2D_inv_i = Sigma_2D_i.inv()\n",
        "        G_i = sp.exp(-0.5 * (d_i.T * Sigma_2D_inv_i * d_i)[0, 0])\n",
        "        prod = 1\n",
        "        for j in range(0, i):\n",
        "            mu2Dx_j = (mu3Dx[j] * P[0, 0] + mu3Dy[j] * P[0, 1] + mu3Dz[j] * P[0, 2] + P[0, 3])/(mu3Dx[j] * P[2, 0] + mu3Dy[j] * P[2,1] + mu3Dz[j] * P[2, 2] + P[2, 3])\n",
        "            mu2Dy_j = (mu3Dx[j] * P[1, 0] + mu3Dy[j] * P[1, 1] + mu3Dz[j] * P[1, 2] + P[1, 3])/(mu3Dx[j] * P[2, 0] + mu3Dy[j] * P[2,1] + mu3Dz[j] * P[2, 2] + P[2, 3])\n",
        "            d_j = p - sp.Matrix([mu2Dx_j, mu2Dy_j])\n",
        "            S_j = sp.Matrix([[sx[j], 0, 0], [0, sy[j], 0], [0, 0, sz[j]]])\n",
        "            Sigma_2D_j = J * W * R_i * S_j * S_j.T * R_i.T * W.T * J.T\n",
        "            Sigma_2D_inv_j = Sigma_2D_j.inv()\n",
        "            G_j = sp.exp(-0.5 * (d_j.T * Sigma_2D_inv_j * d_j)[0, 0])\n",
        "            prod *= (1 - G_j)\n",
        "        direction = mu3D_i - cp\n",
        "        c_i = build_color(sh_i.T, direction)\n",
        "        C_values += c_i * G_i * prod\n",
        "    return C_values\n",
        "\n",
        "L = 0\n",
        "for k in tqdm(range(M)):\n",
        "    L += (C_(p_k.row(k).T) - C_hat[k])**2\n",
        "\n",
        "print(\"L=\")\n",
        "display(L)\n",
        "\n",
        "print(f\"Evaluation data: {values}:\")\n",
        "print(f\"L evaluated:\", L.subs(values).evalf())\n",
        "\n",
        "### Sympy can't handle this size of derivatives...\n",
        "dL_dmu = []\n",
        "for i in tqdm(range(N)):\n",
        "    dL_dmu_i = sp.diff(L, sp.Matrix([mu3Dx[i], mu3Dy[i], mu3Dz[i]]))\n",
        "    dL_dmu_num = dL_dmu_i.subs(values).evalf().tolist()\n",
        "    print(f\"Derivative w.r.t μ_{i} evaluated:\", dL_dmu_num )\n",
        "    dL_dmu.append(dL_dmu_num)\n",
        "\n",
        "dL_ds = []\n",
        "for i in tqdm(range(N)):\n",
        "    dL_ds_i = sp.diff(L, sp.Matrix([sx[i], sy[i], sz[i]]))\n",
        "    dL_ds_num = dL_ds_i.subs(values).evalf().tolist()\n",
        "    print(f\"Derivative w.r.t s_{i} evaluated:\", dL_ds_num )\n",
        "    dL_ds.append(dL_ds_num)\n",
        "\n",
        "dL_dsh = []\n",
        "for i in tqdm(range(N)):\n",
        "    dL_dsh_i = sp.diff(L, sp.Matrix([sh0[i], sh1[i], sh2[i], sh3[i]]))\n",
        "    dL_dsh_num = dL_dsh_i.subs(values).evalf().tolist()\n",
        "    print(f\"Derivative w.r.t sh_{i} evaluated:\",dL_dsh_num )\n",
        "    dL_dsh.append(dL_dsh_num)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Implementação PyTorch:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "def build_color(sh_i, rays_d):\n",
        "    lmax = 1  # Maximum spherical harmonics degree\n",
        "    C0 = 0.28209479177387814\n",
        "    C1 = 0.4886025119029199\n",
        "    color = C0 * sh_i[..., 0]\n",
        "    if lmax > 0:\n",
        "        x, y, z = rays_d[0, ...], rays_d[1, ...], rays_d[2, ...]\n",
        "        color = (color -\n",
        "                C1 * y * sh_i[..., 1] +\n",
        "                C1 * z * sh_i[..., 2] -\n",
        "                C1 * x * sh_i[..., 3])\n",
        "    \n",
        "    color = (color + 0.5)\n",
        "    return color\n",
        "\n",
        "N = 3  # Number of gaussians\n",
        "M = 4 # Number of pixels\n",
        "mu3D = torch.tensor([[1.0+i, 1.0+i, 1.0+i] for i in range(N)], requires_grad=True)\n",
        "s = torch.tensor([[0.5*(i+1), 0.5*(i+1), 0.5*(i+1)] for i in range(N)], requires_grad=True) \n",
        "sh = torch.tensor([[1.0/(i+1), 1.0/(i+1), 1.0/(i+1), 1.0/(i+1)] for i in range(N)], requires_grad=True)\n",
        "\n",
        "p_k = torch.tensor([[i, j] for i in range(int(M**(0.5))) for j in range(int(M**(0.5)))], requires_grad=False)\n",
        "C_hat = torch.tensor([k for k in range(M)])\n",
        "c = torch.tensor([1.0/(i+1) for i in range(N)], requires_grad=False)\n",
        "R_i = torch.tensor([[1.0, 0.0, 0.0], [0.0, 1.0, 0.0],[0.0, 0.0, 1.0]], requires_grad=False)\n",
        "W = torch.tensor([[1.0, 0.0, 0.0], [0.0, 1.0, 0.0],[0.0, 0.0, 1.0]], requires_grad=False)\n",
        "J = torch.tensor([[1.0, 0.0, 0.0], [0.0, 1.0, 0.0]], requires_grad=False)\n",
        "P = torch.tensor([[1000.0, 0.0, 500.0, 300.0], [0.0, 1000.0, 500.0, 300.0], [300.0, 300.0, 300.0, 300.0]], requires_grad=False)\n",
        "cp = torch.tensor([0.0, 0.0, 0.0], requires_grad=False)\n",
        "\n",
        "#Forward pass\n",
        "def C_(p):\n",
        "    # Expression for C(x)\n",
        "    C_values = torch.tensor(0.0) \n",
        "    for i in range(N):\n",
        "        mu2Dx_i = (mu3D[i,0] * P[0, 0] + mu3D[i,1] * P[0, 1] + mu3D[i,2] * P[0, 2] + P[0, 3])/(mu3D[i,0] * P[2, 0] + mu3D[i,1] * P[2,1] + mu3D[i,2] * P[2, 2] + P[2, 3])\n",
        "        mu2Dy_i = (mu3D[i,0] * P[1, 0] + mu3D[i,1] * P[1, 1] + mu3D[i,2] * P[1, 2] + P[1, 3])/(mu3D[i,0] * P[2, 0] + mu3D[i,1] * P[2,1] + mu3D[i,2] * P[2, 2] + P[2, 3])\n",
        "        d_i = p - torch.stack([mu2Dx_i, mu2Dy_i])\n",
        "        S_i = torch.diag(s[i])\n",
        "        Sigma_2D_i = J @ W @ R_i @ S_i @ S_i.T @ R_i.T @ W.T @ J.T\n",
        "        Sigma_2D_inv_i = torch.linalg.inv(Sigma_2D_i)\n",
        "        G_i = torch.exp(-0.5 * (d_i.T @ Sigma_2D_inv_i @ d_i))\n",
        "        prod = 1\n",
        "        for j in range(i):\n",
        "            mu2Dx_j = (mu3D[j,0] * P[0, 0] + mu3D[j,1] * P[0, 1] + mu3D[j,2] * P[0, 2] + P[0, 3])/(mu3D[j,0] * P[2, 0] + mu3D[j,1] * P[2,1] + mu3D[j,2] * P[2, 2] + P[2, 3])\n",
        "            mu2Dy_j = (mu3D[j,0] * P[1, 0] + mu3D[j,1] * P[1, 1] + mu3D[j,2] * P[1, 2] + P[1, 3])/(mu3D[j,0] * P[2, 0] + mu3D[j,1] * P[2,1] + mu3D[j,2] * P[2, 2] + P[2, 3])\n",
        "            d_j = p - torch.stack([mu2Dx_j, mu2Dy_j])\n",
        "            S_j = torch.diag(s[j])\n",
        "            Sigma_2D_j = J @ W @ R_i @ S_j @ S_j.T @ R_i.T @ W.T @ J.T\n",
        "            Sigma_2D_inv_j = torch.linalg.inv(Sigma_2D_j)\n",
        "            G_j = torch.exp(-0.5 * (d_j.T @ Sigma_2D_inv_j @ d_j))\n",
        "            prod *= (1 - G_j)\n",
        "        direction = mu3D[i] - cp\n",
        "        c_i = build_color(sh[i], direction)\n",
        "        C_values += c_i * G_i * prod\n",
        "    return C_values\n",
        "\n",
        "L = torch.tensor(0.0) \n",
        "for k in tqdm(range(M)):\n",
        "    C = C_(p_k[k])\n",
        "    L += ( C - C_hat[k])**2\n",
        "\n",
        "print(\"L=\", L)\n",
        "\n",
        "# Backward pass\n",
        "L.backward()\n",
        "\n",
        "# Gradients\n",
        "print(mu3D.grad)\n",
        "print(s.grad)\n",
        "print(sh.grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Comparação"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dL_dmu = torch.tensor(dL_dmu, dtype=torch.float32).reshape(mu3D.grad.shape)\n",
        "dL_ds = torch.tensor(dL_ds, dtype=torch.float32).reshape(s.grad.shape)\n",
        "dL_dsh = torch.tensor(dL_dsh, dtype=torch.float32).reshape(sh.grad.shape)\n",
        "\n",
        "# Gradients\n",
        "print(mu3D.grad, \"=\\n\", dL_dmu, \"\\n\")\n",
        "print(s.grad, \"=\\n\", dL_ds, \"\\n\" )\n",
        "print(sh.grad, \"=\\n\", dL_dsh, \"\\n\" )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Exemplo 15"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "$\\displaystyle \\mathcal{L}(C, C') = L_{DSSIM}(C, C')$\n",
        "\n",
        "- $C$ e $C'$ são as imagens reconstruída e prevista respectivamente, vetorizadas com $M$ pixels cada\n",
        "\n",
        "- $\\displaystyle L_{DSSIM}(C, C') = 1- \\left(\\frac{2\\overline{C}\\ \\overline{C'}+ 10^{-9}}{\\overline{C}^2 + \\overline{C'}^2 + 10^{-9}}\\right) \\cdot \\left(\\frac{2\\sigma_{CC'}+ 10^{-9}}{\\sigma_{C}^2 + \\sigma_{C'}^2 + 10^{-9}}\\right)$\n",
        "- $\\mathbf{p_k}$ é a coordenada do pixel de posição $k$ em cada imagem vetorizada\n",
        "\n",
        "- $\\overline{C}$, $\\overline{C'}$ são as médias de $C$ e $C'$, ou seja:\n",
        "    - $ \\displaystyle \\overline{C} = \\frac{1}{M} \\sum_{k=1}^{M}  C(\\mathbf{p_k})$\n",
        "    - $ \\displaystyle \\overline{C'} = \\frac{1}{M} \\sum_{k=1}^{M}  C'(\\mathbf{p_k})$\n",
        "- $\\sigma_{C}$, $\\sigma_{C'}$ são as variâncias de $C$ e $C'$, ou seja:\n",
        "    - $ \\displaystyle \\sigma_{C} = \\frac{1}{M} \\sum_{k=1}^{M} (C(\\mathbf{p_k}) - \\overline{C} )^2  $\n",
        "    - $ \\displaystyle \\sigma_{C'} = \\frac{1}{M} \\sum_{k=1}^{M} (C'(\\mathbf{p_k}) - \\overline{C'} )^2  $\n",
        "- $\\sigma_{CC'}$ é a covariância de $C$ e $C'$, ou seja:\n",
        "    - $ \\displaystyle \\sigma_{CC'} = \\frac{1}{M} \\sum_{k=1}^{M} (C(\\mathbf{p_k}) - \\overline{C} )(C'(\\mathbf{p_k}) - \\overline{C'} )  $\n",
        "- $\\displaystyle C(\\mathbf{p_k}) = \\sum_{i=1}^{N} \\left[ c_i \\ G_i(\\mathbf{p_k})  \\prod_{j=1}^{i-1} (1- G_j(\\mathbf{p_k})) \\right]$;\n",
        "- $\\displaystyle c_i = \\sum_{l=0}^{l_{max}}\\sum_{m=-l}^{l}k_l^mY_l^m$;\n",
        "- $Y_l^m$ é uma função harmônica esférica de grau $l$ e ordem $m$;\n",
        "- $G_i(\\mathbf{p_k}) = exp(-0.5 (\\mathbf{p_k}-\\mathbf{\\mu}^{2D}_i)^T \\Sigma_i^{'-1} (\\mathbf{p_k}-\\mathbf{\\mu}^{2D}_i))$;\n",
        "\n",
        "- $\\mu^{2D}_{i} = \\begin{bmatrix}\\frac{μ^{3D}_{ix} {P}_{0,0} + μ^{3D}_{iy} {P}_{0,1} + μ^{3D}_{iz} {P}_{0,2} + {P}_{0,3}}{μ^{3D}_{ix} {P}_{2,0} + μ^{3D}_{iy} {P}_{2,1} + μ^{3D}_{iz} {P}_{2,2} + {P}_{2,3}}\\\\\\frac{μ^{3D}_{ix} {P}_{1,0} + μ^{3D}_{iy} {P}_{1,1} + μ^{3D}_{iz} {P}_{1,2} + {P}_{1,3}}{μ^{3D}_{ix} {P}_{2,0} + μ^{3D}_{iy} {P}_{2,1} + μ^{3D}_{iz} {P}_{2,2} + {P}_{2,3}}\\end{bmatrix}$ , $P_{m,n}$ são as entradas de uma matriz $P \\in \\mathbb{R}^{3 \\times 4}$;\n",
        "- $\\Sigma_i' = JWR_iS_iS_i^TR_i^TW^TJ^T$;\n",
        "- $S_i = diag(\\mathbf{s}_{i}) = \\begin{bmatrix} s_{ix} & 0 & 0 \\\\ 0 & s_{iy} &0 \\\\ 0 & 0 & s_{iz} \\end{bmatrix}$;\n",
        "- $J \\in \\mathbb{R}^{2 \\times 3}$ e $W,R_i \\in \\mathbb{R}^{3 \\times 3}$;\n",
        "- ${k_{li}^m} \\in \\mathbb{R}$, $\\mathbf{\\mu}^{2D}_i,\\mathbf{p_k} \\in \\mathbb{R}^2$ e $cp, \\mathbf{\\mu}^{3D}_i, \\mathbf{s}_{i} \\in \\mathbb{R}^3$;\n",
        "- $\\mathbf{\\mu}^{3D}_{i}, {k_{li}^m}$ e $\\mathbf{s}_{i}$ serão atualizados pelo gradiente descendente.\n",
        "\n",
        "$\\displaystyle \\nabla(\\mu^{3D}) = \\frac{\\partial \\mathcal{L}}{\\partial \\mu^{3D}} =\\frac{\\partial}{\\partial \\mu^{3D}}\\left(\\frac{2\\overline{C}\\ \\overline{C'}+ 10^{-9}}{\\overline{C}^2 + \\overline{C'}^2 + 10^{-9}}\\right) \\cdot \\left(\\frac{2\\sigma_{CC'}+ 10^{-9}}{\\sigma_{C}^2 + \\sigma_{C'}^2 + 10^{-9}}\\right) + \\left(\\frac{2\\overline{C}\\ \\overline{C'}+ 10^{-9}}{\\overline{C}^2 + \\overline{C'}^2 + 10^{-9}}\\right) \\frac{\\partial}{\\partial \\mu^{3D}} \\left(\\frac{2\\sigma_{CC'}+ 10^{-9}}{\\sigma_{C}^2 + \\sigma_{C'}^2 + 10^{-9}}\\right)$\n",
        "\n",
        "$\\displaystyle \\nabla(\\mathbf{s}) = \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{s}}  = \\frac{\\partial}{\\partial \\mathbf{s}}\\left(\\frac{2\\overline{C}\\ \\overline{C'}+ 10^{-9}}{\\overline{C}^2 + \\overline{C'}^2 + 10^{-9}}\\right) \\cdot \\left(\\frac{2\\sigma_{CC'}+ 10^{-9}}{\\sigma_{C}^2 + \\sigma_{C'}^2 + 10^{-9}}\\right) + \\left(\\frac{2\\overline{C}\\ \\overline{C'}+ 10^{-9}}{\\overline{C}^2 + \\overline{C'}^2 + 10^{-9}}\\right) \\frac{\\partial}{\\partial \\mathbf{s}} \\left(\\frac{2\\sigma_{CC'}+ 10^{-9}}{\\sigma_{C}^2 + \\sigma_{C'}^2 + 10^{-9}}\\right)$\n",
        "\n",
        "$\\displaystyle \\nabla(k) = \\frac{\\partial \\mathcal{L}}{\\partial k}  = \\frac{\\partial}{\\partial k}\\left(\\frac{2\\overline{C}\\ \\overline{C'}+ 10^{-9}}{\\overline{C}^2 + \\overline{C'}^2 + 10^{-9}}\\right) \\cdot \\left(\\frac{2\\sigma_{CC'}+ 10^{-9}}{\\sigma_{C}^2 + \\sigma_{C'}^2 + 10^{-9}}\\right) + \\left(\\frac{2\\overline{C}\\ \\overline{C'}+ 10^{-9}}{\\overline{C}^2 + \\overline{C'}^2 + 10^{-9}}\\right) \\frac{\\partial}{\\partial k} \\left(\\frac{2\\sigma_{CC'}+ 10^{-9}}{\\sigma_{C}^2 + \\sigma_{C'}^2 + 10^{-9}}\\right)$\n",
        "\n",
        "Para $N=3, M=4, C'(\\mathbf{p_k}) = k, l_{max}=1, k_{li}^m=1/(i+1), cp = [0,0,0], \\ \\mathbf{\\mu^{3D}_i}=[1+i,1+i,1+i],\\ \\mathbf{s_i} = [0.5(i+1),0.5(i+1),0.5(i+1)]$, $J = diag(1,1,0), P = diag(1,1,1,0), W = I, R_i=I \\rightarrow$\n",
        "\n",
        "$\\nabla(\\mu) = ?$\n",
        "\n",
        "$\\nabla(\\mathbf{s}) = ?$\n",
        "\n",
        "$\\nabla(k) = ?$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Implementação SymPy:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c8b5450d5fb6427ebe760f6df1b035ed",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/4 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.0747213673008819\n",
            "L=\n",
            "Evaluation data: {μ_x[0]: 1, μ_y[0]: 1, μ_z[0]: 1, s_x[0]: 0.5, s_y[0]: 0.5, s_z[0]: 0.5, sh_0[0]: 1.0, sh_1[0]: 1.0, sh_2[0]: 1.0, sh_3[0]: 1.0, μ_x[1]: 2, μ_y[1]: 2, μ_z[1]: 2, s_x[1]: 1.0, s_y[1]: 1.0, s_z[1]: 1.0, sh_0[1]: 0.5, sh_1[1]: 0.5, sh_2[1]: 0.5, sh_3[1]: 0.5, μ_x[2]: 3, μ_y[2]: 3, μ_z[2]: 3, s_x[2]: 1.5, s_y[2]: 1.5, s_z[2]: 1.5, sh_0[2]: 0.3333333333333333, sh_1[2]: 0.3333333333333333, sh_2[2]: 0.3333333333333333, sh_3[2]: 0.3333333333333333}:\n",
            "L evaluated: 0.992887347678695\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "72f0bc74662a4131a4acd0fe0b5a6811",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Derivative w.r.t μ_0 evaluated: [[0.0124251889940151], [0.0125779356047527], [-0.0111994942393178]]\n",
            "Derivative w.r.t μ_1 evaluated: [[0.00863845206681180], [0.00899944377054137], [-0.00873734297318112]]\n",
            "Derivative w.r.t μ_2 evaluated: [[0.00232789691964532], [0.00252158995460855], [-0.00241591817895880]]\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a095c858940c4d93818e35996e6b12ac",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Derivative w.r.t s_0 evaluated: [[-0.00774147418890434], [-0.00847465792044467], [0]]\n",
            "Derivative w.r.t s_1 evaluated: [[-0.000237352604004352], [-0.00186181527078744], [0]]\n",
            "Derivative w.r.t s_2 evaluated: [[0.000386389861524028], [-0.000465859492314193], [0]]\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ed46c14cae684c3ca274355a99ffda77",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Derivative w.r.t sh_0 evaluated: [[-0.00684190568634900], [0.0118505282693509], [-0.0118505282693509], [0.0118505282693509]]\n",
            "Derivative w.r.t sh_1 evaluated: [[-0.0101361292723854], [0.0351125817837154], [-0.0351125817837154], [0.0351125817837154]]\n",
            "Derivative w.r.t sh_2 evaluated: [[-0.00419213593065449], [0.0217829772723858], [-0.0217829772723858], [0.0217829772723858]]\n"
          ]
        }
      ],
      "source": [
        "import sympy as sp\n",
        "import sympy.stats as stats\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "def mean(expr_list):\n",
        "    mean_expr = sum(expr_list) / len(expr_list)\n",
        "    return mean_expr \n",
        "\n",
        "def variance(expr_list1, mean_expr1, expr_list2=None, mean_expr2=None):\n",
        "    if not expr_list2:\n",
        "        variance_expr = sum((x - mean_expr1)**2 for x in expr_list1) / (len(expr_list1)-1)\n",
        "    else:\n",
        "        variance_expr = sum((x - mean_expr1)*(y - mean_expr2) for x,y in zip(expr_list1, expr_list2) ) / (len(expr_list1)-1)\n",
        "    return variance_expr \n",
        "\n",
        "def dssim_loss(img1, img2):\n",
        "    mu_img1 = mean(img1)\n",
        "    mu_img2 = mean(img2)\n",
        "    var_img1 = variance(img1, mu_img1)\n",
        "    var_img2 = variance(img2, mu_img2)\n",
        "    cov_img1img2 = variance(img1, mu_img1, img2, mu_img2)\n",
        "    print(cov_img1img2.subs(values).evalf())\n",
        "    ssim = (2*mu_img1*mu_img2 +10e-9)/(mu_img1**2 + mu_img2**2 + 10e-9)\n",
        "    ssim *= (2*cov_img1img2 + 10e-9)/(var_img1**2 + var_img2**2 + 10e-9)\n",
        "    return 1 - ssim\n",
        "\n",
        "def build_color(sh, rays_d):\n",
        "    lmax = 1  # Maximum spherical harmonics degree\n",
        "    C0 = 0.28209479177387814\n",
        "    C1 = 0.4886025119029199\n",
        "\n",
        "    color = C0 * sh[:, 0][0]\n",
        "    if lmax > 0:\n",
        "        x, y, z = rays_d[0, :][0], rays_d[1, :][0], rays_d[2, :][0]\n",
        "        color = (color -\n",
        "                C1 * y * sh[:, 1][0] +\n",
        "                C1 * z * sh[:, 2][0] -\n",
        "                C1 * x * sh[:, 3][0])\n",
        "    \n",
        "    color = (color + 0.5)\n",
        "    return color\n",
        "\n",
        "N = 3  # Number of gaussians\n",
        "M = 4 # Number of pixels\n",
        "R_i = sp.Matrix([[1.0, 0.0, 0.0], [0.0, 1.0, 0.0],[0.0, 0.0, 1.0]]) # assumir conhecida por simplicidade\n",
        "W = sp.Matrix([[1.0, 0.0, 0.0], [0.0, 1.0, 0.0],[0.0, 0.0, 1.0]])\n",
        "J = sp.Matrix([[1.0, 0.0, 0.0], [0.0, 1.0, 0.0]])\n",
        "P = sp.Matrix([[1000.0, 0.0, 500.0, 300.0], [0.0, 1000.0, 500.0, 300.0], [300.0, 300.0, 300.0, 300.0]])\n",
        "cp = sp.Matrix([0.0, 0.0, 0.0])\n",
        "\n",
        "# variam para n=1,...,N\n",
        "sh0 = sp.IndexedBase('sh_0') # k_0^0   <-- grau_max = 0 \n",
        "sh1 = sp.IndexedBase('sh_1') # k_1^{-1}\n",
        "sh2 = sp.IndexedBase('sh_2') # k_1^{0}\n",
        "sh3 = sp.IndexedBase('sh_3') # k_1^{1} <-- grau_max = 1 \n",
        "\n",
        "mu3Dx = sp.IndexedBase('μ_x')\n",
        "mu3Dy = sp.IndexedBase('μ_y')\n",
        "mu3Dz = sp.IndexedBase('μ_z')\n",
        "\n",
        "sx = sp.IndexedBase('s_x')\n",
        "sy = sp.IndexedBase('s_y')\n",
        "sz = sp.IndexedBase('s_z')\n",
        "\n",
        "# Coordenada do ponto de pesquisa\n",
        "p_k = sp.Matrix([[i, j] for i in range(int(M**(0.5))) for j in range(int(M**(0.5)))])\n",
        "\n",
        "# Observações\n",
        "C_hat = sp.Matrix([[k] for k in range(M)])\n",
        "\n",
        "# Generate values to apply to the derivative\n",
        "values = {}\n",
        "for i in range(N):\n",
        "    values = values | {mu3Dx[i]: 1+i, mu3Dy[i]: 1+i, mu3Dz[i]: 1+i,\n",
        "                       sx[i]:0.5*(i+1), sy[i]:0.5*(i+1), sz[i]:0.5*(i+1),\n",
        "                       sh0[i]:1/(i+1),sh1[i]:1/(i+1), sh2[i]:1/(i+1), sh3[i]:1/(i+1) }\n",
        "\n",
        "#Forward pass\n",
        "def C_(p):\n",
        "    # Expression for C(x)\n",
        "    C_values = 0 \n",
        "    for i in range(0,N):   \n",
        "        sh_i = sp.Matrix([sh0[i], sh1[i], sh2[i], sh3[i]])\n",
        "        mu3D_i = sp.Matrix([mu3Dx[i], mu3Dy[i], mu3Dz[i]])\n",
        "        mu2Dx_i = (mu3Dx[i] * P[0, 0] + mu3Dy[i] * P[0, 1] + mu3Dz[i] * P[0, 2] + P[0, 3])/(mu3Dx[i] * P[2, 0] + mu3Dy[i] * P[2,1] + mu3Dz[i] * P[2, 2] + P[2, 3])\n",
        "        mu2Dy_i = (mu3Dx[i] * P[1, 0] + mu3Dy[i] * P[1, 1] + mu3Dz[i] * P[1, 2] + P[1, 3])/(mu3Dx[i] * P[2, 0] + mu3Dy[i] * P[2,1] + mu3Dz[i] * P[2, 2] + P[2, 3])\n",
        "        d_i = p - sp.Matrix([mu2Dx_i, mu2Dy_i])\n",
        "        S_i = sp.Matrix([[sx[i], 0, 0], [0, sy[i], 0], [0, 0, sz[i]]])\n",
        "        Sigma_2D_i = J * W * R_i * S_i * S_i.T * R_i.T * W.T * J.T\n",
        "        Sigma_2D_inv_i = Sigma_2D_i.inv()\n",
        "        G_i = sp.exp(-0.5 * (d_i.T * Sigma_2D_inv_i * d_i)[0, 0])\n",
        "        prod = 1\n",
        "        for j in range(0, i):\n",
        "            mu2Dx_j = (mu3Dx[j] * P[0, 0] + mu3Dy[j] * P[0, 1] + mu3Dz[j] * P[0, 2] + P[0, 3])/(mu3Dx[j] * P[2, 0] + mu3Dy[j] * P[2,1] + mu3Dz[j] * P[2, 2] + P[2, 3])\n",
        "            mu2Dy_j = (mu3Dx[j] * P[1, 0] + mu3Dy[j] * P[1, 1] + mu3Dz[j] * P[1, 2] + P[1, 3])/(mu3Dx[j] * P[2, 0] + mu3Dy[j] * P[2,1] + mu3Dz[j] * P[2, 2] + P[2, 3])\n",
        "            d_j = p - sp.Matrix([mu2Dx_j, mu2Dy_j])\n",
        "            S_j = sp.Matrix([[sx[j], 0, 0], [0, sy[j], 0], [0, 0, sz[j]]])\n",
        "            Sigma_2D_j = J * W * R_i * S_j * S_j.T * R_i.T * W.T * J.T\n",
        "            Sigma_2D_inv_j = Sigma_2D_j.inv()\n",
        "            G_j = sp.exp(-0.5 * (d_j.T * Sigma_2D_inv_j * d_j)[0, 0])\n",
        "            prod *= (1 - G_j)\n",
        "        direction = mu3D_i - cp\n",
        "        c_i = build_color(sh_i.T, direction)\n",
        "        C_values += c_i * G_i * prod\n",
        "    return C_values\n",
        "\n",
        "C = []\n",
        "for k in tqdm(range(M)):\n",
        "    C.append( C_(p_k.row(k).T) )\n",
        "C = sp.Matrix(C)\n",
        "\n",
        "L = dssim_loss(C, C_hat)\n",
        "\n",
        "print(\"L=\")\n",
        "# display(L) # slow\n",
        "\n",
        "print(f\"Evaluation data: {values}:\")\n",
        "print(f\"L evaluated:\", L.subs(values).evalf())\n",
        "\n",
        "## Sympy can't handle these sizes of derivatives...\n",
        "dL_dmu = []\n",
        "for i in tqdm(range(N)):\n",
        "    dL_dmu_i = sp.diff(L, sp.Matrix([mu3Dx[i], mu3Dy[i], mu3Dz[i]]))\n",
        "    dL_dmu_num = dL_dmu_i.subs(values).evalf().tolist()\n",
        "    print(f\"Derivative w.r.t μ_{i} evaluated:\", dL_dmu_num )\n",
        "    dL_dmu.append(dL_dmu_num)\n",
        "\n",
        "dL_ds = []\n",
        "for i in tqdm(range(N)):\n",
        "    dL_ds_i = sp.diff(L, sp.Matrix([sx[i], sy[i], sz[i]]))\n",
        "    dL_ds_num = dL_ds_i.subs(values).evalf().tolist()\n",
        "    print(f\"Derivative w.r.t s_{i} evaluated:\", dL_ds_num )\n",
        "    dL_ds.append(dL_ds_num)\n",
        "\n",
        "dL_dsh = []\n",
        "for i in tqdm(range(N)):\n",
        "    dL_dsh_i = sp.diff(L, sp.Matrix([sh0[i], sh1[i], sh2[i], sh3[i]]))\n",
        "    dL_dsh_num = dL_dsh_i.subs(values).evalf().tolist()\n",
        "    print(f\"Derivative w.r.t sh_{i} evaluated:\",dL_dsh_num )\n",
        "    dL_dsh.append(dL_dsh_num)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Implementação PyTorch:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "dafa5212e18b42a3af476d09dcd18687",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/4 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "L= tensor(0.9929, grad_fn=<RsubBackward1>)\n",
            "tensor([[ 0.0124,  0.0126, -0.0112],\n",
            "        [ 0.0086,  0.0090, -0.0087],\n",
            "        [ 0.0023,  0.0025, -0.0024]])\n",
            "tensor([[-0.0077, -0.0085,  0.0000],\n",
            "        [-0.0002, -0.0019,  0.0000],\n",
            "        [ 0.0004, -0.0005,  0.0000]])\n",
            "tensor([[-0.0068,  0.0119, -0.0119,  0.0119],\n",
            "        [-0.0101,  0.0351, -0.0351,  0.0351],\n",
            "        [-0.0042,  0.0218, -0.0218,  0.0218]])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "def dssim_loss(img1, img2):\n",
        "    mu_img1 = torch.mean(img1)\n",
        "    mu_img2 = torch.mean(img2)\n",
        "    var_img1 = torch.var(img1)\n",
        "    var_img2 = torch.var(img2)\n",
        "    cov_img1img2 = ((img1 - mu_img1) * (img2 - mu_img2)).sum() / (img1.size(0) - 1)\n",
        "    ssim = (2*mu_img1*mu_img2 +10e-9)/(mu_img1**2 + mu_img2**2 + 10e-9)\n",
        "    ssim *= (2*cov_img1img2 + 10e-9)/(var_img1**2 + var_img2**2 + 10e-9)\n",
        "    return 1 - ssim\n",
        "\n",
        "def build_color(sh_i, rays_d):\n",
        "    lmax = 1  # Maximum spherical harmonics degree\n",
        "    C0 = 0.28209479177387814\n",
        "    C1 = 0.4886025119029199\n",
        "    color = C0 * sh_i[..., 0]\n",
        "    if lmax > 0:\n",
        "        x, y, z = rays_d[0, ...], rays_d[1, ...], rays_d[2, ...]\n",
        "        color = (color -\n",
        "                C1 * y * sh_i[..., 1] +\n",
        "                C1 * z * sh_i[..., 2] -\n",
        "                C1 * x * sh_i[..., 3])\n",
        "    \n",
        "    color = (color + 0.5)\n",
        "    return color\n",
        "\n",
        "N = 3  # Number of gaussians\n",
        "M = 4 # Number of pixels\n",
        "\n",
        "R_i = torch.tensor([[1.0, 0.0, 0.0], [0.0, 1.0, 0.0],[0.0, 0.0, 1.0]], requires_grad=False)\n",
        "W = torch.tensor([[1.0, 0.0, 0.0], [0.0, 1.0, 0.0],[0.0, 0.0, 1.0]], requires_grad=False)\n",
        "J = torch.tensor([[1.0, 0.0, 0.0], [0.0, 1.0, 0.0]], requires_grad=False)\n",
        "P = torch.tensor([[1000.0, 0.0, 500.0, 300.0], [0.0, 1000.0, 500.0, 300.0], [300.0, 300.0, 300.0, 300.0]], requires_grad=False)\n",
        "cp = torch.tensor([0.0, 0.0, 0.0], requires_grad=False)\n",
        "\n",
        "# variam para n=1,...,N\n",
        "mu3D = torch.tensor([[1.0+i, 1.0+i, 1.0+i] for i in range(N)], requires_grad=True)\n",
        "s = torch.tensor([[0.5*(i+1), 0.5*(i+1), 0.5*(i+1)] for i in range(N)], requires_grad=True) \n",
        "sh = torch.tensor([[1.0/(i+1), 1.0/(i+1), 1.0/(i+1), 1.0/(i+1)] for i in range(N)], requires_grad=True)\n",
        "\n",
        "# Coordenada do ponto de pesquisa\n",
        "p_k = torch.tensor([[i, j] for i in range(int(M**(0.5))) for j in range(int(M**(0.5)))], requires_grad=False)\n",
        "\n",
        "# Observações\n",
        "C_hat = torch.tensor([k for k in range(M)], dtype=torch.float32)\n",
        "\n",
        "#Forward pass\n",
        "def C_(p):\n",
        "    # Expression for C(x)\n",
        "    C_values = torch.tensor(0.0) \n",
        "    for i in range(N):\n",
        "        mu2Dx_i = (mu3D[i,0] * P[0, 0] + mu3D[i,1] * P[0, 1] + mu3D[i,2] * P[0, 2] + P[0, 3])/(mu3D[i,0] * P[2, 0] + mu3D[i,1] * P[2,1] + mu3D[i,2] * P[2, 2] + P[2, 3])\n",
        "        mu2Dy_i = (mu3D[i,0] * P[1, 0] + mu3D[i,1] * P[1, 1] + mu3D[i,2] * P[1, 2] + P[1, 3])/(mu3D[i,0] * P[2, 0] + mu3D[i,1] * P[2,1] + mu3D[i,2] * P[2, 2] + P[2, 3])\n",
        "        d_i = p - torch.stack([mu2Dx_i, mu2Dy_i])\n",
        "        S_i = torch.diag(s[i])\n",
        "        Sigma_2D_i = J @ W @ R_i @ S_i @ S_i.T @ R_i.T @ W.T @ J.T\n",
        "        Sigma_2D_inv_i = torch.linalg.inv(Sigma_2D_i)\n",
        "        G_i = torch.exp(-0.5 * (d_i.T @ Sigma_2D_inv_i @ d_i))\n",
        "        prod = 1\n",
        "        for j in range(i):\n",
        "            mu2Dx_j = (mu3D[j,0] * P[0, 0] + mu3D[j,1] * P[0, 1] + mu3D[j,2] * P[0, 2] + P[0, 3])/(mu3D[j,0] * P[2, 0] + mu3D[j,1] * P[2,1] + mu3D[j,2] * P[2, 2] + P[2, 3])\n",
        "            mu2Dy_j = (mu3D[j,0] * P[1, 0] + mu3D[j,1] * P[1, 1] + mu3D[j,2] * P[1, 2] + P[1, 3])/(mu3D[j,0] * P[2, 0] + mu3D[j,1] * P[2,1] + mu3D[j,2] * P[2, 2] + P[2, 3])\n",
        "            d_j = p - torch.stack([mu2Dx_j, mu2Dy_j])\n",
        "            S_j = torch.diag(s[j])\n",
        "            Sigma_2D_j = J @ W @ R_i @ S_j @ S_j.T @ R_i.T @ W.T @ J.T\n",
        "            Sigma_2D_inv_j = torch.linalg.inv(Sigma_2D_j)\n",
        "            G_j = torch.exp(-0.5 * (d_j.T @ Sigma_2D_inv_j @ d_j))\n",
        "            prod *= (1 - G_j)\n",
        "        direction = mu3D[i] - cp\n",
        "        c_i = build_color(sh[i], direction)\n",
        "        C_values += c_i * G_i * prod\n",
        "    return C_values\n",
        "\n",
        "C = []\n",
        "for k in tqdm(range(M)):\n",
        "    C.append( C_(p_k[k]) )\n",
        "C = torch.stack(C)\n",
        "\n",
        "L = dssim_loss( C , C_hat)\n",
        "print(\"L=\", L)\n",
        "\n",
        "# Backward pass\n",
        "L.backward()\n",
        "\n",
        "# Gradients\n",
        "print(mu3D.grad)\n",
        "print(s.grad)\n",
        "print(sh.grad)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Comparação:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[ 0.0124,  0.0126, -0.0112],\n",
            "        [ 0.0086,  0.0090, -0.0087],\n",
            "        [ 0.0023,  0.0025, -0.0024]]) =\n",
            " tensor([[ 0.0124,  0.0126, -0.0112],\n",
            "        [ 0.0086,  0.0090, -0.0087],\n",
            "        [ 0.0023,  0.0025, -0.0024]]) \n",
            "\n",
            "tensor([[-0.0077, -0.0085,  0.0000],\n",
            "        [-0.0002, -0.0019,  0.0000],\n",
            "        [ 0.0004, -0.0005,  0.0000]]) =\n",
            " tensor([[-0.0077, -0.0085,  0.0000],\n",
            "        [-0.0002, -0.0019,  0.0000],\n",
            "        [ 0.0004, -0.0005,  0.0000]]) \n",
            "\n",
            "tensor([[-0.0068,  0.0119, -0.0119,  0.0119],\n",
            "        [-0.0101,  0.0351, -0.0351,  0.0351],\n",
            "        [-0.0042,  0.0218, -0.0218,  0.0218]]) =\n",
            " tensor([[-0.0068,  0.0119, -0.0119,  0.0119],\n",
            "        [-0.0101,  0.0351, -0.0351,  0.0351],\n",
            "        [-0.0042,  0.0218, -0.0218,  0.0218]]) \n",
            "\n"
          ]
        }
      ],
      "source": [
        "dL_dmu = torch.tensor(dL_dmu, dtype=torch.float32).reshape(mu3D.grad.shape)\n",
        "dL_ds = torch.tensor(dL_ds, dtype=torch.float32).reshape(s.grad.shape)\n",
        "dL_dsh = torch.tensor(dL_dsh, dtype=torch.float32).reshape(sh.grad.shape)\n",
        "\n",
        "# Gradients\n",
        "print(mu3D.grad, \"=\\n\", dL_dmu, \"\\n\")\n",
        "print(s.grad, \"=\\n\", dL_ds, \"\\n\" )\n",
        "print(sh.grad, \"=\\n\", dL_dsh, \"\\n\" )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Exemplo 16"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "$\\displaystyle \\mathcal{L}(C, C') = L_{recon}(C, C') + L_{DSSIM}(C, C')$\n",
        "\n",
        "- $C$ e $C'$ são as imagens, reconstruída e prevista respectivamente, vetorizadas com $M$ pixels cada\n",
        "- $\\displaystyle L_{recon}(C, C') =\\sum_{k=1}^{M} || C(\\mathbf{p_k}) - C'(\\mathbf{p_k}) ||^2 $\n",
        "\n",
        "- $\\displaystyle L_{DSSIM}(C, C') = 1- \\left(\\frac{2\\overline{C}\\overline{C'}+ 10^{-9}}{\\overline{C}^2 + \\overline{C'}^2 + 10^{-9}}\\right) \\cdot \\left(\\frac{2\\sigma_{CC'}+ 10^{-9}}{\\sigma_{C}^2 + \\sigma_{C'}^2 + 10^{-9}}\\right)$\n",
        "- $\\mathbf{p_k}$ é a coordenada do pixel de posição $k$ em cada imagem vetorizada\n",
        "- $\\overline{C}$, $\\overline{C'}$ são as médias de $C$ e $C'$\n",
        "- $\\sigma_{C}$, $\\sigma_{C'}$ são as variâncias de $C$ e $C'$\n",
        "- $\\sigma_{CC'}$ é a covariância de $C$ e $C'$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Implementação SymPy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5febf18b94b7437d8b6e0e841cf56752",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/4 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "L=\n",
            "Evaluation data: {μ_x[0]: 1, μ_y[0]: 1, μ_z[0]: 1, s_x[0]: 0.5, s_y[0]: 0.5, s_z[0]: 0.5, sh_0[0]: 1.0, sh_1[0]: 1.0, sh_2[0]: 1.0, sh_3[0]: 1.0, μ_x[1]: 2, μ_y[1]: 2, μ_z[1]: 2, s_x[1]: 1.0, s_y[1]: 1.0, s_z[1]: 1.0, sh_0[1]: 0.5, sh_1[1]: 0.5, sh_2[1]: 0.5, sh_3[1]: 0.5, μ_x[2]: 3, μ_y[2]: 3, μ_z[2]: 3, s_x[2]: 1.5, s_y[2]: 1.5, s_z[2]: 1.5, sh_0[2]: 0.3333333333333333, sh_1[2]: 0.3333333333333333, sh_2[2]: 0.3333333333333333, sh_3[2]: 0.3333333333333333}:\n",
            "L evaluated: 0.992887347678695\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2564f80632844f75a10db32540f7b2c9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Derivative w.r.t μ_0 evaluated: [[0.0124251889940151], [0.0125779356047527], [-0.0111994942393178]]\n",
            "Derivative w.r.t μ_1 evaluated: [[0.00863845206681180], [0.00899944377054137], [-0.00873734297318112]]\n",
            "Derivative w.r.t μ_2 evaluated: [[0.00232789691964532], [0.00252158995460855], [-0.00241591817895880]]\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7c65451497a6436ebf9bc19ac46c4b8d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Derivative w.r.t s_0 evaluated: [[-0.00774147418890434], [-0.00847465792044467], [0]]\n",
            "Derivative w.r.t s_1 evaluated: [[-0.000237352604004352], [-0.00186181527078744], [0]]\n",
            "Derivative w.r.t s_2 evaluated: [[0.000386389861524028], [-0.000465859492314193], [0]]\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "13cc336dd5734a4685efc2b6bab5f56f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Derivative w.r.t sh_0 evaluated: [[-0.00684190568634900], [0.0118505282693509], [-0.0118505282693509], [0.0118505282693509]]\n",
            "Derivative w.r.t sh_1 evaluated: [[-0.0101361292723854], [0.0351125817837154], [-0.0351125817837154], [0.0351125817837154]]\n",
            "Derivative w.r.t sh_2 evaluated: [[-0.00419213593065449], [0.0217829772723858], [-0.0217829772723858], [0.0217829772723858]]\n"
          ]
        }
      ],
      "source": [
        "import sympy as sp\n",
        "import sympy.stats as stats\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "def mean(expr_list):\n",
        "    mean_expr = sum(expr_list) / len(expr_list)\n",
        "    return mean_expr \n",
        "\n",
        "def variance(expr_list1, mean_expr1, expr_list2=None, mean_expr2=None):\n",
        "    if not expr_list2:\n",
        "        variance_expr = sum((x - mean_expr1)**2 for x in expr_list1) / (len(expr_list1)-1)\n",
        "    else:\n",
        "        variance_expr = sum((x - mean_expr1)*(y - mean_expr2) for x,y in zip(expr_list1, expr_list2) ) / (len(expr_list1)-1)\n",
        "    return variance_expr \n",
        "\n",
        "def dssim_loss(img1, img2):\n",
        "    mu_img1 = mean(img1)\n",
        "    mu_img2 = mean(img2)\n",
        "    var_img1 = variance(img1, mu_img1)\n",
        "    var_img2 = variance(img2, mu_img2)\n",
        "    cov_img1img2 = variance(img1, mu_img1, img2, mu_img2)\n",
        "    ssim = (2*mu_img1*mu_img2 +10e-9)/(mu_img1**2 + mu_img2**2 + 10e-9)\n",
        "    ssim *= (2*cov_img1img2 + 10e-9)/(var_img1**2 + var_img2**2 + 10e-9)\n",
        "    return 1 - ssim\n",
        "\n",
        "def build_color(sh, rays_d):\n",
        "    lmax = 1  # Maximum spherical harmonics degree\n",
        "    C0 = 0.28209479177387814\n",
        "    C1 = 0.4886025119029199\n",
        "\n",
        "    color = C0 * sh[:, 0][0]\n",
        "    if lmax > 0:\n",
        "        x, y, z = rays_d[0, :][0], rays_d[1, :][0], rays_d[2, :][0]\n",
        "        color = (color -\n",
        "                C1 * y * sh[:, 1][0] +\n",
        "                C1 * z * sh[:, 2][0] -\n",
        "                C1 * x * sh[:, 3][0])\n",
        "    \n",
        "    color = (color + 0.5)\n",
        "    return color\n",
        "\n",
        "N = 3  # Number of gaussians\n",
        "M = 4 # Number of pixels\n",
        "R_i = sp.Matrix([[1.0, 0.0, 0.0], [0.0, 1.0, 0.0],[0.0, 0.0, 1.0]]) # assumir conhecida por simplicidade\n",
        "W = sp.Matrix([[1.0, 0.0, 0.0], [0.0, 1.0, 0.0],[0.0, 0.0, 1.0]])\n",
        "J = sp.Matrix([[1.0, 0.0, 0.0], [0.0, 1.0, 0.0]])\n",
        "P = sp.Matrix([[1000.0, 0.0, 500.0, 300.0], [0.0, 1000.0, 500.0, 300.0], [300.0, 300.0, 300.0, 300.0]])\n",
        "cp = sp.Matrix([0.0, 0.0, 0.0])\n",
        "\n",
        "# variam para n=1,...,N\n",
        "sh0 = sp.IndexedBase('sh_0') # k_0^0   <-- grau_max = 0 \n",
        "sh1 = sp.IndexedBase('sh_1') # k_1^{-1}\n",
        "sh2 = sp.IndexedBase('sh_2') # k_1^{0}\n",
        "sh3 = sp.IndexedBase('sh_3') # k_1^{1} <-- grau_max = 1 \n",
        "\n",
        "mu3Dx = sp.IndexedBase('μ_x')\n",
        "mu3Dy = sp.IndexedBase('μ_y')\n",
        "mu3Dz = sp.IndexedBase('μ_z')\n",
        "\n",
        "sx = sp.IndexedBase('s_x')\n",
        "sy = sp.IndexedBase('s_y')\n",
        "sz = sp.IndexedBase('s_z')\n",
        "\n",
        "# Coordenada do ponto de pesquisa\n",
        "p_k = sp.Matrix([[i, j] for i in range(int(M**(0.5))) for j in range(int(M**(0.5)))])\n",
        "\n",
        "# Observações\n",
        "C_hat = sp.Matrix([[k] for k in range(M)])\n",
        "\n",
        "# Generate values to apply to the derivative\n",
        "values = {}\n",
        "for i in range(N):\n",
        "    values = values | {mu3Dx[i]: 1+i, mu3Dy[i]: 1+i, mu3Dz[i]: 1+i,\n",
        "                       sx[i]:0.5*(i+1), sy[i]:0.5*(i+1), sz[i]:0.5*(i+1),\n",
        "                       sh0[i]:1/(i+1),sh1[i]:1/(i+1), sh2[i]:1/(i+1), sh3[i]:1/(i+1) }\n",
        "\n",
        "#Forward pass\n",
        "def C_(p):\n",
        "    # Expression for C(x)\n",
        "    C_values = 0 \n",
        "    for i in range(0,N):   \n",
        "        sh_i = sp.Matrix([sh0[i], sh1[i], sh2[i], sh3[i]])\n",
        "        mu3D_i = sp.Matrix([mu3Dx[i], mu3Dy[i], mu3Dz[i]])\n",
        "        mu2Dx_i = (mu3Dx[i] * P[0, 0] + mu3Dy[i] * P[0, 1] + mu3Dz[i] * P[0, 2] + P[0, 3])/(mu3Dx[i] * P[2, 0] + mu3Dy[i] * P[2,1] + mu3Dz[i] * P[2, 2] + P[2, 3])\n",
        "        mu2Dy_i = (mu3Dx[i] * P[1, 0] + mu3Dy[i] * P[1, 1] + mu3Dz[i] * P[1, 2] + P[1, 3])/(mu3Dx[i] * P[2, 0] + mu3Dy[i] * P[2,1] + mu3Dz[i] * P[2, 2] + P[2, 3])\n",
        "        d_i = p - sp.Matrix([mu2Dx_i, mu2Dy_i])\n",
        "        S_i = sp.Matrix([[sx[i], 0, 0], [0, sy[i], 0], [0, 0, sz[i]]])\n",
        "        Sigma_2D_i = J * W * R_i * S_i * S_i.T * R_i.T * W.T * J.T\n",
        "        Sigma_2D_inv_i = Sigma_2D_i.inv()\n",
        "        G_i = sp.exp(-0.5 * (d_i.T * Sigma_2D_inv_i * d_i)[0, 0])\n",
        "        prod = 1\n",
        "        for j in range(0, i):\n",
        "            mu2Dx_j = (mu3Dx[j] * P[0, 0] + mu3Dy[j] * P[0, 1] + mu3Dz[j] * P[0, 2] + P[0, 3])/(mu3Dx[j] * P[2, 0] + mu3Dy[j] * P[2,1] + mu3Dz[j] * P[2, 2] + P[2, 3])\n",
        "            mu2Dy_j = (mu3Dx[j] * P[1, 0] + mu3Dy[j] * P[1, 1] + mu3Dz[j] * P[1, 2] + P[1, 3])/(mu3Dx[j] * P[2, 0] + mu3Dy[j] * P[2,1] + mu3Dz[j] * P[2, 2] + P[2, 3])\n",
        "            d_j = p - sp.Matrix([mu2Dx_j, mu2Dy_j])\n",
        "            S_j = sp.Matrix([[sx[j], 0, 0], [0, sy[j], 0], [0, 0, sz[j]]])\n",
        "            Sigma_2D_j = J * W * R_i * S_j * S_j.T * R_i.T * W.T * J.T\n",
        "            Sigma_2D_inv_j = Sigma_2D_j.inv()\n",
        "            G_j = sp.exp(-0.5 * (d_j.T * Sigma_2D_inv_j * d_j)[0, 0])\n",
        "            prod *= (1 - G_j)\n",
        "        direction = mu3D_i - cp\n",
        "        c_i = build_color(sh_i.T, direction)\n",
        "        C_values += c_i * G_i * prod\n",
        "    return C_values\n",
        "\n",
        "L1 = 0   \n",
        "C = []\n",
        "for k in tqdm(range(M)):\n",
        "    C.append( C_(p_k.row(k).T) )\n",
        "    L1 += (C[-1] - C_hat[k])**2\n",
        "C = sp.Matrix(C)\n",
        "\n",
        "L2 = dssim_loss(C, C_hat)\n",
        "\n",
        "L = L2 #+L2\n",
        "print(\"L=\")\n",
        "# display(L) # slow\n",
        "\n",
        "print(f\"Evaluation data: {values}:\")\n",
        "print(f\"L evaluated:\", L.subs(values).evalf())\n",
        "\n",
        "# Sympy can't handle these sizes of derivatives...\n",
        "dL_dmu = []\n",
        "for i in tqdm(range(N)):\n",
        "    dL_dmu_i = sp.diff(L, sp.Matrix([mu3Dx[i], mu3Dy[i], mu3Dz[i]]))\n",
        "    dL_dmu_num = dL_dmu_i.subs(values).evalf().tolist()\n",
        "    print(f\"Derivative w.r.t μ_{i} evaluated:\", dL_dmu_num )\n",
        "    dL_dmu.append(dL_dmu_num)\n",
        "\n",
        "dL_ds = []\n",
        "for i in tqdm(range(N)):\n",
        "    dL_ds_i = sp.diff(L, sp.Matrix([sx[i], sy[i], sz[i]]))\n",
        "    dL_ds_num = dL_ds_i.subs(values).evalf().tolist()\n",
        "    print(f\"Derivative w.r.t s_{i} evaluated:\", dL_ds_num )\n",
        "    dL_ds.append(dL_ds_num)\n",
        "\n",
        "dL_dsh = []\n",
        "for i in tqdm(range(N)):\n",
        "    dL_dsh_i = sp.diff(L, sp.Matrix([sh0[i], sh1[i], sh2[i], sh3[i]]))\n",
        "    dL_dsh_num = dL_dsh_i.subs(values).evalf().tolist()\n",
        "    print(f\"Derivative w.r.t sh_{i} evaluated:\",dL_dsh_num )\n",
        "    dL_dsh.append(dL_dsh_num)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Implementação PyTorch:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "76c49c18238b4da8b0fabbf2d9cb4757",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/4 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "L= tensor(0.9929, grad_fn=<RsubBackward1>)\n",
            "tensor([[ 0.0124,  0.0126, -0.0112],\n",
            "        [ 0.0086,  0.0090, -0.0087],\n",
            "        [ 0.0023,  0.0025, -0.0024]])\n",
            "tensor([[-0.0077, -0.0085,  0.0000],\n",
            "        [-0.0002, -0.0019,  0.0000],\n",
            "        [ 0.0004, -0.0005,  0.0000]])\n",
            "tensor([[-0.0068,  0.0119, -0.0119,  0.0119],\n",
            "        [-0.0101,  0.0351, -0.0351,  0.0351],\n",
            "        [-0.0042,  0.0218, -0.0218,  0.0218]])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "def dssim_loss(img1, img2):\n",
        "    mu_img1 = torch.mean(img1)\n",
        "    mu_img2 = torch.mean(img2)\n",
        "    var_img1 = torch.var(img1)\n",
        "    var_img2 = torch.var(img2)\n",
        "    cov_img1img2 = ((img1 - mu_img1) * (img2 - mu_img2)).sum() / (img1.size(0) - 1)\n",
        "    ssim = (2*mu_img1*mu_img2 +10e-9)/(mu_img1**2 + mu_img2**2 + 10e-9)\n",
        "    ssim *= (2*cov_img1img2 + 10e-9)/(var_img1**2 + var_img2**2 + 10e-9)\n",
        "    return 1 - ssim\n",
        "\n",
        "def build_color(sh_i, rays_d):\n",
        "    lmax = 1  # Maximum spherical harmonics degree\n",
        "    C0 = 0.28209479177387814\n",
        "    C1 = 0.4886025119029199\n",
        "    color = C0 * sh_i[..., 0]\n",
        "    if lmax > 0:\n",
        "        x, y, z = rays_d[0, ...], rays_d[1, ...], rays_d[2, ...]\n",
        "        color = (color -\n",
        "                C1 * y * sh_i[..., 1] +\n",
        "                C1 * z * sh_i[..., 2] -\n",
        "                C1 * x * sh_i[..., 3])\n",
        "    \n",
        "    color = (color + 0.5)\n",
        "    return color\n",
        "\n",
        "N = 3  # Number of gaussians\n",
        "M = 4 # Number of pixels\n",
        "\n",
        "R_i = torch.tensor([[1.0, 0.0, 0.0], [0.0, 1.0, 0.0],[0.0, 0.0, 1.0]], requires_grad=False)\n",
        "W = torch.tensor([[1.0, 0.0, 0.0], [0.0, 1.0, 0.0],[0.0, 0.0, 1.0]], requires_grad=False)\n",
        "J = torch.tensor([[1.0, 0.0, 0.0], [0.0, 1.0, 0.0]], requires_grad=False)\n",
        "P = torch.tensor([[1000.0, 0.0, 500.0, 300.0], [0.0, 1000.0, 500.0, 300.0], [300.0, 300.0, 300.0, 300.0]], requires_grad=False)\n",
        "cp = torch.tensor([0.0, 0.0, 0.0], requires_grad=False)\n",
        "\n",
        "# variam para n=1,...,N\n",
        "mu3D = torch.tensor([[1.0+i, 1.0+i, 1.0+i] for i in range(N)], requires_grad=True)\n",
        "s = torch.tensor([[0.5*(i+1), 0.5*(i+1), 0.5*(i+1)] for i in range(N)], requires_grad=True) \n",
        "sh = torch.tensor([[1.0/(i+1), 1.0/(i+1), 1.0/(i+1), 1.0/(i+1)] for i in range(N)], requires_grad=True)\n",
        "\n",
        "# Coordenada do ponto de pesquisa\n",
        "p_k = torch.tensor([[i, j] for i in range(int(M**(0.5))) for j in range(int(M**(0.5)))], requires_grad=False)\n",
        "\n",
        "# Observações\n",
        "C_hat = torch.tensor([k for k in range(M)], dtype=torch.float32)\n",
        "\n",
        "#Forward pass\n",
        "def C_(p):\n",
        "    # Expression for C(x)\n",
        "    C_values = torch.tensor(0.0) \n",
        "    for i in range(N):\n",
        "        mu2Dx_i = (mu3D[i,0] * P[0, 0] + mu3D[i,1] * P[0, 1] + mu3D[i,2] * P[0, 2] + P[0, 3])/(mu3D[i,0] * P[2, 0] + mu3D[i,1] * P[2,1] + mu3D[i,2] * P[2, 2] + P[2, 3])\n",
        "        mu2Dy_i = (mu3D[i,0] * P[1, 0] + mu3D[i,1] * P[1, 1] + mu3D[i,2] * P[1, 2] + P[1, 3])/(mu3D[i,0] * P[2, 0] + mu3D[i,1] * P[2,1] + mu3D[i,2] * P[2, 2] + P[2, 3])\n",
        "        d_i = p - torch.stack([mu2Dx_i, mu2Dy_i])\n",
        "        S_i = torch.diag(s[i])\n",
        "        Sigma_2D_i = J @ W @ R_i @ S_i @ S_i.T @ R_i.T @ W.T @ J.T\n",
        "        Sigma_2D_inv_i = torch.linalg.inv(Sigma_2D_i)\n",
        "        G_i = torch.exp(-0.5 * (d_i.T @ Sigma_2D_inv_i @ d_i))\n",
        "        prod = 1\n",
        "        for j in range(i):\n",
        "            mu2Dx_j = (mu3D[j,0] * P[0, 0] + mu3D[j,1] * P[0, 1] + mu3D[j,2] * P[0, 2] + P[0, 3])/(mu3D[j,0] * P[2, 0] + mu3D[j,1] * P[2,1] + mu3D[j,2] * P[2, 2] + P[2, 3])\n",
        "            mu2Dy_j = (mu3D[j,0] * P[1, 0] + mu3D[j,1] * P[1, 1] + mu3D[j,2] * P[1, 2] + P[1, 3])/(mu3D[j,0] * P[2, 0] + mu3D[j,1] * P[2,1] + mu3D[j,2] * P[2, 2] + P[2, 3])\n",
        "            d_j = p - torch.stack([mu2Dx_j, mu2Dy_j])\n",
        "            S_j = torch.diag(s[j])\n",
        "            Sigma_2D_j = J @ W @ R_i @ S_j @ S_j.T @ R_i.T @ W.T @ J.T\n",
        "            Sigma_2D_inv_j = torch.linalg.inv(Sigma_2D_j)\n",
        "            G_j = torch.exp(-0.5 * (d_j.T @ Sigma_2D_inv_j @ d_j))\n",
        "            prod *= (1 - G_j)\n",
        "        direction = mu3D[i] - cp\n",
        "        c_i = build_color(sh[i], direction)\n",
        "        C_values += c_i * G_i * prod\n",
        "    return C_values\n",
        "\n",
        "L1 = torch.tensor(0.0)\n",
        "C = []\n",
        "for k in tqdm(range(M)):\n",
        "    C.append( C_(p_k[k]) )\n",
        "    L1 += ( C[-1] - C_hat[k])**2\n",
        "C = torch.stack(C)\n",
        "\n",
        "L2 = dssim_loss( C , C_hat)\n",
        "\n",
        "L = L2 #+L2\n",
        "print(\"L=\", L)\n",
        "\n",
        "# Backward pass\n",
        "L.backward()\n",
        "\n",
        "# Gradients\n",
        "print(mu3D.grad)\n",
        "print(s.grad)\n",
        "print(sh.grad)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Comparação:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[ 0.0124,  0.0126, -0.0112],\n",
            "        [ 0.0086,  0.0090, -0.0087],\n",
            "        [ 0.0023,  0.0025, -0.0024]]) =\n",
            " tensor([[ 0.0124,  0.0126, -0.0112],\n",
            "        [ 0.0086,  0.0090, -0.0087],\n",
            "        [ 0.0023,  0.0025, -0.0024]]) \n",
            "\n",
            "tensor([[-0.0077, -0.0085,  0.0000],\n",
            "        [-0.0002, -0.0019,  0.0000],\n",
            "        [ 0.0004, -0.0005,  0.0000]]) =\n",
            " tensor([[-0.0077, -0.0085,  0.0000],\n",
            "        [-0.0002, -0.0019,  0.0000],\n",
            "        [ 0.0004, -0.0005,  0.0000]]) \n",
            "\n",
            "tensor([[-0.0068,  0.0119, -0.0119,  0.0119],\n",
            "        [-0.0101,  0.0351, -0.0351,  0.0351],\n",
            "        [-0.0042,  0.0218, -0.0218,  0.0218]]) =\n",
            " tensor([[-0.0068,  0.0119, -0.0119,  0.0119],\n",
            "        [-0.0101,  0.0351, -0.0351,  0.0351],\n",
            "        [-0.0042,  0.0218, -0.0218,  0.0218]]) \n",
            "\n"
          ]
        }
      ],
      "source": [
        "dL_dmu = torch.tensor(dL_dmu, dtype=torch.float32).reshape(mu3D.grad.shape)\n",
        "dL_ds = torch.tensor(dL_ds, dtype=torch.float32).reshape(s.grad.shape)\n",
        "dL_dsh = torch.tensor(dL_dsh, dtype=torch.float32).reshape(sh.grad.shape)\n",
        "\n",
        "# Gradients\n",
        "print(mu3D.grad, \"=\\n\", dL_dmu, \"\\n\")\n",
        "print(s.grad, \"=\\n\", dL_ds, \"\\n\" )\n",
        "print(sh.grad, \"=\\n\", dL_dsh, \"\\n\" )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "torch-splatting",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
