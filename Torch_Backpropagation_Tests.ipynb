{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/HumbertoDiego/3dgs-reformulated/blob/main/Torch_Backpropagation_Tests.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B_7htBtewwmv"
      },
      "source": [
        "#### Time everything"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "time: 0 ns (started: 2025-06-11 10:14:37 -03:00)\n"
          ]
        }
      ],
      "source": [
        "%load_ext autotime"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bKJDMfBrw28w"
      },
      "source": [
        "## Check Cuda and environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hpLhVpnWw6hr",
        "outputId": "b6e6d9f6-e3d3-40fc-81fd-cac87618d898"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Wed Jun 11 10:14:38 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 572.61                 Driver Version: 572.61         CUDA Version: 12.8     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                  Driver-Model | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  NVIDIA RTX A4000             WDDM  |   00000000:01:00.0  On |                  Off |\n",
            "| 41%   36C    P8             17W /  140W |     602MiB /  16376MiB |      1%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|    0   N/A  N/A             912    C+G   ...indows\\System32\\ShellHost.exe      N/A      |\n",
            "|    0   N/A  N/A            4044    C+G   ....0.3296.68\\msedgewebview2.exe      N/A      |\n",
            "|    0   N/A  N/A            4568    C+G   ...em32\\ApplicationFrameHost.exe      N/A      |\n",
            "|    0   N/A  N/A            5832    C+G   ...ntrolPanel\\SystemSettings.exe      N/A      |\n",
            "|    0   N/A  N/A            7904    C+G   ...y\\StartMenuExperienceHost.exe      N/A      |\n",
            "|    0   N/A  N/A            8492    C+G   ..._cw5n1h2txyewy\\SearchHost.exe      N/A      |\n",
            "|    0   N/A  N/A            8596    C+G   C:\\Windows\\explorer.exe               N/A      |\n",
            "|    0   N/A  N/A           11560    C+G   ....0.3296.68\\msedgewebview2.exe      N/A      |\n",
            "|    0   N/A  N/A           14920    C+G   ...8bbwe\\PhoneExperienceHost.exe      N/A      |\n",
            "|    0   N/A  N/A           15320    C+G   ...ms\\Microsoft VS Code\\Code.exe      N/A      |\n",
            "|    0   N/A  N/A           16072    C+G   ...t\\Edge\\Application\\msedge.exe      N/A      |\n",
            "|    0   N/A  N/A           19468    C+G   ....0.3296.68\\msedgewebview2.exe      N/A      |\n",
            "|    0   N/A  N/A           20104    C+G   ...64__v10z8vjag6ke6\\HP.myHP.exe      N/A      |\n",
            "+-----------------------------------------------------------------------------------------+\n",
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2025 NVIDIA Corporation\n",
            "Built on Wed_Apr__9_19:29:17_Pacific_Daylight_Time_2025\n",
            "Cuda compilation tools, release 12.9, V12.9.41\n",
            "Build cuda_12.9.r12.9/compiler.35813241_0\n",
            "gcc (MinGW-W64 x86_64-ucrt-posix-seh, built by Brecht Sanders, r8) 13.2.0\n",
            "Copyright (C) 2023 Free Software Foundation, Inc.\n",
            "This is free software; see the source for copying conditions.  There is NO\n",
            "warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n",
            "\n",
            "True 2.5.1 NVIDIA RTX A4000\n",
            "time: 2.28 s (started: 2025-06-11 10:14:38 -03:00)\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi\n",
        "!nvcc --version\n",
        "!gcc --version\n",
        "!python -c \"import torch; print(torch.cuda.is_available(), torch.__version__, torch.cuda.get_device_name(0))\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SpaQWAQg1VtD"
      },
      "source": [
        "# Exemplo 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hCJ8Q0ZKex7z"
      },
      "source": [
        "$y = x^2$\n",
        "\n",
        "$\\nabla(x) = \\frac{dy}{dx}=2x$\n",
        "\n",
        "Para $x=3 \\rightarrow \\nabla(x)=6$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Implementação PyTorch:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5HlzC_wc2ZiB",
        "outputId": "bbd865ef-263c-4325-df3d-d9c419c3e762"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([6.])\n",
            "time: 1.5 s (started: 2025-06-11 10:14:45 -03:00)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "x = torch.tensor([3.0], requires_grad=True)\n",
        "\n",
        "# Forward pass\n",
        "y = x **2\n",
        "\n",
        "# Backward pass\n",
        "y.backward()\n",
        "print(x.grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V_t9-fiKeh43"
      },
      "source": [
        "# Exemplo 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2g3dFVs4gahF"
      },
      "source": [
        "$y = x  w + 1$\n",
        "\n",
        "$\\nabla(x) = \\frac{\\partial y}{\\partial x}=w$\n",
        "\n",
        "$\\nabla(w) = \\frac{\\partial y}{\\partial w}=x$\n",
        "\n",
        "Para $x= 2$ e $w=3  \\rightarrow$\n",
        "\n",
        "$\\nabla(x)=3$\n",
        "\n",
        "$\\nabla(w)=2$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Implementação PyTorch:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c_q1wVPteLjH",
        "outputId": "16717168-ca49-4c0b-b874-8319141ae054"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(3.)\n",
            "tensor(2.)\n",
            "time: 0 ns (started: 2025-06-11 10:14:52 -03:00)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "x = torch.tensor(2.0, requires_grad=True)\n",
        "w = torch.tensor(3.0, requires_grad=True)\n",
        "\n",
        "# Forward pass\n",
        "y = x * w + 1\n",
        "\n",
        "# Backward pass\n",
        "y.backward()\n",
        "print(x.grad)\n",
        "print(w.grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Exemplo 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "$y = e^{(-x^2/2)}$\n",
        "\n",
        "Onde $x$ é um parâmetro a ser atualizado pelo gradiente descendente.\n",
        "\n",
        "$\\nabla(x) = \\frac{\\partial y}{\\partial x}= -xe^{(-x^2/2)}$\n",
        "\n",
        "Para $[x]=[2] \\rightarrow$\n",
        "\n",
        "$\\nabla(x) = -0.270670566473225$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Implementação SymPy:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "y:\n"
          ]
        },
        {
          "data": {
            "text/latex": [
              "$\\displaystyle e^{- \\frac{x^{2}}{2}}$"
            ],
            "text/plain": [
              "exp(-x**2/2)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dy/dx:\n"
          ]
        },
        {
          "data": {
            "text/latex": [
              "$\\displaystyle - x e^{- \\frac{x^{2}}{2}}$"
            ],
            "text/plain": [
              "-x*exp(-x**2/2)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Derivative evaluated at x=2: -0.270670566473225\n",
            "time: 0 ns (started: 2025-06-11 10:16:52 -03:00)\n"
          ]
        }
      ],
      "source": [
        "import sympy as sp\n",
        "\n",
        "x= sp.symbols('x')\n",
        "y = sp.exp(-x**2 / 2)\n",
        "\n",
        "y_derivative = sp.diff(y, x)\n",
        "\n",
        "print(\"y:\")\n",
        "display(y)\n",
        "\n",
        "print(\"dy/dx:\")\n",
        "display(y_derivative)\n",
        "\n",
        "print(\"Derivative evaluated at x=2:\",  y_derivative.evalf(subs={x: 2}))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Implementação PyTorch:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(-0.2707)\n",
            "time: 0 ns (started: 2025-06-10 17:14:31 -03:00)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "x = torch.tensor(2.0, requires_grad=True)\n",
        "\n",
        "# Forward pass\n",
        "y = torch.exp(-x**2 / 2)\n",
        "\n",
        "# Backward pass\n",
        "y.backward()\n",
        "\n",
        "# Gradients\n",
        "print(x.grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Exemplo 4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "$y = \\frac{1}{\\sigma\\sqrt{2\\pi}} e^{-0.5\\frac{(x-\\mu)^2}{\\sigma^2}}$\n",
        "\n",
        "Onde $x$ é um parâmetro a ser atualizado pelo gradiente descendente.\n",
        "\n",
        "$\\nabla(x) = \\frac{\\partial y}{\\partial x}= - \\frac{0.25 \\sqrt{2} \\left(2 x - 2 μ\\right) e^{- \\frac{0.5 \\left(x - μ\\right)^{2}}{σ^{2}}}}{\\sqrt{\\pi} σ^{3}}$\n",
        "\n",
        "Para $[x, \\mu, \\sigma]=[2,0,1] \\rightarrow$\n",
        "\n",
        "$\\nabla(x) = -0.107981933026376$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Implementação SymPy:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Function: sqrt(2)*exp(-0.5*(x - μ)**2/σ**2)/(2*sqrt(pi)*σ)\n",
            "Derivative w.r.t x: -0.25*sqrt(2)*(2*x - 2*μ)*exp(-0.5*(x - μ)**2/σ**2)/(sqrt(pi)*σ**3)\n",
            "Derivative w.r.t x evaluated at x=2, μ=0, σ=1: -0.107981933026376\n",
            "time: 0 ns (started: 2025-06-10 17:14:31 -03:00)\n"
          ]
        }
      ],
      "source": [
        "import sympy as sp\n",
        "\n",
        "x= sp.symbols('x')\n",
        "mu = sp.symbols('μ')\n",
        "sigma = sp.symbols('σ')\n",
        "\n",
        "y = 1/(sigma * sp.sqrt(2 * sp.pi)) * sp.exp(-1/2 * (x - mu)**2 / sigma**2)\n",
        "\n",
        "dy_dx = sp.diff(y, x)\n",
        "print(\"Function:\", y)\n",
        "print(\"Derivative w.r.t x:\", dy_dx)\n",
        "print(\"Derivative w.r.t x evaluated at x=2, μ=0, σ=1:\",  dy_dx.evalf(subs={x: 2, mu: 0, sigma: 1}))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Implementação PyTorch:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(-0.1080)\n",
            "time: 0 ns (started: 2025-06-11 09:53:34 -03:00)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "x = torch.tensor(2.0, requires_grad=True)\n",
        "mu = torch.tensor(0.0, requires_grad=False)\n",
        "sigma = torch.tensor(1.0, requires_grad=False)\n",
        "\n",
        "# Forward pass\n",
        "y = 1/(sigma * torch.sqrt(torch.tensor(2.0 * torch.pi))) * torch.exp(-1/2 * (x - mu)**2 / sigma**2)\n",
        "\n",
        "# Backward pass\n",
        "y.backward()\n",
        "\n",
        "# Gradients\n",
        "print(x.grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Exemplo 5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "$y = \\frac{1}{\\sigma\\sqrt{2\\pi}} e^{-0.5\\frac{(x-\\mu)^2}{\\sigma^2}}$\n",
        "\n",
        "Onde $\\mu$ e $\\sigma$ são parâmetros a seres atualizados pelo gradiente descendente.\n",
        "\n",
        "$\\nabla(\\mu) = \\frac{\\partial y}{\\partial \\mu}= - \\frac{0.25 \\sqrt{2} \\left(- 2 x + 2 μ\\right) e^{- \\frac{0.5 \\left(x - μ\\right)^{2}}{σ^{2}}}}{\\sqrt{\\pi} σ^{3}}$\n",
        "\n",
        "$\\nabla(\\sigma) = \\frac{\\partial y}{\\partial \\sigma}= - \\frac{\\sqrt{2} e^{- \\frac{0.5 \\left(x - μ\\right)^{2}}{σ^{2}}}}{2 \\sqrt{\\pi} σ^{2}} + \\frac{0.5 \\sqrt{2} \\left(x - μ\\right)^{2} e^{- \\frac{0.5 \\left(x - μ\\right)^{2}}{σ^{2}}}}{\\sqrt{\\pi} σ^{4}} $\n",
        "\n",
        "Para $[x, \\mu, \\sigma]=[2,0,1] \\rightarrow$\n",
        "\n",
        "$\\nabla(\\mu) = 0.107981933026376$\n",
        "\n",
        "$\\nabla(\\sigma) = 0.161972899539564$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Implementação SymPy:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 294,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Function: sqrt(2)*exp(-0.5*(x - μ)**2/σ**2)/(2*sqrt(pi)*σ)\n",
            "Derivative w.r.t μ: -0.25*sqrt(2)*(-2*x + 2*μ)*exp(-0.5*(x - μ)**2/σ**2)/(sqrt(pi)*σ**3)\n",
            "Derivative w.r.t σ: -sqrt(2)*exp(-0.5*(x - μ)**2/σ**2)/(2*sqrt(pi)*σ**2) + 0.5*sqrt(2)*(x - μ)**2*exp(-0.5*(x - μ)**2/σ**2)/(sqrt(pi)*σ**4)\n",
            "Derivative w.r.t μ evaluated at x=2, μ=0, σ=1: 0.107981933026376\n",
            "Derivative w.r.t σ evaluated at x=2, μ=0, σ=1: 0.161972899539564\n",
            "time: 16 ms (started: 2025-06-10 17:14:31 -03:00)\n"
          ]
        }
      ],
      "source": [
        "import sympy as sp\n",
        "\n",
        "x= sp.symbols('x')\n",
        "mu = sp.symbols('μ')\n",
        "sigma = sp.symbols('σ')\n",
        "\n",
        "y = 1/(sigma * sp.sqrt(2 * sp.pi)) * sp.exp(-1/2 * (x - mu)**2 / sigma**2)\n",
        "\n",
        "dy_dmu = sp.diff(y, mu)\n",
        "dy_dsigma = sp.diff(y, sigma)\n",
        "\n",
        "print(\"Function:\", y)\n",
        "print(\"Derivative w.r.t μ:\", dy_dmu)\n",
        "print(\"Derivative w.r.t σ:\", dy_dsigma)\n",
        "print(\"Derivative w.r.t μ evaluated at x=2, μ=0, σ=1:\",  dy_dmu.evalf(subs={x: 2, mu: 0, sigma: 1}))\n",
        "print(\"Derivative w.r.t σ evaluated at x=2, μ=0, σ=1:\",  dy_dsigma.evalf(subs={x: 2, mu: 0, sigma: 1}))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Implementação PyTorch:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(0.1080)\n",
            "tensor(0.1620)\n",
            "time: 0 ns (started: 2025-06-11 09:54:17 -03:00)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "x = torch.tensor(2.0, requires_grad=False)\n",
        "mu = torch.tensor(0.0, requires_grad=True)\n",
        "sigma = torch.tensor(1.0, requires_grad=True)\n",
        "# Forward pass\n",
        "y = 1/(sigma * torch.sqrt(torch.tensor(2.0 * torch.pi))) * torch.exp(-1/2 * (x - mu)**2 / sigma**2)\n",
        "\n",
        "# Backward pass\n",
        "y.backward()\n",
        "\n",
        "# Gradients\n",
        "print(mu.grad)\n",
        "print(sigma.grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Exemplo 6"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "$G(x) = e^{-0.5 (\\mathbf{x}-\\mathbf{\\mu})^T \\Sigma^{-1} (\\mathbf{x}-\\mathbf{\\mu}) }$\n",
        "\n",
        "Onde:\n",
        "- $\\mathbf{\\mu},\\mathbf{x} \\in \\mathbb{R}^3$;\n",
        "- $\\Sigma \\in \\mathbb{R}^{3\\times 3}$;\n",
        "- $\\mathbf{\\mu}$ é um parâmetro a ser atualizado pelo gradiente descendente.\n",
        "\n",
        "$\\nabla(\\mu) = \\frac{\\partial G}{\\partial \\mu}= \\left[\\frac{\\partial G}{\\partial \\mu_1}, \\frac{\\partial G}{\\partial \\mu_2}, \\frac{\\partial G}{\\partial \\mu_3}\\right] = \n",
        "\\begin{bmatrix}- \\frac{\\left(- 0.5 x_{1} + 0.5 μ_{1}\\right) \\left({\\Sigma}_{1,1} {\\Sigma}_{2,2} - {\\Sigma}_{1,2} {\\Sigma}_{2,1}\\right)}{{\\Sigma}_{0,0} {\\Sigma}_{1,1} {\\Sigma}_{2,2} - {\\Sigma}_{0,0} {\\Sigma}_{1,2} {\\Sigma}_{2,1} - {\\Sigma}_{0,1} {\\Sigma}_{1,0} {\\Sigma}_{2,2} + {\\Sigma}_{0,1} {\\Sigma}_{1,2} {\\Sigma}_{2,0} + {\\Sigma}_{0,2} {\\Sigma}_{1,0} {\\Sigma}_{2,1} - {\\Sigma}_{0,2} {\\Sigma}_{1,1} {\\Sigma}_{2,0}} + \\frac{0.5 \\left(x_{1} - μ_{1}\\right) \\left({\\Sigma}_{1,1} {\\Sigma}_{2,2} - {\\Sigma}_{1,2} {\\Sigma}_{2,1}\\right)}{{\\Sigma}_{0,0} {\\Sigma}_{1,1} {\\Sigma}_{2,2} - {\\Sigma}_{0,0} {\\Sigma}_{1,2} {\\Sigma}_{2,1} - {\\Sigma}_{0,1} {\\Sigma}_{1,0} {\\Sigma}_{2,2} + {\\Sigma}_{0,1} {\\Sigma}_{1,2} {\\Sigma}_{2,0} + {\\Sigma}_{0,2} {\\Sigma}_{1,0} {\\Sigma}_{2,1} - {\\Sigma}_{0,2} {\\Sigma}_{1,1} {\\Sigma}_{2,0}} - \\frac{\\left(- 0.5 x_{2} + 0.5 μ_{2}\\right) \\left(- {\\Sigma}_{1,0} {\\Sigma}_{2,2} + {\\Sigma}_{1,2} {\\Sigma}_{2,0}\\right)}{{\\Sigma}_{0,0} {\\Sigma}_{1,1} {\\Sigma}_{2,2} - {\\Sigma}_{0,0} {\\Sigma}_{1,2} {\\Sigma}_{2,1} - {\\Sigma}_{0,1} {\\Sigma}_{1,0} {\\Sigma}_{2,2} + {\\Sigma}_{0,1} {\\Sigma}_{1,2} {\\Sigma}_{2,0} + {\\Sigma}_{0,2} {\\Sigma}_{1,0} {\\Sigma}_{2,1} - {\\Sigma}_{0,2} {\\Sigma}_{1,1} {\\Sigma}_{2,0}} + \\frac{0.5 \\left(x_{2} - μ_{2}\\right) \\left(- {\\Sigma}_{0,1} {\\Sigma}_{2,2} + {\\Sigma}_{0,2} {\\Sigma}_{2,1}\\right)}{{\\Sigma}_{0,0} {\\Sigma}_{1,1} {\\Sigma}_{2,2} - {\\Sigma}_{0,0} {\\Sigma}_{1,2} {\\Sigma}_{2,1} - {\\Sigma}_{0,1} {\\Sigma}_{1,0} {\\Sigma}_{2,2} + {\\Sigma}_{0,1} {\\Sigma}_{1,2} {\\Sigma}_{2,0} + {\\Sigma}_{0,2} {\\Sigma}_{1,0} {\\Sigma}_{2,1} - {\\Sigma}_{0,2} {\\Sigma}_{1,1} {\\Sigma}_{2,0}} - \\frac{\\left(- 0.5 x_{3} + 0.5 μ_{3}\\right) \\left({\\Sigma}_{1,0} {\\Sigma}_{2,1} - {\\Sigma}_{1,1} {\\Sigma}_{2,0}\\right)}{{\\Sigma}_{0,0} {\\Sigma}_{1,1} {\\Sigma}_{2,2} - {\\Sigma}_{0,0} {\\Sigma}_{1,2} {\\Sigma}_{2,1} - {\\Sigma}_{0,1} {\\Sigma}_{1,0} {\\Sigma}_{2,2} + {\\Sigma}_{0,1} {\\Sigma}_{1,2} {\\Sigma}_{2,0} + {\\Sigma}_{0,2} {\\Sigma}_{1,0} {\\Sigma}_{2,1} - {\\Sigma}_{0,2} {\\Sigma}_{1,1} {\\Sigma}_{2,0}} + \\frac{0.5 \\left(x_{3} - μ_{3}\\right) \\left({\\Sigma}_{0,1} {\\Sigma}_{1,2} - {\\Sigma}_{0,2} {\\Sigma}_{1,1}\\right)}{{\\Sigma}_{0,0} {\\Sigma}_{1,1} {\\Sigma}_{2,2} - {\\Sigma}_{0,0} {\\Sigma}_{1,2} {\\Sigma}_{2,1} - {\\Sigma}_{0,1} {\\Sigma}_{1,0} {\\Sigma}_{2,2} + {\\Sigma}_{0,1} {\\Sigma}_{1,2} {\\Sigma}_{2,0} + {\\Sigma}_{0,2} {\\Sigma}_{1,0} {\\Sigma}_{2,1} - {\\Sigma}_{0,2} {\\Sigma}_{1,1} {\\Sigma}_{2,0}}\\\\- \\frac{\\left(- 0.5 x_{1} + 0.5 μ_{1}\\right) \\left(- {\\Sigma}_{0,1} {\\Sigma}_{2,2} + {\\Sigma}_{0,2} {\\Sigma}_{2,1}\\right)}{{\\Sigma}_{0,0} {\\Sigma}_{1,1} {\\Sigma}_{2,2} - {\\Sigma}_{0,0} {\\Sigma}_{1,2} {\\Sigma}_{2,1} - {\\Sigma}_{0,1} {\\Sigma}_{1,0} {\\Sigma}_{2,2} + {\\Sigma}_{0,1} {\\Sigma}_{1,2} {\\Sigma}_{2,0} + {\\Sigma}_{0,2} {\\Sigma}_{1,0} {\\Sigma}_{2,1} - {\\Sigma}_{0,2} {\\Sigma}_{1,1} {\\Sigma}_{2,0}} + \\frac{0.5 \\left(x_{1} - μ_{1}\\right) \\left(- {\\Sigma}_{1,0} {\\Sigma}_{2,2} + {\\Sigma}_{1,2} {\\Sigma}_{2,0}\\right)}{{\\Sigma}_{0,0} {\\Sigma}_{1,1} {\\Sigma}_{2,2} - {\\Sigma}_{0,0} {\\Sigma}_{1,2} {\\Sigma}_{2,1} - {\\Sigma}_{0,1} {\\Sigma}_{1,0} {\\Sigma}_{2,2} + {\\Sigma}_{0,1} {\\Sigma}_{1,2} {\\Sigma}_{2,0} + {\\Sigma}_{0,2} {\\Sigma}_{1,0} {\\Sigma}_{2,1} - {\\Sigma}_{0,2} {\\Sigma}_{1,1} {\\Sigma}_{2,0}} - \\frac{\\left(- 0.5 x_{2} + 0.5 μ_{2}\\right) \\left({\\Sigma}_{0,0} {\\Sigma}_{2,2} - {\\Sigma}_{0,2} {\\Sigma}_{2,0}\\right)}{{\\Sigma}_{0,0} {\\Sigma}_{1,1} {\\Sigma}_{2,2} - {\\Sigma}_{0,0} {\\Sigma}_{1,2} {\\Sigma}_{2,1} - {\\Sigma}_{0,1} {\\Sigma}_{1,0} {\\Sigma}_{2,2} + {\\Sigma}_{0,1} {\\Sigma}_{1,2} {\\Sigma}_{2,0} + {\\Sigma}_{0,2} {\\Sigma}_{1,0} {\\Sigma}_{2,1} - {\\Sigma}_{0,2} {\\Sigma}_{1,1} {\\Sigma}_{2,0}} + \\frac{0.5 \\left(x_{2} - μ_{2}\\right) \\left({\\Sigma}_{0,0} {\\Sigma}_{2,2} - {\\Sigma}_{0,2} {\\Sigma}_{2,0}\\right)}{{\\Sigma}_{0,0} {\\Sigma}_{1,1} {\\Sigma}_{2,2} - {\\Sigma}_{0,0} {\\Sigma}_{1,2} {\\Sigma}_{2,1} - {\\Sigma}_{0,1} {\\Sigma}_{1,0} {\\Sigma}_{2,2} + {\\Sigma}_{0,1} {\\Sigma}_{1,2} {\\Sigma}_{2,0} + {\\Sigma}_{0,2} {\\Sigma}_{1,0} {\\Sigma}_{2,1} - {\\Sigma}_{0,2} {\\Sigma}_{1,1} {\\Sigma}_{2,0}} - \\frac{\\left(- 0.5 x_{3} + 0.5 μ_{3}\\right) \\left(- {\\Sigma}_{0,0} {\\Sigma}_{2,1} + {\\Sigma}_{0,1} {\\Sigma}_{2,0}\\right)}{{\\Sigma}_{0,0} {\\Sigma}_{1,1} {\\Sigma}_{2,2} - {\\Sigma}_{0,0} {\\Sigma}_{1,2} {\\Sigma}_{2,1} - {\\Sigma}_{0,1} {\\Sigma}_{1,0} {\\Sigma}_{2,2} + {\\Sigma}_{0,1} {\\Sigma}_{1,2} {\\Sigma}_{2,0} + {\\Sigma}_{0,2} {\\Sigma}_{1,0} {\\Sigma}_{2,1} - {\\Sigma}_{0,2} {\\Sigma}_{1,1} {\\Sigma}_{2,0}} + \\frac{0.5 \\left(x_{3} - μ_{3}\\right) \\left(- {\\Sigma}_{0,0} {\\Sigma}_{1,2} + {\\Sigma}_{0,2} {\\Sigma}_{1,0}\\right)}{{\\Sigma}_{0,0} {\\Sigma}_{1,1} {\\Sigma}_{2,2} - {\\Sigma}_{0,0} {\\Sigma}_{1,2} {\\Sigma}_{2,1} - {\\Sigma}_{0,1} {\\Sigma}_{1,0} {\\Sigma}_{2,2} + {\\Sigma}_{0,1} {\\Sigma}_{1,2} {\\Sigma}_{2,0} + {\\Sigma}_{0,2} {\\Sigma}_{1,0} {\\Sigma}_{2,1} - {\\Sigma}_{0,2} {\\Sigma}_{1,1} {\\Sigma}_{2,0}}\\\\- \\frac{\\left(- 0.5 x_{1} + 0.5 μ_{1}\\right) \\left({\\Sigma}_{0,1} {\\Sigma}_{1,2} - {\\Sigma}_{0,2} {\\Sigma}_{1,1}\\right)}{{\\Sigma}_{0,0} {\\Sigma}_{1,1} {\\Sigma}_{2,2} - {\\Sigma}_{0,0} {\\Sigma}_{1,2} {\\Sigma}_{2,1} - {\\Sigma}_{0,1} {\\Sigma}_{1,0} {\\Sigma}_{2,2} + {\\Sigma}_{0,1} {\\Sigma}_{1,2} {\\Sigma}_{2,0} + {\\Sigma}_{0,2} {\\Sigma}_{1,0} {\\Sigma}_{2,1} - {\\Sigma}_{0,2} {\\Sigma}_{1,1} {\\Sigma}_{2,0}} + \\frac{0.5 \\left(x_{1} - μ_{1}\\right) \\left({\\Sigma}_{1,0} {\\Sigma}_{2,1} - {\\Sigma}_{1,1} {\\Sigma}_{2,0}\\right)}{{\\Sigma}_{0,0} {\\Sigma}_{1,1} {\\Sigma}_{2,2} - {\\Sigma}_{0,0} {\\Sigma}_{1,2} {\\Sigma}_{2,1} - {\\Sigma}_{0,1} {\\Sigma}_{1,0} {\\Sigma}_{2,2} + {\\Sigma}_{0,1} {\\Sigma}_{1,2} {\\Sigma}_{2,0} + {\\Sigma}_{0,2} {\\Sigma}_{1,0} {\\Sigma}_{2,1} - {\\Sigma}_{0,2} {\\Sigma}_{1,1} {\\Sigma}_{2,0}} - \\frac{\\left(- 0.5 x_{2} + 0.5 μ_{2}\\right) \\left(- {\\Sigma}_{0,0} {\\Sigma}_{1,2} + {\\Sigma}_{0,2} {\\Sigma}_{1,0}\\right)}{{\\Sigma}_{0,0} {\\Sigma}_{1,1} {\\Sigma}_{2,2} - {\\Sigma}_{0,0} {\\Sigma}_{1,2} {\\Sigma}_{2,1} - {\\Sigma}_{0,1} {\\Sigma}_{1,0} {\\Sigma}_{2,2} + {\\Sigma}_{0,1} {\\Sigma}_{1,2} {\\Sigma}_{2,0} + {\\Sigma}_{0,2} {\\Sigma}_{1,0} {\\Sigma}_{2,1} - {\\Sigma}_{0,2} {\\Sigma}_{1,1} {\\Sigma}_{2,0}} + \\frac{0.5 \\left(x_{2} - μ_{2}\\right) \\left(- {\\Sigma}_{0,0} {\\Sigma}_{2,1} + {\\Sigma}_{0,1} {\\Sigma}_{2,0}\\right)}{{\\Sigma}_{0,0} {\\Sigma}_{1,1} {\\Sigma}_{2,2} - {\\Sigma}_{0,0} {\\Sigma}_{1,2} {\\Sigma}_{2,1} - {\\Sigma}_{0,1} {\\Sigma}_{1,0} {\\Sigma}_{2,2} + {\\Sigma}_{0,1} {\\Sigma}_{1,2} {\\Sigma}_{2,0} + {\\Sigma}_{0,2} {\\Sigma}_{1,0} {\\Sigma}_{2,1} - {\\Sigma}_{0,2} {\\Sigma}_{1,1} {\\Sigma}_{2,0}} - \\frac{\\left(- 0.5 x_{3} + 0.5 μ_{3}\\right) \\left({\\Sigma}_{0,0} {\\Sigma}_{1,1} - {\\Sigma}_{0,1} {\\Sigma}_{1,0}\\right)}{{\\Sigma}_{0,0} {\\Sigma}_{1,1} {\\Sigma}_{2,2} - {\\Sigma}_{0,0} {\\Sigma}_{1,2} {\\Sigma}_{2,1} - {\\Sigma}_{0,1} {\\Sigma}_{1,0} {\\Sigma}_{2,2} + {\\Sigma}_{0,1} {\\Sigma}_{1,2} {\\Sigma}_{2,0} + {\\Sigma}_{0,2} {\\Sigma}_{1,0} {\\Sigma}_{2,1} - {\\Sigma}_{0,2} {\\Sigma}_{1,1} {\\Sigma}_{2,0}} + \\frac{0.5 \\left(x_{3} - μ_{3}\\right) \\left({\\Sigma}_{0,0} {\\Sigma}_{1,1} - {\\Sigma}_{0,1} {\\Sigma}_{1,0}\\right)}{{\\Sigma}_{0,0} {\\Sigma}_{1,1} {\\Sigma}_{2,2} - {\\Sigma}_{0,0} {\\Sigma}_{1,2} {\\Sigma}_{2,1} - {\\Sigma}_{0,1} {\\Sigma}_{1,0} {\\Sigma}_{2,2} + {\\Sigma}_{0,1} {\\Sigma}_{1,2} {\\Sigma}_{2,0} + {\\Sigma}_{0,2} {\\Sigma}_{1,0} {\\Sigma}_{2,1} - {\\Sigma}_{0,2} {\\Sigma}_{1,1} {\\Sigma}_{2,0}}\\end{bmatrix}^T$\n",
        "\n",
        "Para $\\mathbf{x}=[2,2,3],\\  \\mathbf{\\mu}=[1,1,1] \\ $ e $\\Sigma = I \\rightarrow$\n",
        "\n",
        "$\\nabla(\\mu) = [0.0497870683678639, 0.0497870683678639, 0.0995741367357279] $"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Implementação SymPy:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "G:\n"
          ]
        },
        {
          "data": {
            "text/latex": [
              "$\\displaystyle \\left[\\begin{matrix}1.0 e^{- 0.5 x_{1}^{2} + 1.0 x_{1} μ_{1} - 0.5 x_{2}^{2} + 1.0 x_{2} μ_{2} - 0.5 x_{3}^{2} + 1.0 x_{3} μ_{3} - 0.5 μ_{1}^{2} - 0.5 μ_{2}^{2} - 0.5 μ_{3}^{2}}\\end{matrix}\\right]$"
            ],
            "text/plain": [
              "Matrix([[1.0*exp(-0.5*x_1**2 + 1.0*x_1*μ_1 - 0.5*x_2**2 + 1.0*x_2*μ_2 - 0.5*x_3**2 + 1.0*x_3*μ_3 - 0.5*μ_1**2 - 0.5*μ_2**2 - 0.5*μ_3**2)]])"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Derivative of G w.r.t μ (dG/dmu):\n"
          ]
        },
        {
          "data": {
            "text/latex": [
              "$\\displaystyle \\left[\\begin{matrix}\\left[\\left[\\begin{matrix}1.0 \\left(x_{1} - μ_{1}\\right) e^{- 0.5 x_{1}^{2} + 1.0 x_{1} μ_{1} - 0.5 x_{2}^{2} + 1.0 x_{2} μ_{2} - 0.5 x_{3}^{2} + 1.0 x_{3} μ_{3} - 0.5 μ_{1}^{2} - 0.5 μ_{2}^{2} - 0.5 μ_{3}^{2}}\\end{matrix}\\right]\\right]\\\\\\left[\\left[\\begin{matrix}1.0 \\left(x_{2} - μ_{2}\\right) e^{- 0.5 x_{1}^{2} + 1.0 x_{1} μ_{1} - 0.5 x_{2}^{2} + 1.0 x_{2} μ_{2} - 0.5 x_{3}^{2} + 1.0 x_{3} μ_{3} - 0.5 μ_{1}^{2} - 0.5 μ_{2}^{2} - 0.5 μ_{3}^{2}}\\end{matrix}\\right]\\right]\\\\\\left[\\left[\\begin{matrix}1.0 \\left(x_{3} - μ_{3}\\right) e^{- 0.5 x_{1}^{2} + 1.0 x_{1} μ_{1} - 0.5 x_{2}^{2} + 1.0 x_{2} μ_{2} - 0.5 x_{3}^{2} + 1.0 x_{3} μ_{3} - 0.5 μ_{1}^{2} - 0.5 μ_{2}^{2} - 0.5 μ_{3}^{2}}\\end{matrix}\\right]\\right]\\end{matrix}\\right]$"
            ],
            "text/plain": [
              "[[[[1.0*(x_1 - μ_1)*exp(-0.5*x_1**2 + 1.0*x_1*μ_1 - 0.5*x_2**2 + 1.0*x_2*μ_2 - 0.5*x_3**2 + 1.0*x_3*μ_3 - 0.5*μ_1**2 - 0.5*μ_2**2 - 0.5*μ_3**2)]]], [[[1.0*(x_2 - μ_2)*exp(-0.5*x_1**2 + 1.0*x_1*μ_1 - 0.5*x_2**2 + 1.0*x_2*μ_2 - 0.5*x_3**2 + 1.0*x_3*μ_3 - 0.5*μ_1**2 - 0.5*μ_2**2 - 0.5*μ_3**2)]]], [[[1.0*(x_3 - μ_3)*exp(-0.5*x_1**2 + 1.0*x_1*μ_1 - 0.5*x_2**2 + 1.0*x_2*μ_2 - 0.5*x_3**2 + 1.0*x_3*μ_3 - 0.5*μ_1**2 - 0.5*μ_2**2 - 0.5*μ_3**2)]]]]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Derivative w.r.t μ evaluated at x=[2,2,3], μ=[1,1,1]:\n",
            " [[[[0.0497870683678639]]], [[[0.0497870683678639]]], [[[0.0995741367357279]]]]\n",
            "time: 391 ms (started: 2025-06-11 09:59:03 -03:00)\n"
          ]
        }
      ],
      "source": [
        "import sympy as sp\n",
        "\n",
        "x1, x2, x3= sp.symbols('x_1 x_2 x_3')\n",
        "mu1, mu2, mu3 = sp.symbols('μ_1 μ_2 μ_3')\n",
        "x = sp.Matrix([x1, x2, x3])\n",
        "mu = sp.Matrix([mu1, mu2, mu3])\n",
        "Sigma = sp.Matrix([[1,0,0],[0,1,0],[0,0,1]])\n",
        "\n",
        "G = sp.exp(-0.5 * (x - mu).T * Sigma.inv() * (x - mu)) \n",
        "\n",
        "dG_dmu = sp.diff(G, mu)\n",
        "\n",
        "print(\"G:\")\n",
        "display(G)\n",
        "print(\"Derivative of G w.r.t μ (dG/dmu):\")\n",
        "display(sp.simplify(dG_dmu))\n",
        "dG_dmu_num = dG_dmu.subs({x1:2, x2:2, x3:3, mu1:1, mu2:1, mu3:1}).tolist()\n",
        "print(\"Derivative w.r.t μ evaluated at x=[2,2,3], μ=[1,1,1]:\\n\", dG_dmu_num )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Implementação PyTorch:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[0.0498],\n",
            "        [0.0498],\n",
            "        [0.0996]])\n",
            "time: 0 ns (started: 2025-06-11 10:04:48 -03:00)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "x = torch.tensor([[2.0], [2.0], [3.0]], requires_grad=True)\n",
        "mu = torch.tensor([[1.0], [1.0], [1.0]], requires_grad=True)\n",
        "Sigma = torch.eye(3)\n",
        "\n",
        "# Forward pass\n",
        "dx = x - mu\n",
        "g = torch.exp(-0.5 * dx.T @ torch.inverse(Sigma) @ dx)\n",
        "\n",
        "# Backward pass\n",
        "g.backward()\n",
        "\n",
        "# Gradients\n",
        "print(mu.grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Comparação:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[0.0498],\n",
            "        [0.0498],\n",
            "        [0.0996]]) \n",
            " tensor([[0.0498],\n",
            "        [0.0498],\n",
            "        [0.0996]])\n",
            "time: 0 ns (started: 2025-06-11 10:04:52 -03:00)\n"
          ]
        }
      ],
      "source": [
        "dg_dmu = torch.tensor(dG_dmu_num, dtype=torch.float32).reshape(mu.grad.shape)\n",
        "\n",
        "# Gradients\n",
        "print(mu.grad,\"\\n\", dg_dmu)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Exemplo 7"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "$\\displaystyle C(\\mathbf{p}) = \\sum_{i=1}^{N} \\left[ c_i \\ G_i(\\mathbf{p})  \\prod_{j=1}^{i-1} (1- G_j(\\mathbf{p})) \\right]$\n",
        "\n",
        "Onde:\n",
        "- $\\displaystyle G_i(\\mathbf{p}) = exp(-0.5 (\\mathbf{p}-\\mathbf{\\mu_i})^T \\Sigma_i^{'-1} (\\mathbf{p}-\\mathbf{\\mu_i}))$;\n",
        "- $c_i \\in \\mathbb{R}$;\n",
        "- $\\mathbf{\\mu_i},\\mathbf{p} \\in \\mathbb{R}^2$;\n",
        "- $\\Sigma_i' \\in \\mathbb{R}^{2\\times 2}$;\n",
        "- $\\mathbf{\\mu_i}$ são parâmetros a ser atualizados pelo gradiente descendente.\n",
        "\n",
        "$\\displaystyle \\nabla(\\mu) = \\frac{\\partial C}{\\partial \\mu} = \\sum_{i=1}^{N} \\left[ c_i \\ \\frac{\\partial G_i}{\\partial \\mu} \\prod_{j=1}^{i-1} (1- G_j)  + c_i G_i \\sum_{k=1}^{i-1} \\left(\\prod_{j=1}^{k-1}  (1- G_j)\\right) \\left(-\\frac{\\partial G_k}{\\partial \\mu}\\right) \\left(\\prod_{j=k+1}^{i-1}  (1- G_j)\\right) \\right]$\n",
        "\n",
        "Para $N=3, \\mathbf{p}=[2,3],\\ c_i=1/(i+1),\\ \\mathbf{\\mu_i}=[1+i,1+i] \\ $ e $\\Sigma_i^{'-1} = I \\rightarrow$\n",
        "\n",
        "$\\nabla(\\mu) = \\begin{bmatrix} 0.0507 &  0.1013  \\\\ 0.0000 &  0.1658 \\\\ -0.0730 &  0.0000 \\end{bmatrix}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Implementação SymPy:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "C(p)=\n"
          ]
        },
        {
          "data": {
            "text/latex": [
              "$\\displaystyle \\left(1 - e^{- 0.5 \\left(p_{x} - {μ_{x}}_{0}\\right) \\left(1.0 p_{x} - 1.0 {μ_{x}}_{0}\\right) - 0.5 \\left(p_{y} - {μ_{y}}_{0}\\right) \\left(1.0 p_{y} - 1.0 {μ_{y}}_{0}\\right)}\\right) \\left(1 - e^{- 0.5 \\left(p_{x} - {μ_{x}}_{1}\\right) \\left(1.0 p_{x} - 1.0 {μ_{x}}_{1}\\right) - 0.5 \\left(p_{y} - {μ_{y}}_{1}\\right) \\left(1.0 p_{y} - 1.0 {μ_{y}}_{1}\\right)}\\right) e^{- 0.5 \\left(p_{x} - {μ_{x}}_{2}\\right) \\left(1.0 p_{x} - 1.0 {μ_{x}}_{2}\\right) - 0.5 \\left(p_{y} - {μ_{y}}_{2}\\right) \\left(1.0 p_{y} - 1.0 {μ_{y}}_{2}\\right)} {c}_{2} + \\left(1 - e^{- 0.5 \\left(p_{x} - {μ_{x}}_{0}\\right) \\left(1.0 p_{x} - 1.0 {μ_{x}}_{0}\\right) - 0.5 \\left(p_{y} - {μ_{y}}_{0}\\right) \\left(1.0 p_{y} - 1.0 {μ_{y}}_{0}\\right)}\\right) e^{- 0.5 \\left(p_{x} - {μ_{x}}_{1}\\right) \\left(1.0 p_{x} - 1.0 {μ_{x}}_{1}\\right) - 0.5 \\left(p_{y} - {μ_{y}}_{1}\\right) \\left(1.0 p_{y} - 1.0 {μ_{y}}_{1}\\right)} {c}_{1} + e^{- 0.5 \\left(p_{x} - {μ_{x}}_{0}\\right) \\left(1.0 p_{x} - 1.0 {μ_{x}}_{0}\\right) - 0.5 \\left(p_{y} - {μ_{y}}_{0}\\right) \\left(1.0 p_{y} - 1.0 {μ_{y}}_{0}\\right)} {c}_{0}$"
            ],
            "text/plain": [
              "(1 - exp(-0.5*(p_x - μ_x[0])*(1.0*p_x - 1.0*μ_x[0]) - 0.5*(p_y - μ_y[0])*(1.0*p_y - 1.0*μ_y[0])))*(1 - exp(-0.5*(p_x - μ_x[1])*(1.0*p_x - 1.0*μ_x[1]) - 0.5*(p_y - μ_y[1])*(1.0*p_y - 1.0*μ_y[1])))*exp(-0.5*(p_x - μ_x[2])*(1.0*p_x - 1.0*μ_x[2]) - 0.5*(p_y - μ_y[2])*(1.0*p_y - 1.0*μ_y[2]))*c[2] + (1 - exp(-0.5*(p_x - μ_x[0])*(1.0*p_x - 1.0*μ_x[0]) - 0.5*(p_y - μ_y[0])*(1.0*p_y - 1.0*μ_y[0])))*exp(-0.5*(p_x - μ_x[1])*(1.0*p_x - 1.0*μ_x[1]) - 0.5*(p_y - μ_y[1])*(1.0*p_y - 1.0*μ_y[1]))*c[1] + exp(-0.5*(p_x - μ_x[0])*(1.0*p_x - 1.0*μ_x[0]) - 0.5*(p_y - μ_y[0])*(1.0*p_y - 1.0*μ_y[0]))*c[0]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "C(p) evaluated at {p_x: 2, p_y: 3, μ_x[0]: 1, μ_y[0]: 1, c[0]: 1.0, μ_x[1]: 2, μ_y[1]: 2, c[1]: 0.5, μ_x[2]: 3, μ_y[2]: 3, c[2]: 0.3333333333333333}: 0.433477305494832\n",
            "Derivative w.r.t μ_0 evaluated: [[0.0506615694581183], [0.101323138916237]]\n",
            "Derivative w.r.t μ_1 evaluated: [[0], [0.165811109756010]]\n",
            "Derivative w.r.t μ_2 evaluated: [[-0.0730205111985485], [0]]\n",
            "time: 203 ms (started: 2025-06-11 10:20:23 -03:00)\n"
          ]
        }
      ],
      "source": [
        "import sympy as sp\n",
        "\n",
        "N = 3  # Number of components\n",
        "Sigma_inv_i = sp.Matrix([[1.0, 0.0], [0.0, 1.0]]) # assumir conhecida por simplicidade\n",
        "Sigma_inv_j = sp.Matrix([[1.0, 0.0], [0.0, 1.0]])\n",
        "\n",
        "# variam para n=1,...,N\n",
        "c = sp.IndexedBase('c')\n",
        "mux = sp.IndexedBase('μ_x')\n",
        "muy = sp.IndexedBase('μ_y')\n",
        "\n",
        "# Ponto de pesquisa\n",
        "px, py= sp.symbols('p_x p_y')\n",
        "p = sp.Matrix([px, py])\n",
        "\n",
        "# Generate values to apply to the derivative\n",
        "values = {px:2, py:3}\n",
        "for i in range(N):\n",
        "    values = values | {mux[i]:1+i, muy[i]:1+i, c[i]:1/(i+1)}\n",
        "\n",
        "# Expression for C(x)\n",
        "C = 0 \n",
        "for i in range(0,N):\n",
        "    d_i = p - sp.Matrix([mux[i], muy[i]])\n",
        "    G_i = sp.exp(-0.5 * (d_i.T * Sigma_inv_i * d_i)[0, 0])\n",
        "    prod = 1\n",
        "    for j in range(0, i):\n",
        "        d_j = p - sp.Matrix([mux[j], muy[j]])\n",
        "        G_j = sp.exp(-0.5 * (d_j.T * Sigma_inv_j * d_j)[0, 0])\n",
        "        prod *= (1 - G_j)\n",
        "    C += c[i] * G_i * prod\n",
        "\n",
        "print(\"C(p)=\")\n",
        "display(C)\n",
        "print(f\"C(p) evaluated at {values}:\", C.subs(values).evalf())\n",
        "\n",
        "dC_dmu =[]\n",
        "for i in range(N):\n",
        "    dC_dmu_i = sp.diff(C, sp.Matrix([mux[i], muy[i]]))\n",
        "    dC_dmu_num = dC_dmu_i.subs(values).evalf().tolist()\n",
        "    print(f\"Derivative w.r.t μ_{i} evaluated:\", dC_dmu_num )\n",
        "    dC_dmu.append(dC_dmu_num)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Implementação PyTorch:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 280,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "C(p)= tensor(0.4335, grad_fn=<AddBackward0>)\n",
            "tensor([[ 0.0507,  0.1013],\n",
            "        [ 0.0000,  0.1658],\n",
            "        [-0.0730,  0.0000]])\n",
            "time: 0 ns (started: 2025-06-10 17:13:55 -03:00)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "N = 3  # Number of components\n",
        "mu = torch.tensor([[1.0+i, 1.0+i] for i in range(N)], requires_grad=True) \n",
        "\n",
        "p = torch.tensor([2.0, 3.0], requires_grad=False)\n",
        "c = torch.tensor([1.0/(i+1) for i in range(N)], requires_grad=False)\n",
        "Sigma_inv_i = torch.tensor([[1.0, 0.0],[0.0, 1.0]], requires_grad=False)\n",
        "Sigma_inv_j = torch.tensor([[1.0, 0.0],[0.0, 1.0]], requires_grad=False)\n",
        "\n",
        "# Forward pass\n",
        "C = torch.tensor(0.0) \n",
        "for i in range(0,N):\n",
        "    d_i = p - mu[i]\n",
        "    G_i = torch.exp(-0.5 * (d_i.T @ Sigma_inv_i @ d_i))\n",
        "    prod = 1\n",
        "    for j in range(0, i):\n",
        "        d_j = p - mu[j]\n",
        "        G_j = torch.exp(-0.5 * (d_j.T @ Sigma_inv_j @ d_j))\n",
        "        prod *= (1 - G_j)\n",
        "    C += c[i] * G_i * prod\n",
        "print(\"C(p)=\", C)\n",
        "# Backward pass\n",
        "C.backward()\n",
        "\n",
        "# Gradients\n",
        "print(mu.grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Comparação:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 281,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[ 0.0507,  0.1013],\n",
            "        [ 0.0000,  0.1658],\n",
            "        [-0.0730,  0.0000]]) \n",
            " tensor([[ 0.0507,  0.1013],\n",
            "        [ 0.0000,  0.1658],\n",
            "        [-0.0730,  0.0000]])\n",
            "time: 0 ns (started: 2025-06-10 17:13:55 -03:00)\n"
          ]
        }
      ],
      "source": [
        "dC_dmu = torch.tensor(dC_dmu, dtype=torch.float32).reshape(mu.grad.shape)\n",
        "\n",
        "# Gradients\n",
        "print(mu.grad, \"\\n\", dC_dmu)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Exemplo 8"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "$\\displaystyle C(\\mathbf{p}) = \\sum_{i=1}^{N} \\left[ c_i \\ G_i(\\mathbf{p})  \\prod_{j=1}^{i-1} (1- G_j(\\mathbf{p})) \\right]$\n",
        "\n",
        "Onde:\n",
        "- $G_i(\\mathbf{p}) = exp(-0.5 (\\mathbf{p}-\\mathbf{\\mu_i})^T \\Sigma_i^{'-1} (\\mathbf{p}-\\mathbf{\\mu_i}))$;\n",
        "- $\\Sigma_i' = JWR_iS_iS_i^TR_i^TW^TJ^T$;\n",
        "- $S_i = diag(\\mathbf{s_i})$;\n",
        "- $J \\in \\mathbb{R}^{2 \\times 3}$ e $W,R_i,S_i \\in \\mathbb{R}^{3 \\times 3}$;\n",
        "- $c_i \\in \\mathbb{R}$, $\\mathbf{\\mu_i},\\mathbf{p} \\in \\mathbb{R}^2$ e $\\mathbf{s_i} \\in \\mathbb{R}^3$;\n",
        "- $\\mathbf{\\mu_i}$ e $\\mathbf{s_i}$ serão atualizados pelo gradiente descendente.\n",
        "\n",
        "$\\displaystyle \\nabla(\\mu) = \\frac{\\partial C}{\\partial \\mu} = \\sum_{i=1}^{N} \\left[ c_i \\ \\frac{\\partial G_i}{\\partial \\mu} \\prod_{j=1}^{i-1} (1- G_j)  + c_i G_i \\sum_{k=1}^{i-1} \\left(\\prod_{j=1}^{k-1}  (1- G_j)\\right) \\left(-\\frac{\\partial G_k}{\\partial \\mu}\\right) \\left(\\prod_{j=k+1}^{i-1}  (1- G_j)\\right) \\right]$\n",
        "\n",
        "$\\displaystyle \\nabla(\\mathbf{s}) = \\frac{\\partial C}{\\partial \\mathbf{s}} = \\sum_{i=1}^{N} \\left[ c_i \\ \\frac{\\partial G_i}{\\partial \\mathbf{s}} \\prod_{j=1}^{i-1} (1- G_j)  + c_i G_i \\sum_{k=1}^{i-1} \\left(\\prod_{j=1}^{k-1}  (1- G_j)\\right) \\left(-\\frac{\\partial G_k}{\\partial \\mathbf{s}}\\right) \\left(\\prod_{j=k+1}^{i-1}  (1- G_j)\\right) \\right]$\n",
        "\n",
        "Para $N=3, \\mathbf{p}=[2,3],\\ c_i=1/(i+1),\\ \\mathbf{\\mu_i}=[1+i,1+i],\\ \\mathbf{s_i} = [0.5(i+1),0.5(i+1),0.5(i+1)]$, $J = diag(1,1,0), W = I, R_i=I \\rightarrow$\n",
        "\n",
        "$\\nabla(\\mu) = ?$\n",
        "\n",
        "$\\nabla(\\mathbf{s}) = ?$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Implementação SymPy:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 282,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "C(p)=\n"
          ]
        },
        {
          "data": {
            "text/latex": [
              "$\\displaystyle \\left(1 - e^{- \\frac{0.5 \\left(p_{x} - {μ_{x}}_{0}\\right)^{2}}{{s_{x}}_{0}^{2}} - \\frac{0.5 \\left(p_{y} - {μ_{y}}_{0}\\right)^{2}}{{s_{y}}_{0}^{2}}}\\right) \\left(1 - e^{- \\frac{0.5 \\left(p_{x} - {μ_{x}}_{1}\\right)^{2}}{{s_{x}}_{1}^{2}} - \\frac{0.5 \\left(p_{y} - {μ_{y}}_{1}\\right)^{2}}{{s_{y}}_{1}^{2}}}\\right) e^{- \\frac{0.5 \\left(p_{x} - {μ_{x}}_{2}\\right)^{2}}{{s_{x}}_{2}^{2}} - \\frac{0.5 \\left(p_{y} - {μ_{y}}_{2}\\right)^{2}}{{s_{y}}_{2}^{2}}} {c}_{2} + \\left(1 - e^{- \\frac{0.5 \\left(p_{x} - {μ_{x}}_{0}\\right)^{2}}{{s_{x}}_{0}^{2}} - \\frac{0.5 \\left(p_{y} - {μ_{y}}_{0}\\right)^{2}}{{s_{y}}_{0}^{2}}}\\right) e^{- \\frac{0.5 \\left(p_{x} - {μ_{x}}_{1}\\right)^{2}}{{s_{x}}_{1}^{2}} - \\frac{0.5 \\left(p_{y} - {μ_{y}}_{1}\\right)^{2}}{{s_{y}}_{1}^{2}}} {c}_{1} + e^{- \\frac{0.5 \\left(p_{x} - {μ_{x}}_{0}\\right)^{2}}{{s_{x}}_{0}^{2}} - \\frac{0.5 \\left(p_{y} - {μ_{y}}_{0}\\right)^{2}}{{s_{y}}_{0}^{2}}} {c}_{0}$"
            ],
            "text/plain": [
              "(1 - exp(-0.5*(p_x - μ_x[0])**2/s_x[0]**2 - 0.5*(p_y - μ_y[0])**2/s_y[0]**2))*(1 - exp(-0.5*(p_x - μ_x[1])**2/s_x[1]**2 - 0.5*(p_y - μ_y[1])**2/s_y[1]**2))*exp(-0.5*(p_x - μ_x[2])**2/s_x[2]**2 - 0.5*(p_y - μ_y[2])**2/s_y[2]**2)*c[2] + (1 - exp(-0.5*(p_x - μ_x[0])**2/s_x[0]**2 - 0.5*(p_y - μ_y[0])**2/s_y[0]**2))*exp(-0.5*(p_x - μ_x[1])**2/s_x[1]**2 - 0.5*(p_y - μ_y[1])**2/s_y[1]**2)*c[1] + exp(-0.5*(p_x - μ_x[0])**2/s_x[0]**2 - 0.5*(p_y - μ_y[0])**2/s_y[0]**2)*c[0]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation data: {p_x: 2, p_y: 3, μ_x[0]: 1, μ_y[0]: 1, c[0]: 1.0, s_x[0]: 0.5, s_y[0]: 0.5, s_z[0]: 0.5, μ_x[1]: 2, μ_y[1]: 2, c[1]: 0.5, s_x[1]: 1.0, s_y[1]: 1.0, s_z[1]: 1.0, μ_x[2]: 3, μ_y[2]: 3, c[2]: 0.3333333333333333, s_x[2]: 1.5, s_y[2]: 1.5, s_z[2]: 1.5}:\n",
            "C(p) evaluated: 0.408314066132132\n",
            "Derivative w.r.t μ_0 evaluated: [[0.000107454877800111], [0.000214909755600221]]\n",
            "Derivative w.r.t μ_1 evaluated: [[0], [0.141368316370717]]\n",
            "Derivative w.r.t μ_2 evaluated: [[-0.0466742686981012], [0]]\n",
            "Derivative w.r.t s_0 evaluated: [[0.000214909755600221], [0.000859639022400885], [0]]\n",
            "Derivative w.r.t s_1 evaluated: [[0], [0.141368316370717], [0]]\n",
            "Derivative w.r.t s_2 evaluated: [[0.0311161791320675], [0], [0]]\n",
            "time: 781 ms (started: 2025-06-10 17:13:55 -03:00)\n"
          ]
        }
      ],
      "source": [
        "import sympy as sp\n",
        "\n",
        "N = 3  # Number of components\n",
        "R_i = sp.Matrix([[1.0, 0.0, 0.0], [0.0, 1.0, 0.0],[0.0, 0.0, 1.0]]) # assumir conhecida por simplicidade\n",
        "W = sp.Matrix([[1.0, 0.0, 0.0], [0.0, 1.0, 0.0],[0.0, 0.0, 1.0]])\n",
        "J = sp.Matrix([[1.0, 0.0, 0.0], [0.0, 1.0, 0.0]])\n",
        "\n",
        "# variam para n=1,...,N\n",
        "c = sp.IndexedBase('c')\n",
        "mux = sp.IndexedBase('μ_x')\n",
        "muy = sp.IndexedBase('μ_y')\n",
        "sx = sp.IndexedBase('s_x')\n",
        "sy = sp.IndexedBase('s_y')\n",
        "sz = sp.IndexedBase('s_z')\n",
        "\n",
        "# Ponto de pesquisa\n",
        "px, py= sp.symbols('p_x p_y')\n",
        "p = sp.Matrix([px, py])\n",
        "\n",
        "# Generate values to apply to the derivative\n",
        "values = {px:2, py:3}\n",
        "for i in range(N):\n",
        "    values = values | {mux[i]:1+i, muy[i]:1+i, c[i]:1/(i+1), sx[i]:0.5*(i+1), sy[i]:0.5*(i+1), sz[i]:0.5*(i+1)}\n",
        "\n",
        "# Expression for C(x)\n",
        "C = 0 \n",
        "for i in range(0,N):\n",
        "    d_i = p - sp.Matrix([mux[i], muy[i]])\n",
        "    S_i = sp.Matrix([[sx[i], 0, 0], [0, sy[i], 0], [0, 0, sz[i]]])\n",
        "    Sigma_2D_i = J * W * R_i * S_i * S_i.T * R_i.T * W.T * J.T\n",
        "    Sigma_2D_inv_i = Sigma_2D_i.inv()\n",
        "    G_i = sp.exp(-0.5 * (d_i.T * Sigma_2D_inv_i * d_i)[0, 0])\n",
        "    prod = 1\n",
        "    for j in range(0, i):\n",
        "        d_j = p - sp.Matrix([mux[j], muy[j]])\n",
        "        S_j = sp.Matrix([[sx[j], 0, 0], [0, sy[j], 0], [0, 0, sz[j]]])\n",
        "        Sigma_2D_j = J * W * R_i * S_j * S_j.T * R_i.T * W.T * J.T\n",
        "        Sigma_2D_inv_j = Sigma_2D_j.inv()\n",
        "        G_j = sp.exp(-0.5 * (d_j.T * Sigma_2D_inv_j * d_j)[0, 0])\n",
        "        prod *= (1 - G_j)\n",
        "    C += c[i] * G_i * prod\n",
        "\n",
        "print(\"C(p)=\")\n",
        "display(C)\n",
        "\n",
        "print(f\"Evaluation data: {values}:\")\n",
        "print(f\"C(p) evaluated:\", C.subs(values).evalf())\n",
        "\n",
        "dC_dmu = []\n",
        "for i in range(N):\n",
        "    dC_dmu_i = sp.diff(C, sp.Matrix([mux[i], muy[i]]))\n",
        "    dC_dmu_num = dC_dmu_i.subs(values).evalf().tolist()\n",
        "    print(f\"Derivative w.r.t μ_{i} evaluated:\", dC_dmu_num )\n",
        "    dC_dmu.append(dC_dmu_num)\n",
        "\n",
        "dC_ds = []\n",
        "for i in range(N):\n",
        "    dC_ds_i = sp.diff(C, sp.Matrix([sx[i], sy[i], sz[i]]))\n",
        "    dC_ds_num = dC_ds_i.subs(values).evalf().tolist()\n",
        "    print(f\"Derivative w.r.t s_{i} evaluated:\", dC_ds_num )\n",
        "    dC_ds.append(dC_ds_num)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Implementação PyTorch:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 283,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "C(p) = tensor(0.4083, grad_fn=<AddBackward0>)\n",
            "tensor([[ 1.0745e-04,  2.1491e-04],\n",
            "        [ 0.0000e+00,  1.4137e-01],\n",
            "        [-4.6674e-02,  0.0000e+00]])\n",
            "tensor([[0.0002, 0.0009, 0.0000],\n",
            "        [0.0000, 0.1414, 0.0000],\n",
            "        [0.0311, 0.0000, 0.0000]])\n",
            "time: 16 ms (started: 2025-06-10 17:13:56 -03:00)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "N = 3  # Number of components\n",
        "mu = torch.tensor([[1.0+i, 1.0+i] for i in range(N)], requires_grad=True)\n",
        "s = torch.tensor([[0.5*(i+1), 0.5*(i+1), 0.5*(i+1)] for i in range(N)], requires_grad=True) \n",
        "\n",
        "p = torch.tensor([2.0, 3.0], requires_grad=False)\n",
        "c = torch.tensor([1.0/(i+1) for i in range(N)], requires_grad=False)\n",
        "R_i = torch.tensor([[1.0, 0.0, 0.0], [0.0, 1.0, 0.0],[0.0, 0.0, 1.0]], requires_grad=False)\n",
        "W = torch.tensor([[1.0, 0.0, 0.0], [0.0, 1.0, 0.0],[0.0, 0.0, 1.0]], requires_grad=False)\n",
        "J = torch.tensor([[1.0, 0.0, 0.0], [0.0, 1.0, 0.0]], requires_grad=False)\n",
        "\n",
        "# Forward pass\n",
        "C = torch.tensor(0.0) \n",
        "for i in range(N):\n",
        "    d_i = p - mu[i]\n",
        "    S_i = torch.diag(s[i])\n",
        "    Sigma_2D_i = J @ W @ R_i @ S_i @ S_i.T @ R_i.T @ W.T @ J.T\n",
        "    Sigma_2D_inv_i = torch.linalg.inv(Sigma_2D_i)\n",
        "    G_i = torch.exp(-0.5 * (d_i.T @ Sigma_2D_inv_i @ d_i))\n",
        "    prod = 1\n",
        "    for j in range(i):\n",
        "        d_j = p - mu[j]\n",
        "        S_j = torch.diag(s[j])\n",
        "        Sigma_2D_j = J @ W @ R_i @ S_j @ S_j.T @ R_i.T @ W.T @ J.T\n",
        "        Sigma_2D_inv_j = torch.linalg.inv(Sigma_2D_j)\n",
        "        G_j = torch.exp(-0.5 * (d_j.T @ Sigma_2D_inv_j @ d_j))\n",
        "        prod *= (1 - G_j)\n",
        "    C += c[i] * G_i * prod\n",
        "print(\"C(p) =\", C)\n",
        "\n",
        "# Backward pass\n",
        "C.backward()\n",
        "\n",
        "# Gradients\n",
        "print(mu.grad)\n",
        "print(s.grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Comparação:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 284,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[ 1.0745e-04,  2.1491e-04],\n",
            "        [ 0.0000e+00,  1.4137e-01],\n",
            "        [-4.6674e-02,  0.0000e+00]]) \n",
            " tensor([[ 1.0745e-04,  2.1491e-04],\n",
            "        [ 0.0000e+00,  1.4137e-01],\n",
            "        [-4.6674e-02,  0.0000e+00]])\n",
            "tensor([[0.0002, 0.0009, 0.0000],\n",
            "        [0.0000, 0.1414, 0.0000],\n",
            "        [0.0311, 0.0000, 0.0000]]) \n",
            " tensor([[0.0002, 0.0009, 0.0000],\n",
            "        [0.0000, 0.1414, 0.0000],\n",
            "        [0.0311, 0.0000, 0.0000]])\n",
            "time: 0 ns (started: 2025-06-10 17:13:56 -03:00)\n"
          ]
        }
      ],
      "source": [
        "dC_dmu = torch.tensor(dC_dmu, dtype=torch.float32).reshape(mu.grad.shape)\n",
        "dC_ds = torch.tensor(dC_ds, dtype=torch.float32).reshape(s.grad.shape)\n",
        "\n",
        "# Gradients\n",
        "print(mu.grad, \"\\n\", dC_dmu)\n",
        "print(s.grad, \"\\n\", dC_ds )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Exemplo 9"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "$\\displaystyle \\mathcal{L} = \\sum_{k=1}^{256}|| C(\\mathbf{p_k}) - \\hat{C}(\\mathbf{p_k}) ||^2$\n",
        "\n",
        "Onde:\n",
        "- $\\hat{C}(\\mathbf{p_k})$ são observações;\n",
        "- $\\displaystyle C(\\mathbf{p_k}) = \\sum_{i=1}^{N} \\left[ c_i \\ G_i(\\mathbf{p_k})  \\prod_{j=1}^{i-1} (1- G_j(\\mathbf{p_k})) \\right]$;\n",
        "\n",
        "- $G_i(\\mathbf{p_k}) = exp(-0.5 (\\mathbf{p_k}-\\mathbf{\\mu}^{2D}_i)^T \\Sigma_i^{'-1} (\\mathbf{p_k}-\\mathbf{\\mu}^{2D}_i))$;\n",
        "- $\\mu^{2D}_{i} = \\begin{bmatrix}\\frac{μ^{3D}_{ix} {P}_{0,0} + μ^{3D}_{iy} {P}_{0,1} + μ^{3D}_{iz} {P}_{0,2} + {P}_{0,3}}{μ^{3D}_{ix} {P}_{2,0} + μ^{3D}_{iy} {P}_{2,1} + μ^{3D}_{iz} {P}_{2,2} + {P}_{2,3}}\\\\\\frac{μ^{3D}_{ix} {P}_{1,0} + μ^{3D}_{iy} {P}_{1,1} + μ^{3D}_{iz} {P}_{1,2} + {P}_{1,3}}{μ^{3D}_{ix} {P}_{2,0} + μ^{3D}_{iy} {P}_{2,1} + μ^{3D}_{iz} {P}_{2,2} + {P}_{2,3}}\\end{bmatrix}$\n",
        "- $\\Sigma_i' = JWR_iS_iS_i^TR_i^TW^TJ^T$;\n",
        "- $S_i = diag(\\mathbf{s}_{i}) = \\begin{bmatrix} s_{ix} & 0 & 0 \\\\ 0 & s_{iy} &0 \\\\ 0 & 0 & s_{iz} \\end{bmatrix}$;\n",
        "- $J \\in \\mathbb{R}^{2 \\times 3}$ e $W,R_i,S_i \\in \\mathbb{R}^{3 \\times 3}$;\n",
        "- $c_i, P_{m,n} \\in \\mathbb{R}$, $\\mathbf{\\mu}^{2D}_i,\\mathbf{p_k} \\in \\mathbb{R}^2$ e $\\mathbf{\\mu}^{3D}_i, \\mathbf{s}_{i} \\in \\mathbb{R}^3$;\n",
        "- $\\mathbf{\\mu}^{3D}_{i}$ e $\\mathbf{s}_{i}$ serão atualizados pelo gradiente descendente.\n",
        "\n",
        "$\\displaystyle \\nabla(\\mu^{3D}) = \\frac{\\partial \\mathcal{L}}{\\partial \\mu^{3D}} = \\sum_{k=1}^{256} 2 \\cdot || C - \\hat{C} || \\cdot || \\frac{\\partial C}{\\partial \\mu^{3D}}|| $\n",
        "\n",
        "$\\displaystyle \\nabla(\\mathbf{s}) = \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{s}} = \\sum_{k=1}^{256} 2 \\cdot || C - \\hat{C} || \\cdot || \\frac{\\partial C}{\\partial \\mathbf{s}}||$\n",
        "\n",
        "Para $N=3, \\mathbf{p}=[2,3],\\ c_i=1/(i+1),\\ \\mathbf{\\mu_i}=[1+i,1+i],\\ \\mathbf{s_i} = [0.5(i+1),0.5(i+1),0.5(i+1)]$, $J = diag(1,1,0), W = I, R_i=I \\rightarrow$\n",
        "\n",
        "$\\nabla(\\mu) = ?$\n",
        "\n",
        "$\\nabla(\\mathbf{s}) = ?$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sympy as sp\n",
        "N = 256\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qMy2NKTcip_j"
      },
      "source": [
        "# Exemplo 5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dvfKbG4Ek_nC"
      },
      "source": [
        "$m = 2w_1 + 3w_2 + b$\n",
        "\n",
        "Onde $w_i$ e $b_i$ são parâmetros a serem atualizados pelo gradiente descendente.\n",
        "\n",
        "$\\nabla(w_1) = \\frac{\\partial m}{\\partial w_1}=2$\n",
        "\n",
        "$\\nabla(w_2) = \\frac{\\partial m}{\\partial w_2}=3$\n",
        "\n",
        "$\\nabla(b) = \\frac{\\partial m}{\\partial b}=1$\n",
        "\n",
        "Para quaisquer $[w_1, w_2]$ e $[b] \\rightarrow$\n",
        "\n",
        "$\\nabla(w_1) = 2$\n",
        "\n",
        "$\\nabla(w_2) = 3$\n",
        "\n",
        "$\\nabla(b) = 1$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ye81MEAlhSxU",
        "outputId": "a50d8fcd-7e1e-46c9-8652-646dcc34b9ba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Weight gradient: tensor([[2., 3.]])\n",
            "Bias gradient: tensor([1.])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Define a simple model\n",
        "m = nn.Linear(in_features=2, out_features=1)  # Single layer: m = xA^T + b\n",
        "\n",
        "# Input and target\n",
        "x = torch.tensor([[2.0, 3.0]], requires_grad=True)\n",
        "\n",
        "# Forward pass\n",
        "pred = m(x)\n",
        "\n",
        "# Backward pass\n",
        "pred.backward()\n",
        "\n",
        "# Gradients\n",
        "print(\"Weight gradient:\", m.weight.grad)\n",
        "print(\"Bias gradient:\", m.bias.grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Exemplo 5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "$m = x_1w_1 + x_2w_2 + x_3w_3 + b$\n",
        "\n",
        "Onde $w_i$ e $b$ são parâmetros a serem atualizados pelo gradiente descendente.\n",
        "\n",
        "$\\nabla(w_1) = \\frac{\\partial m}{\\partial w_1}=x_1$\n",
        "\n",
        "$\\nabla(w_2) = \\frac{\\partial m}{\\partial w_2}=x_2$\n",
        "\n",
        "$\\nabla(w_3) = \\frac{\\partial m}{\\partial w_2}=x_3$\n",
        "\n",
        "$\\nabla(b) = \\frac{\\partial m}{\\partial b}=1$\n",
        "\n",
        "Para $[x_1, x_2, x_3]=[2, 3, 4]$ e quaisquer $[w_1, w_2, w_3,b] \\rightarrow$\n",
        "\n",
        "$\\nabla(w_1) = 2$\n",
        "\n",
        "$\\nabla(w_2) = 3$\n",
        "\n",
        "$\\nabla(w_3) = 4$\n",
        "\n",
        "$\\nabla(b) = 1$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "kQovmYXlitGQ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Weight gradient: tensor([[2., 3., 4.]])\n",
            "Bias gradient: tensor([1.])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Define a simple model\n",
        "m = nn.Linear(in_features=3, out_features=1)  # Single layer: m = xA^T + b\n",
        "\n",
        "# Input and target\n",
        "x = torch.tensor([[2.0, 3.0, 4.0]], requires_grad=True)\n",
        "\n",
        "# Forward pass\n",
        "pred = m(x)\n",
        "\n",
        "# Backward pass\n",
        "pred.backward()\n",
        "\n",
        "# Gradients\n",
        "print(\"Weight gradient:\", m.weight.grad)\n",
        "print(\"Bias gradient:\", m.bias.grad)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "torch-splatting",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
